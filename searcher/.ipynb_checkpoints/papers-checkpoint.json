[{"title": "Let's Stop Incorrect Comparisons in End-to-end Relation Extraction! (EMNLP 2020)", "authors": "mlia deeplearning", "abstract": "", "publicationOrg": "EMNLP", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2009.10684.pdf", "pdfPath": "", "publicationUrl": "https://arxiv.org/pdf/2009.10684.pdf", "codeUrl": "https://github.com/btaille/sincere", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/sIG8g85D03k", "videoPath": "/data/cache/1/videos/Let's Stop Incorrect Comparisons in End-to-end Relation Extraction! (EMNLP 2020).mp4", "pdfText": "", "videoStruct": [{"timeStart": "00-00-01", "timeEnd": "00-00-05", "sentence": "I'm gonna the year phd student I saw the city and bent by"}, {"timeStart": "00-00-05", "timeEnd": "00-00-10", "sentence": "today I present my work on English composer and twenty election"}, {"timeStart": "00-00-10", "timeEnd": "00-00-12", "sentence": "don't walk with us on"}, {"timeStart": "00-00-12", "timeEnd": "00-00-14", "sentence": "to fuck with a ten and path again"}, {"timeStart": "00-00-15", "timeEnd": "00-00-22", "sentence": "let me chill I couldn't show an additional session our two key information tasks for example a lot of knowledge construction"}, {"timeStart": "00-00-22", "timeEnd": "00-00-24", "sentence": "even a sentence"}, {"timeStart": "00-00-24", "timeEnd": "00-00-27", "sentence": "would like to expect dimensions of entities"}, {"timeStart": "00-00-27", "timeEnd": "00-00-29", "sentence": "as well as relations express between them"}, {"timeStart": "00-00-30", "timeEnd": "00-00-32", "sentence": "in end two Englishmen"}, {"timeStart": "00-00-32", "timeEnd": "00-00-36", "sentence": "a single my dad he's trying to perform both test and to model their interdependence"}, {"timeStart": "00-00-37", "timeEnd": "00-00-39", "sentence": "email us at to get full pose different models"}, {"timeStart": "00-00-40", "timeEnd": "00-00-43", "sentence": "because civil delusions set up that he used"}, {"timeStart": "00-00-43", "timeEnd": "00-00-47", "sentence": "some of them present in Clyde Buffalo bison to previous wach"}, {"timeStart": "00-00-47", "timeEnd": "00-00-51", "sentence": "amen good in the article is to raise awareness on this issue"}, {"timeStart": "00-00-51", "timeEnd": "00-00-54", "sentence": "but in order to be useful to Dodge origins"}, {"timeStart": "00-00-54", "timeEnd": "00-00-58", "sentence": "and we feel stuff form if we get out your review and then come to our more specific points"}, {"timeStart": "00-00-59", "timeEnd": "00-01-02", "sentence": "before the injury and degeneration"}, {"timeStart": "00-01-02", "timeEnd": "00-01-05", "sentence": "let's see how we would get on both test in a biplane approach"}, {"timeStart": "00-01-06", "timeEnd": "00-01-09", "sentence": "it was just a farm they mention"}, {"timeStart": "00-01-09", "timeEnd": "00-01-11", "sentence": "often modelled as sequence labeling"}, {"timeStart": "00-01-11", "timeEnd": "00-01-16", "sentence": "for each word you predict the dagger that he is like fifty for both the type of city"}, {"timeStart": "00-01-16", "timeEnd": "00-01-19", "sentence": "and the position of the world inside the mention"}, {"timeStart": "00-01-19", "timeEnd": "00-01-23", "sentence": "your kids will need a job to do so is a badass MC"}, {"timeStart": "00-01-24", "timeEnd": "00-01-29", "sentence": "today we would violate eventually simple kiss failure over plain language with it"}, {"timeStart": "00-01-30", "timeEnd": "00-01-34", "sentence": "and yeah I can also be viewed as responder Vegas vacation test"}, {"timeStart": "00-01-34", "timeEnd": "00-01-38", "sentence": "which is useful for nested or overlapping mentions"}, {"timeStart": "00-01-39", "timeEnd": "00-01-44", "sentence": "what pussy by subsidize he's asked by teachers if I has entered mentions honored"}, {"timeStart": "00-01-45", "timeEnd": "00-01-47", "sentence": "even the output of"}, {"timeStart": "00-01-47", "timeEnd": "00-01-52", "sentence": "relations section is then guests as classification of every pair of the decade entities"}, {"timeStart": "00-01-53", "timeEnd": "00-01-56", "sentence": "such a kissy for your takes as input a pair of entities"}, {"timeStart": "00-01-56", "timeEnd": "00-01-58", "sentence": "ends a convex intense"}, {"timeStart": "00-01-58", "timeEnd": "00-02-01", "sentence": "and it must predict which relation is expressed between the entities"}, {"timeStart": "00-02-02", "timeEnd": "00-02-05", "sentence": "when poopy dog to to charged with so is peace was pulling"}, {"timeStart": "00-02-05", "timeEnd": "00-02-08", "sentence": "was its intensity two faced ass"}, {"timeStart": "00-02-08", "timeEnd": "00-02-09", "sentence": "the twenty tees"}, {"timeStart": "00-02-09", "timeEnd": "00-02-14", "sentence": "and sweet substance he's before between and after identities"}, {"timeStart": "00-02-15", "timeEnd": "00-02-20", "sentence": "each bath is put into a fixed length of connotation and the concatenation is fed to a class a failure"}, {"timeStart": "00-02-21", "timeEnd": "00-02-27", "sentence": "today we can also see pts feedback with the concatenation of the quantities and the sentence"}, {"timeStart": "00-02-27", "timeEnd": "00-02-30", "sentence": "and we use this to get to train a classifier"}, {"timeStart": "00-02-31", "timeEnd": "00-02-36", "sentence": "so this gives us a better coach we are to support my dads are trained"}, {"timeStart": "00-02-36", "timeEnd": "00-02-38", "sentence": "see Patti and applied sequentially"}, {"timeStart": "00-02-38", "timeEnd": "00-02-45", "sentence": "while this enables modular it also prevents formulating the intuitive inter dependency between tasks"}, {"timeStart": "00-02-45", "timeEnd": "00-02-47", "sentence": "and it gets to a application"}, {"timeStart": "00-02-47", "timeEnd": "00-02-50", "sentence": "and this is why joint models have been proposed"}, {"timeStart": "00-02-51", "timeEnd": "00-02-55", "sentence": "so as a first approach is the incremental city proposed Bailey n g"}, {"timeStart": "00-02-56", "timeEnd": "00-03-00", "sentence": "sentence he's passed while they were and for each world we were formed"}, {"timeStart": "00-03-00", "timeEnd": "00-03-03", "sentence": "ninety two step and religion step"}, {"timeStart": "00-03-03", "timeEnd": "00-03-06", "sentence": "we begin with interesting to diction"}, {"timeStart": "00-03-06", "timeEnd": "00-03-09", "sentence": "and as soon as we have detected several entities"}, {"timeStart": "00-03-09", "timeEnd": "00-03-11", "sentence": "we can examine potential relations between them"}, {"timeStart": "00-03-13", "timeEnd": "00-03-17", "sentence": "you ain't to say keep opposed to guess this joint task as it table feeding problem"}, {"timeStart": "00-03-18", "timeEnd": "00-03-23", "sentence": "they notice that the information we want to excite can be opened as a table"}, {"timeStart": "00-03-23", "timeEnd": "00-03-26", "sentence": "whether they go to contain information on entities"}, {"timeStart": "00-03-26", "timeEnd": "00-03-28", "sentence": "and the o's or says information on the relations"}, {"timeStart": "00-03-28", "timeEnd": "00-03-32", "sentence": "that scans in defined as a sequence Lee Billy"}, {"timeStart": "00-03-32", "timeEnd": "00-03-35", "sentence": "they spell civil orders for this sequence with feeling"}, {"timeStart": "00-03-35", "timeEnd": "00-03-40", "sentence": "and we can see that if we begin with the diagonal with armed entity files petition"}, {"timeStart": "00-03-40", "timeEnd": "00-03-42", "sentence": "me daddy too to pay player coach"}, {"timeStart": "00-03-42", "timeEnd": "00-03-46", "sentence": "but we can also recovers incremental sitting by changing this order"}, {"timeStart": "00-03-47", "timeEnd": "00-03-50", "sentence": "the setting has been used in several additional works"}, {"timeStart": "00-03-50", "timeEnd": "00-03-53", "sentence": "including a new work by a wank and Lou"}, {"timeStart": "00-03-53", "timeEnd": "00-03-55", "sentence": "please MT dads he's like an feelings"}, {"timeStart": "00-03-55", "timeEnd": "00-03-58", "sentence": "and we bought a new state of the earth is urs"}, {"timeStart": "00-03-58", "timeEnd": "00-04-03", "sentence": "in both equipment and table settings the inter dependency"}, {"timeStart": "00-04-03", "timeEnd": "00-04-06", "sentence": "is murdered in the sequence of predictions"}, {"timeStart": "00-04-06", "timeEnd": "00-04-10", "sentence": "and Josie approach is to keep the pipeline such a"}, {"timeStart": "00-04-10", "timeEnd": "00-04-13", "sentence": "Vicky neighbor interactions in a shared language"}, {"timeStart": "00-04-13", "timeEnd": "00-04-19", "sentence": "he's been doing on the information shared by city code LLS we can distinguish two sub classes of models"}, {"timeStart": "00-04-20", "timeEnd": "00-04-24", "sentence": "thirty four days keeps the pattern approach"}, {"timeStart": "00-04-24", "timeEnd": "00-04-30", "sentence": "and do any outputs are explicitly used to perform relation k suffocation of every pair"}, {"timeStart": "00-04-30", "timeEnd": "00-04-33", "sentence": "imitation wood days"}, {"timeStart": "00-04-33", "timeEnd": "00-04-36", "sentence": "did ecuador's do not directly share information"}, {"timeStart": "00-04-36", "timeEnd": "00-04-40", "sentence": "I was a shirt less explicit information such as in yeah labour in BS"}, {"timeStart": "00-04-41", "timeEnd": "00-04-45", "sentence": "the mud miss make a relation cultured for each bell field"}, {"timeStart": "00-04-45", "timeEnd": "00-04-47", "sentence": "SME I need to take the finish"}, {"timeStart": "00-04-48", "timeEnd": "00-04-52", "sentence": "however is it a bit he's not feed sequentially but in two steps"}, {"timeStart": "00-04-52", "timeEnd": "00-04-54", "sentence": "entities then relations"}, {"timeStart": "00-04-55", "timeEnd": "00-04-59", "sentence": "so we can see that the civil works that use entity feelings"}, {"timeStart": "00-04-59", "timeEnd": "00-05-02", "sentence": "first sequence tagging in yeah"}, {"timeStart": "00-05-02", "timeEnd": "00-05-04", "sentence": "but also"}, {"timeStart": "00-05-04", "timeEnd": "00-05-07", "sentence": "spend of it and yeah and we can look"}, {"timeStart": "00-05-07", "timeEnd": "00-05-11", "sentence": "and easy on your porch that enables"}, {"timeStart": "00-05-11", "timeEnd": "00-05-13", "sentence": "overlapping entities"}, {"timeStart": "00-05-14", "timeEnd": "00-05-19", "sentence": "and we can also see that civil work used selection"}, {"timeStart": "00-05-21", "timeEnd": "00-05-28", "sentence": "finally on a more original notes Leanne could this claim into a relations section as Messi"}, {"timeStart": "00-05-28", "timeEnd": "00-05-32", "sentence": "each relation corresponds to a template of civil questions"}, {"timeStart": "00-05-32", "timeEnd": "00-05-34", "sentence": "that are on sale when after user"}, {"timeStart": "00-05-34", "timeEnd": "00-05-36", "sentence": "with a belt based q a mood"}, {"timeStart": "00-05-37", "timeEnd": "00-05-41", "sentence": "so if this gives us an overview of purpose models"}, {"timeStart": "00-05-41", "timeEnd": "00-05-43", "sentence": "was very diverse a coach is"}, {"timeStart": "00-05-43", "timeEnd": "00-05-47", "sentence": "and we can not own evolution from incremental cities to share with our cities"}, {"timeStart": "00-05-47", "timeEnd": "00-05-51", "sentence": "she missed a nice early present a tions used by this model"}, {"timeStart": "00-05-51", "timeEnd": "00-05-55", "sentence": "I followed the general and a friend and went form and cuffed"}, {"timeStart": "00-05-55", "timeEnd": "00-05-59", "sentence": "tune in contextual and begins and finally language without please"}, {"timeStart": "00-06-00", "timeEnd": "00-06-09", "sentence": "we can wound now which elements among all the fun proposals are the most useful for and to enter relation"}, {"timeStart": "00-06-09", "timeEnd": "00-06-12", "sentence": "and when to oneself that"}, {"timeStart": "00-06-13", "timeEnd": "00-06-16", "sentence": "recommend benchmarks and common ethics"}, {"timeStart": "00-06-18", "timeEnd": "00-06-23", "sentence": "however civil cities have been used to evaluate and to any relationship"}, {"timeStart": "00-06-24", "timeEnd": "00-06-28", "sentence": "in all cities we generally report to schools"}, {"timeStart": "00-06-28", "timeEnd": "00-06-33", "sentence": "went for n yeah that takes it to a counter the diction of every entity was involved innovation or not"}, {"timeStart": "00-06-33", "timeEnd": "00-06-36", "sentence": "and one for relations lecture"}, {"timeStart": "00-06-36", "timeEnd": "00-06-38", "sentence": "only focus on the relations"}, {"timeStart": "00-06-39", "timeEnd": "00-06-47", "sentence": "so we use places on record in f one for booths but this information is not sufficient to entirely describes erosion city"}, {"timeStart": "00-06-49", "timeEnd": "00-06-52", "sentence": "we could use in cody's identify three different settings"}, {"timeStart": "00-06-53", "timeEnd": "00-06-55", "sentence": "bundles and relax"}, {"timeStart": "00-06-55", "timeEnd": "00-06-58", "sentence": "just fix it in is the most hated"}, {"timeStart": "00-06-58", "timeEnd": "00-07-02", "sentence": "he claims that both the instep and boundaries alcohol"}, {"timeStart": "00-07-02", "timeEnd": "00-07-04", "sentence": "for both and yeah relations section"}, {"timeStart": "00-07-05", "timeEnd": "00-07-07", "sentence": "in the boundary setting"}, {"timeStart": "00-07-07", "timeEnd": "00-07-09", "sentence": "the India a vacation is unchanged"}, {"timeStart": "00-07-09", "timeEnd": "00-07-14", "sentence": "but only the boundaries of religion arguments are taken into account for his lecture"}, {"timeStart": "00-07-15", "timeEnd": "00-07-17", "sentence": "enjoy he likes it"}, {"timeStart": "00-07-17", "timeEnd": "00-07-22", "sentence": "and Mr Kennedy is right if at least where to can is a sign we skyped"}, {"timeStart": "00-07-22", "timeEnd": "00-07-24", "sentence": "with no requirements on the bundles"}, {"timeStart": "00-07-24", "timeEnd": "00-07-31", "sentence": "and yeah easily used to take this vacation with the assumption that entity boundaries are obtained in the supply process"}, {"timeStart": "00-07-32", "timeEnd": "00-07-36", "sentence": "white I think civil revolution city is not problematic in self"}, {"timeStart": "00-07-36", "timeEnd": "00-07-39", "sentence": "this mess application brings confusion"}, {"timeStart": "00-07-39", "timeEnd": "00-07-42", "sentence": "and it does lead to civil inquired compassion"}, {"timeStart": "00-07-43", "timeEnd": "00-07-50", "sentence": "so in our paper we identify identify civil license with respect to previous works"}, {"timeStart": "00-07-50", "timeEnd": "00-07-52", "sentence": "so she felt schools"}, {"timeStart": "00-07-52", "timeEnd": "00-07-58", "sentence": "easy confusion between boundaries and cities mainly on age data sets"}, {"timeStart": "00-07-59", "timeEnd": "00-08-04", "sentence": "and no issue is that hesitation due to a different position"}, {"timeStart": "00-08-04", "timeEnd": "00-08-12", "sentence": "and this issue is particularly critical for that assets which are not publicly available such as data sets"}, {"timeStart": "00-08-12", "timeEnd": "00-08-15", "sentence": "and for which only purpose is in skipped thou shall"}, {"timeStart": "00-08-16", "timeEnd": "00-08-21", "sentence": "feels are not they can also be confusion between Michael and Michael likes cars as"}, {"timeStart": "00-08-21", "timeEnd": "00-08-26", "sentence": "this norm details between evaluation setups even between the tests"}, {"timeStart": "00-08-27", "timeEnd": "00-08-33", "sentence": "and we can also mention more Institute issues that are less specific to and to and relations"}, {"timeStart": "00-08-33", "timeEnd": "00-08-36", "sentence": "Eisenberg confusing different crane and Des please"}, {"timeStart": "00-08-36", "timeEnd": "00-08-40", "sentence": "using an easier time speech to sleep nigga"}, {"timeStart": "00-08-40", "timeEnd": "00-08-42", "sentence": "additional development data"}, {"timeStart": "00-08-42", "timeEnd": "00-08-48", "sentence": "and finally both teams are result of a single best horn while she's average genius metti boys"}, {"timeStart": "00-08-48", "timeEnd": "00-08-54", "sentence": "we invite you to find more details in the paper we are where we bought a choice of"}, {"timeStart": "00-08-54", "timeEnd": "00-08-56", "sentence": "there be cheese eggs and favourite of his"}, {"timeStart": "00-08-57", "timeEnd": "00-09-03", "sentence": "so now in the final part of this work we propose a small and a quick study to quantify the impact"}, {"timeStart": "00-09-03", "timeEnd": "00-09-05", "sentence": "of sixty five seats in irrigation"}, {"timeStart": "00-09-06", "timeEnd": "00-09-11", "sentence": "we mainly focus on combining boundaries and sweet Jesus which is the most common mistake"}, {"timeStart": "00-09-11", "timeEnd": "00-09-18", "sentence": "we choose a simple and filling bass line inspired by a recent walk off about twenty twenty"}, {"timeStart": "00-09-19", "timeEnd": "00-09-25", "sentence": "but to obtain his a small boy Europe on the Eve of different kinds we've added two elements"}, {"timeStart": "00-09-25", "timeEnd": "00-09-27", "sentence": "so first the encoder is as a belt"}, {"timeStart": "00-09-27", "timeEnd": "00-09-31", "sentence": "it mouth as you know that it is same reason contextual and beatings"}, {"timeStart": "00-09-31", "timeEnd": "00-09-35", "sentence": "and second and yeah his eyes are cast as splendidly situation"}, {"timeStart": "00-09-38", "timeEnd": "00-09-45", "sentence": "with a formal experiments on a sofa and Canelo for as the two main benchmarks for and to enter relations section"}, {"timeStart": "00-09-46", "timeEnd": "00-09-52", "sentence": "so first hour muji suggests that suspended and yet not beneficial"}, {"timeStart": "00-09-52", "timeEnd": "00-09-54", "sentence": "and I know none of our being entities"}, {"timeStart": "00-09-54", "timeEnd": "00-09-56", "sentence": "I conclude to weather blamed"}, {"timeStart": "00-09-58", "timeEnd": "00-10-03", "sentence": "we can see that language method painting is the main source for improved results"}, {"timeStart": "00-10-05", "timeEnd": "00-10-10", "sentence": "we evaluate the sleep off senior boundaries girls instead of six girls"}, {"timeStart": "00-10-10", "timeEnd": "00-10-15", "sentence": "research in a pleasure to present native import ball game on a four five"}, {"timeStart": "00-10-15", "timeEnd": "00-10-19", "sentence": "which supplies you need as a very limited impact on connect"}, {"timeStart": "00-10-19", "timeEnd": "00-10-22", "sentence": "and this I got less of the different architecture"}, {"timeStart": "00-10-24", "timeEnd": "00-10-29", "sentence": "and this can be explained by one of our loop difference between these two data sets"}, {"timeStart": "00-10-30", "timeEnd": "00-10-36", "sentence": "that is a fact that there is a budget of mapping between a relation and argument types in can l"}, {"timeStart": "00-10-37", "timeEnd": "00-10-39", "sentence": "and this experiment suggest that"}, {"timeStart": "00-10-39", "timeEnd": "00-10-41", "sentence": "have learnt this mapping"}, {"timeStart": "00-10-41", "timeEnd": "00-10-44", "sentence": "I even use it as a shallow yours d"}, {"timeStart": "00-10-45", "timeEnd": "00-10-49", "sentence": "and we can see that to smoke is much more complex in faith"}, {"timeStart": "00-10-50", "timeEnd": "00-10-55", "sentence": "so to conclude we can only cold for a more complete report of the evaluation cities"}, {"timeStart": "00-10-55", "timeEnd": "00-10-58", "sentence": "including every mythic sat a gee"}, {"timeStart": "00-10-58", "timeEnd": "00-11-00", "sentence": "as well as the city says"}, {"timeStart": "00-11-00", "timeEnd": "00-11-08", "sentence": "the other wish we are in computer equation with the Yemen AP twenty twenty three Kurdish cheese"}, {"timeStart": "00-11-08", "timeEnd": "00-11-13", "sentence": "but we also feel that it would be beneficial to unity is evaluation settings"}, {"timeStart": "00-11-13", "timeEnd": "00-11-17", "sentence": "they always reporting booth street and bounded his cars"}, {"timeStart": "00-11-17", "timeEnd": "00-11-20", "sentence": "which could enable more meaningful"}, {"timeStart": "00-11-20", "timeEnd": "00-11-22", "sentence": "cause at the set analysis"}, {"timeStart": "00-11-22", "timeEnd": "00-11-30", "sentence": "on the final points we believe that a single reference girl is not enough to analyze and understand the performance of a modern"}, {"timeStart": "00-11-30", "timeEnd": "00-11-35", "sentence": "and we can point to several works that measure for example the impact of the equal of a lap"}, {"timeStart": "00-11-35", "timeEnd": "00-11-37", "sentence": "in spend days tasks"}, {"timeStart": "00-11-37", "timeEnd": "00-11-42", "sentence": "even two Wells presented at this conference on shadow your cheeks"}, {"timeStart": "00-11-42", "timeEnd": "00-11-44", "sentence": "and in you I don't if I should with it"}]}, {"title": "EMNLP 2020 Talk: Are \"ndocumented Workers\" the Same as \"Illegal Aliens\"? Denotation vs. Connotation", "authors": "Albert Webson", "abstract": "", "publicationOrg": "EMNLP", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2010.02976.pdf", "pdfPath": "", "publicationUrl": "https://arxiv.org/pdf/2010.02976.pdf", "codeUrl": "https://github.com/awebson/congressional_adversary", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/V2pdS6Y_8n0", "videoPath": "/data/cache/1/videos/EMNLP 2020 Talk- Are \u201cUndocumented Workers\u201d the Same as \u201cIllegal Aliens\u201d- Denotation vs. Connotation.mp4", "pdfText": "", "videoStruct": [{"timeStart": "00-00-01", "timeEnd": "00-00-02", "sentence": "my name is alber weapon"}, {"timeStart": "00-00-02", "timeEnd": "00-00-06", "sentence": "I am a phd student at brown adversity advice by early public"}, {"timeStart": "00-00-06", "timeEnd": "00-00-09", "sentence": "in this talk I will present our email py paper"}, {"timeStart": "00-00-09", "timeEnd": "00-00-12", "sentence": "are undocumented workers the same as illegal aliens"}, {"timeStart": "00-00-12", "timeEnd": "00-00-15", "sentence": "disentangling denotation connotation in vector spaces"}, {"timeStart": "00-00-16", "timeEnd": "00-00-19", "sentence": "what are denotation connotation and why should we care"}, {"timeStart": "00-00-19", "timeEnd": "00-00-22", "sentence": "before we get philosophical about definitions"}, {"timeStart": "00-00-22", "timeEnd": "00-00-25", "sentence": "I would like to start with a practical example just to illustrate the problem"}, {"timeStart": "00-00-25", "timeEnd": "00-00-29", "sentence": "so suppose we use our top two favorite search engines and ask them"}, {"timeStart": "00-00-29", "timeEnd": "00-00-31", "sentence": "undocumented workers good"}, {"timeStart": "00-00-31", "timeEnd": "00-00-36", "sentence": "then you get results that show overwhelming pura evidence on the economic benefits of immigrants"}, {"timeStart": "00-00-36", "timeEnd": "00-00-39", "sentence": "but suppose instead that you search for"}, {"timeStart": "00-00-39", "timeEnd": "00-00-40", "sentence": "are you illegal aliens good"}, {"timeStart": "00-00-40", "timeEnd": "00-00-43", "sentence": "then you get articles that are statistical p hacking at best"}, {"timeStart": "00-00-43", "timeEnd": "00-00-45", "sentence": "and alright and a phobia worse"}, {"timeStart": "00-00-45", "timeEnd": "00-00-50", "sentence": "so I know this might be an surprising to many of you but let's reset our assumptions for a second"}, {"timeStart": "00-00-50", "timeEnd": "00-00-52", "sentence": "yeah American politics"}, {"timeStart": "00-00-52", "timeEnd": "00-00-54", "sentence": "both and I can workers any league"}, {"timeStart": "00-00-54", "timeEnd": "00-00-57", "sentence": "referred to roughly the same group of people"}, {"timeStart": "00-00-57", "timeEnd": "00-01-02", "sentence": "then why should we get different results when we ask for a representation of the same reference in the real world"}, {"timeStart": "00-01-03", "timeEnd": "00-01-06", "sentence": "so maybe we say that okay I see the problem"}, {"timeStart": "00-01-06", "timeEnd": "00-01-08", "sentence": "these two phrases mean the same thing"}, {"timeStart": "00-01-08", "timeEnd": "00-01-12", "sentence": "so why don't we just use some corruption solution entity linking"}, {"timeStart": "00-01-12", "timeEnd": "00-01-16", "sentence": "and represent them as the same token your vocabulary"}, {"timeStart": "00-01-16", "timeEnd": "00-01-20", "sentence": "but I certain that they have exactly the same thematic meaning is not right either"}, {"timeStart": "00-01-20", "timeEnd": "00-01-24", "sentence": "supposing a different LP setting let's say natural language am friends"}, {"timeStart": "00-01-24", "timeEnd": "00-01-28", "sentence": "suppose we are given a premise Marco Rubio believes that and lockers are bad"}, {"timeStart": "00-01-28", "timeEnd": "00-01-32", "sentence": "and hypotheses Marco Rubio pleased that illegal aliens are bad"}, {"timeStart": "00-01-32", "timeEnd": "00-01-35", "sentence": "then should this be in talman or contradiction"}, {"timeStart": "00-01-35", "timeEnd": "00-01-40", "sentence": "while birds is ninety three percent Hellman but that's just highlights KO overlap"}, {"timeStart": "00-01-40", "timeEnd": "00-01-43", "sentence": "instead let's try your short proof by contradiction here"}, {"timeStart": "00-01-43", "timeEnd": "00-01-49", "sentence": "suppose the contrary that and Dunkin workers illegal aliens really have exactly the same minute"}, {"timeStart": "00-01-49", "timeEnd": "00-01-54", "sentence": "then we should be able to substitute the two phases between the two sentences"}, {"timeStart": "00-01-54", "timeEnd": "00-01-57", "sentence": "but then the two sentences which is trivially into each other"}, {"timeStart": "00-01-57", "timeEnd": "00-02-01", "sentence": "and we know for fact that it's not a trivial talent"}, {"timeStart": "00-02-01", "timeEnd": "00-02-04", "sentence": "so these two phases can not be interchangeable"}, {"timeStart": "00-02-04", "timeEnd": "00-02-06", "sentence": "and thus they cannot have the same sman YK meaning"}, {"timeStart": "00-02-07", "timeEnd": "00-02-09", "sentence": "so many of you may have recognized that"}, {"timeStart": "00-02-09", "timeEnd": "00-02-11", "sentence": "this is not a new argument at all"}, {"timeStart": "00-02-11", "timeEnd": "00-02-16", "sentence": "in fact he was popularized by Fergus famous sensor reference paper in eighteen ninety four"}, {"timeStart": "00-02-16", "timeEnd": "00-02-20", "sentence": "in which he argued that manic meaning must have at least two components"}, {"timeStart": "00-02-20", "timeEnd": "00-02-22", "sentence": "one being what you're referring to"}, {"timeStart": "00-02-22", "timeEnd": "00-02-25", "sentence": "the other being how are you presenting that reference"}, {"timeStart": "00-02-25", "timeEnd": "00-02-29", "sentence": "but despite the problem was known way back in eighteen ninety four"}, {"timeStart": "00-02-29", "timeEnd": "00-02-32", "sentence": "most methods is thin really take the non reference part too seriously"}, {"timeStart": "00-02-32", "timeEnd": "00-02-37", "sentence": "and they mostly favored theories which reduced everything to just reference and truth conditions"}, {"timeStart": "00-02-39", "timeEnd": "00-02-40", "sentence": "half a century later"}, {"timeStart": "00-02-40", "timeEnd": "00-02-46", "sentence": "folks like Ned block and Gilbert Harman proposed a framework noise conceptual semantics"}, {"timeStart": "00-02-46", "timeEnd": "00-02-52", "sentence": "where they argued that in addition to reference semantics myself so be grounded on mental or psychological concepts"}, {"timeStart": "00-02-52", "timeEnd": "00-02-54", "sentence": "and how he must use such concepts"}, {"timeStart": "00-02-54", "timeEnd": "00-03-00", "sentence": "so for them and I can workers and illegal aliens are the same thing in the external world"}, {"timeStart": "00-03-00", "timeEnd": "00-03-02", "sentence": "but in turn are to our brain"}, {"timeStart": "00-03-02", "timeEnd": "00-03-06", "sentence": "they are encoded as different mental concepts and we use those concepts from Lee"}, {"timeStart": "00-03-06", "timeEnd": "00-03-07", "sentence": "and am under concept"}, {"timeStart": "00-03-07", "timeEnd": "00-03-11", "sentence": "must also be a valid part of the minimum words in addition to reference"}, {"timeStart": "00-03-11", "timeEnd": "00-03-14", "sentence": "so if you take the Union of both the reference and concept"}, {"timeStart": "00-03-14", "timeEnd": "00-03-17", "sentence": "what you get is to factor semantics"}, {"timeStart": "00-03-17", "timeEnd": "00-03-21", "sentence": "and very excitedly it has graduated from armchair linguistics"}, {"timeStart": "00-03-21", "timeEnd": "00-03-26", "sentence": "receiving endorsement from prominent kog num scientists such as Susan Carey"}, {"timeStart": "00-03-26", "timeEnd": "00-03-29", "sentence": "and this forms the basis of our work today"}, {"timeStart": "00-03-29", "timeEnd": "00-03-30", "sentence": "where we argued that"}, {"timeStart": "00-03-30", "timeEnd": "00-03-33", "sentence": "in order to solve the motivating examples earlier"}, {"timeStart": "00-03-33", "timeEnd": "00-03-36", "sentence": "we should explicitly model the two factors are semantics"}, {"timeStart": "00-03-36", "timeEnd": "00-03-43", "sentence": "and we propose a new model that disentangles approaching representation as independent denotation conversation representation"}, {"timeStart": "00-03-43", "timeEnd": "00-03-48", "sentence": "and know real quick that in this paper we called them denotation and connotation"}, {"timeStart": "00-03-48", "timeEnd": "00-03-53", "sentence": "in order to avoid confusion with the technical definitions of reference and concept from existence"}, {"timeStart": "00-03-54", "timeEnd": "00-04-01", "sentence": "we experimented on a word of corpora with different in the Shan condition labels but in the interest of time"}, {"timeStart": "00-04-01", "timeEnd": "00-04-02", "sentence": "this talk only presents the first one"}, {"timeStart": "00-04-03", "timeEnd": "00-04-06", "sentence": "in this experiment our corpses the congressional record"}, {"timeStart": "00-04-06", "timeEnd": "00-04-10", "sentence": "which is the official transcript of the floor speeches of the United States Congress"}, {"timeStart": "00-04-10", "timeEnd": "00-04-15", "sentence": "and here are our denotation is grounded on which legislation each species referring to"}, {"timeStart": "00-04-15", "timeEnd": "00-04-19", "sentence": "and the connotation is going on at the party affiliation of the speaker"}, {"timeStart": "00-04-19", "timeEnd": "00-04-27", "sentence": "because American politics is so sadly polarized these days we can actually reasonably assume that the connotation of every speech"}, {"timeStart": "00-04-27", "timeEnd": "00-04-30", "sentence": "is simply to advance a partisan agenda of the speaker"}, {"timeStart": "00-04-30", "timeEnd": "00-04-34", "sentence": "overall the idea is to emulate a human watching political speeches"}, {"timeStart": "00-04-34", "timeEnd": "00-04-38", "sentence": "where they get this generation connotation labor from the TV subtitles"}, {"timeStart": "00-04-38", "timeEnd": "00-04-41", "sentence": "and that enables humans to graduate"}, {"timeStart": "00-04-41", "timeEnd": "00-04-43", "sentence": "learn to disentangle denotation can"}, {"timeStart": "00-04-43", "timeEnd": "00-04-52", "sentence": "and to implement this we propose an adversarial neural network that learns again independent invitation condition representation and here they are"}, {"timeStart": "00-04-52", "timeEnd": "00-04-57", "sentence": "the full models looks somewhat complicated but I promise is actually quite intuitive"}, {"timeStart": "00-04-57", "timeEnd": "00-05-00", "sentence": "especially since the modest metrical and we only need to go over half of this"}, {"timeStart": "00-05-00", "timeEnd": "00-05-03", "sentence": "so let's focus on the denotation space for now"}, {"timeStart": "00-05-03", "timeEnd": "00-05-07", "sentence": "its goal is to preserve as much denotation information as possible"}, {"timeStart": "00-05-07", "timeEnd": "00-05-10", "sentence": "while removing as much connotation information as possible from the picture"}, {"timeStart": "00-05-11", "timeEnd": "00-05-14", "sentence": "we start with a sentencing court"}, {"timeStart": "00-05-14", "timeEnd": "00-05-16", "sentence": "which in principle could be any newer module"}, {"timeStart": "00-05-16", "timeEnd": "00-05-21", "sentence": "but for reasons we explain them the paper we use a static bag of imbedding in this experiment"}, {"timeStart": "00-05-21", "timeEnd": "00-05-24", "sentence": "initialized with water vac pur trend on the same corpus"}, {"timeStart": "00-05-24", "timeEnd": "00-05-27", "sentence": "and then we feed the encourage sentence"}, {"timeStart": "00-05-27", "timeEnd": "00-05-29", "sentence": "in two two m l p classifier probes"}, {"timeStart": "00-05-29", "timeEnd": "00-05-34", "sentence": "the denotation probe tries to classify the reference that is the legislation under discussion"}, {"timeStart": "00-05-34", "timeEnd": "00-05-40", "sentence": "and thus measuring how much do notation information there is in the sentence encoded by the innovation space"}, {"timeStart": "00-05-40", "timeEnd": "00-05-44", "sentence": "whereas the color vision prob tries to classify partisanship"}, {"timeStart": "00-05-44", "timeEnd": "00-05-48", "sentence": "and does measuring how much can a Tish information there is in the encoded sentence"}, {"timeStart": "00-05-48", "timeEnd": "00-05-51", "sentence": "now because we were actually trying to remove competition here"}, {"timeStart": "00-05-51", "timeEnd": "00-05-54", "sentence": "we construct an adverse or loss"}, {"timeStart": "00-05-54", "timeEnd": "00-06-00", "sentence": "when you combine this two losses together and back proper dad to the innovation space"}, {"timeStart": "00-06-00", "timeEnd": "00-06-08", "sentence": "what you get is that the spaceball tragic old center says such that the commutation prediction is at random"}, {"timeStart": "00-06-08", "timeEnd": "00-06-11", "sentence": "and that is why we had this kale divergence here"}, {"timeStart": "00-06-11", "timeEnd": "00-06-14", "sentence": "this effectively removes any connotation information"}, {"timeStart": "00-06-14", "timeEnd": "00-06-16", "sentence": "from the denotation space"}, {"timeStart": "00-06-16", "timeEnd": "00-06-17", "sentence": "and on the other hand"}, {"timeStart": "00-06-17", "timeEnd": "00-06-23", "sentence": "because the rotation problem is still the usual classification loss so it will encourage the denotation space"}, {"timeStart": "00-06-23", "timeEnd": "00-06-25", "sentence": "to preserve as much dinner his information as possible"}, {"timeStart": "00-06-26", "timeEnd": "00-06-31", "sentence": "and note that the probe themselves are always a greeting updated by their own losses"}, {"timeStart": "00-06-31", "timeEnd": "00-06-34", "sentence": "so they were always diligently measure information"}, {"timeStart": "00-06-34", "timeEnd": "00-06-36", "sentence": "regardless of what the nation in space is doing"}, {"timeStart": "00-06-37", "timeEnd": "00-06-39", "sentence": "so that's why I have the model"}, {"timeStart": "00-06-39", "timeEnd": "00-06-42", "sentence": "and again the other half of exactly symmetrical"}, {"timeStart": "00-06-42", "timeEnd": "00-06-45", "sentence": "where the Cartesian space ball instead preserve connotation"}, {"timeStart": "00-06-45", "timeEnd": "00-06-48", "sentence": "and use an adversary are lost to remove denotation"}, {"timeStart": "00-06-48", "timeEnd": "00-06-52", "sentence": "and lastly we have a reconstruction loss"}, {"timeStart": "00-06-52", "timeEnd": "00-06-53", "sentence": "which ensures that"}, {"timeStart": "00-07-03", "timeEnd": "00-07-05", "sentence": "the only maximized probe assays"}, {"timeStart": "00-07-05", "timeEnd": "00-07-07", "sentence": "and finally"}, {"timeStart": "00-07-07", "timeEnd": "00-07-11", "sentence": "when you combine all this losses together the whole network is trained and to end"}, {"timeStart": "00-07-14", "timeEnd": "00-07-17", "sentence": "wow let's stop the small qualitative example"}, {"timeStart": "00-07-18", "timeEnd": "00-07-23", "sentence": "here is a piece of the preaching space versus our disentangled denotation space"}, {"timeStart": "00-07-23", "timeEnd": "00-07-27", "sentence": "because it's a PC a distance actually matters here and you can see that"}, {"timeStart": "00-07-27", "timeEnd": "00-07-32", "sentence": "and darkening waters illegal aliens are much closer to each other in our dinner vision space"}, {"timeStart": "00-07-32", "timeEnd": "00-07-34", "sentence": "reflecting the fact that these two are"}, {"timeStart": "00-07-34", "timeEnd": "00-07-38", "sentence": "phrases denote the same reference in the external world"}, {"timeStart": "00-07-39", "timeEnd": "00-07-41", "sentence": "here's another favor example my"}, {"timeStart": "00-07-41", "timeEnd": "00-07-43", "sentence": "gummy run healthcare versus public option"}, {"timeStart": "00-07-43", "timeEnd": "00-07-47", "sentence": "again despite their differences in partisan connotations"}, {"timeStart": "00-07-47", "timeEnd": "00-07-52", "sentence": "the most referred the same thing therefore they are much closer in our generation space compared to the patron"}, {"timeStart": "00-07-53", "timeEnd": "00-08-01", "sentence": "there are many more examples please refer to our paper for the follows what I do want to show now is that this aren't just some cheer pic examples"}, {"timeStart": "00-08-01", "timeEnd": "00-08-02", "sentence": "but the action works at scale"}, {"timeStart": "00-08-03", "timeEnd": "00-08-06", "sentence": "here are some randomly sampled partisan wars"}, {"timeStart": "00-08-06", "timeEnd": "00-08-08", "sentence": "color by their policy topic"}, {"timeStart": "00-08-08", "timeEnd": "00-08-13", "sentence": "and here you can see that they're scattered around with no apparent structure in the patron space"}, {"timeStart": "00-08-13", "timeEnd": "00-08-21", "sentence": "however with exactly the same t sni but forward innovation space they're much nicely clustered according to their policy to notation"}, {"timeStart": "00-08-21", "timeEnd": "00-08-24", "sentence": "and meanwhile the same idea works on the connotation spaces wow"}, {"timeStart": "00-08-24", "timeEnd": "00-08-28", "sentence": "so ah hear the same words again just colored by their partisanship"}, {"timeStart": "00-08-28", "timeEnd": "00-08-31", "sentence": "and again no apparent structure in the patron space"}, {"timeStart": "00-08-31", "timeEnd": "00-08-33", "sentence": "but your conversation space"}, {"timeStart": "00-08-33", "timeEnd": "00-08-37", "sentence": "the comes is in structures are as clear as linearly separable"}, {"timeStart": "00-08-37", "timeEnd": "00-08-40", "sentence": "so the pictures are looking pretty good"}, {"timeStart": "00-08-40", "timeEnd": "00-08-43", "sentence": "but how do we quality measure success"}, {"timeStart": "00-08-43", "timeEnd": "00-08-48", "sentence": "walk to do that we introduce a simple and interpret ubl homogeneity metric"}, {"timeStart": "00-08-48", "timeEnd": "00-08-53", "sentence": "based on the percentage of nearest neighbors they're showed the same color and hear colors are policy Natasha"}, {"timeStart": "00-08-53", "timeEnd": "00-08-55", "sentence": "but it could become picture says all"}, {"timeStart": "00-08-55", "timeEnd": "00-09-02", "sentence": "so for example we can measure the homogeneity of the neighborhood around the phrase violence against women act"}, {"timeStart": "00-09-02", "timeEnd": "00-09-04", "sentence": "suppose we take his tough for new neighbors"}, {"timeStart": "00-09-04", "timeEnd": "00-09-07", "sentence": "through which are the same purple color as the center would"}, {"timeStart": "00-09-07", "timeEnd": "00-09-10", "sentence": "that is throughout the four neighborhoods have the same positive note eight"}, {"timeStart": "00-09-10", "timeEnd": "00-09-15", "sentence": "so it's dinner to homogeneity is three divided by four or seventy five per cent"}, {"timeStart": "00-09-15", "timeEnd": "00-09-17", "sentence": "here's another example"}, {"timeStart": "00-09-17", "timeEnd": "00-09-20", "sentence": "let's measure the neighborhood family friendly workplace act"}, {"timeStart": "00-09-20", "timeEnd": "00-09-25", "sentence": "and all Forbes neighbor was had the same green colour as the center award so it's homogenize one hundred percent"}, {"timeStart": "00-09-25", "timeEnd": "00-09-29", "sentence": "and you can see how we can compute this how much natty over a large set of words"}, {"timeStart": "00-09-29", "timeEnd": "00-09-33", "sentence": "to approximate how homogeneous their neighborhood structures are"}, {"timeStart": "00-09-33", "timeEnd": "00-09-35", "sentence": "with respect to some of these are a tribute"}, {"timeStart": "00-09-35", "timeEnd": "00-09-40", "sentence": "and that's how we quantify valet our disentangled representations"}, {"timeStart": "00-09-41", "timeEnd": "00-09-44", "sentence": "when we are measuring denotation how much Nia"}, {"timeStart": "00-09-44", "timeEnd": "00-09-49", "sentence": "as surprisingly our dinner fish in space encodes much modernization structured than the patron"}, {"timeStart": "00-09-49", "timeEnd": "00-09-53", "sentence": "whereas our economy in space removes that denotation structure from the patron"}, {"timeStart": "00-09-53", "timeEnd": "00-09-56", "sentence": "conversely for the chart on the right"}, {"timeStart": "00-09-56", "timeEnd": "00-09-58", "sentence": "measuring contagion homogeneity"}, {"timeStart": "00-09-58", "timeEnd": "00-10-02", "sentence": "we see that the cottage has been successfully encouraged khana to share structure"}, {"timeStart": "00-10-02", "timeEnd": "00-10-06", "sentence": "whereas the denotation space removes that kind of facial structure from the patron"}, {"timeStart": "00-10-07", "timeEnd": "00-10-11", "sentence": "that concludes the intrinsic validation of our models"}, {"timeStart": "00-10-11", "timeEnd": "00-10-15", "sentence": "lastly since we aim to be more than just a theoretical exercise"}, {"timeStart": "00-10-15", "timeEnd": "00-10-19", "sentence": "we also verify the usefulness of our disentangled representations"}, {"timeStart": "00-10-19", "timeEnd": "00-10-22", "sentence": "industry tasks such as information retrieval"}, {"timeStart": "00-10-22", "timeEnd": "00-10-25", "sentence": "so here we are using a corpus of put a ka news articles"}, {"timeStart": "00-10-25", "timeEnd": "00-10-27", "sentence": "collective are some evil task"}, {"timeStart": "00-10-27", "timeEnd": "00-10-30", "sentence": "where each article is labeled by the partisan leaning of the publisher"}, {"timeStart": "00-10-30", "timeEnd": "00-10-34", "sentence": "we also gather a collection of search corries from a recent Gallup poll"}, {"timeStart": "00-10-34", "timeEnd": "00-10-38", "sentence": "and here we set up a pretty standard information ritual pipeline"}, {"timeStart": "00-10-38", "timeEnd": "00-10-41", "sentence": "um we started with a large collection of documents"}, {"timeStart": "00-10-41", "timeEnd": "00-10-47", "sentence": "and then we you so fast tf I DF best model to pre select the top five of the most relevant documents"}, {"timeStart": "00-10-47", "timeEnd": "00-10-49", "sentence": "then we feed them"}, {"timeStart": "00-10-49", "timeEnd": "00-10-52", "sentence": "into a new role relevance matching model"}, {"timeStart": "00-10-52", "timeEnd": "00-10-53", "sentence": "I miss Marco"}, {"timeStart": "00-11-05", "timeEnd": "00-11-12", "sentence": "and we can compare the differences of the retrieved documents using diversity metrics since each documents in labor but the partisanship"}, {"timeStart": "00-11-12", "timeEnd": "00-11-14", "sentence": "so"}, {"timeStart": "00-11-14", "timeEnd": "00-11-17", "sentence": "here higher and decision means more diverse"}, {"timeStart": "00-11-17", "timeEnd": "00-11-18", "sentence": "so you can see that"}, {"timeStart": "00-11-18", "timeEnd": "00-11-24", "sentence": "denotation vectors retrieved them more diverse set of documents for both are top ten and the top one hundred results"}, {"timeStart": "00-11-24", "timeEnd": "00-11-29", "sentence": "and to visualize this we observe that this is especially true for writing in chorus"}, {"timeStart": "00-11-29", "timeEnd": "00-11-31", "sentence": "where if he used the pro trans space"}, {"timeStart": "00-11-31", "timeEnd": "00-11-34", "sentence": "running encourage return overwhelmingly right leaning articles"}, {"timeStart": "00-11-34", "timeEnd": "00-11-36", "sentence": "but have used our dinner Haitian slaves"}, {"timeStart": "00-11-36", "timeEnd": "00-11-40", "sentence": "then the same Christ return articles that are partisan Lea balanced"}, {"timeStart": "00-11-40", "timeEnd": "00-11-43", "sentence": "so just to quickly recap everything"}, {"timeStart": "00-11-43", "timeEnd": "00-11-50", "sentence": "motivated by two factors semantics we propose a model which represents the denotation connotation of language independent Lee"}, {"timeStart": "00-11-50", "timeEnd": "00-11-51", "sentence": "we confirm that our"}, {"timeStart": "00-11-51", "timeEnd": "00-11-53", "sentence": "disentangled representations"}, {"timeStart": "00-11-53", "timeEnd": "00-11-55", "sentence": "encode the desired structure buy both"}, {"timeStart": "00-11-55", "timeEnd": "00-11-59", "sentence": "qualitatively evaluate their clusters urban and political euphemisms"}, {"timeStart": "00-11-59", "timeEnd": "00-12-02", "sentence": "as far as quantity measured there how much needy"}, {"timeStart": "00-12-02", "timeEnd": "00-12-04", "sentence": "and for extras application"}, {"timeStart": "00-12-04", "timeEnd": "00-12-10", "sentence": "we showed that our generation space is capable of improving the viewpoint diversity of documents your information retrieval setting"}, {"timeStart": "00-12-10", "timeEnd": "00-12-12", "sentence": "and that's it"}, {"timeStart": "00-12-12", "timeEnd": "00-12-13", "sentence": "thank you so much for watching"}]}, {"title": "Local Additivity Based Data Augmentation for Semi-supervised NER - A EMNLP 2020 Paper", "authors": "Machine Learning Center at Georgia Tech", "abstract": "", "publicationOrg": "EMNLP", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2010.01677.pdf", "pdfPath": "/data/cache/1/PDFs/LocalAdditivityBasedDataAugmentationforSemisupervisedNERAEMNLP2020Paper.pdf", "publicationUrl": "https://arxiv.org/pdf/2010.01677.pdf", "codeUrl": "https://github.com/GT-SALT/LADA.", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/lSkxbH2P9ow", "videoPath": "/data/cache/1/videos/Local Additivity Based Data Augmentation for Semi-supervised NER - A EMNLP 2020 Paper.mp4", "pdfText": "Named Entity Recognition (NER) that aims to detect the semantic category of entities (e.g., persons, locations, organizations) in unstructured text , is an essential prerequisite for many NLP applications. Being one of the most fundamental and classic sequence labeling tasks in NLP, there have been extensive research from traditional statistical models like Hidden Markov Models  and Conditional Random Fields , to neural network based models such as LSTM-CRF  and BLSTM-CNN-CRF , and to recent pre-training and fine-tuning methods like ELMO , Flair  and BERT . However, most of those models still heavily rely on abundant annotated data to yield the state-of-the-art results , making them hard to be applied into new domains (e.g., social media, medical context or low-resourced languages) that lack labeled data.Different kinds of data augmentation approaches have been designed to alleviate the dependency on labeled data for many NLP tasks, and can be categorized into two broad classes: (1) adversarial attacks at token-levels such as word substitutions  or adding noise , (2) paraphrasing at sentence-levels such as back translations  or submodular optimized models . The former has already been used for NER but struggles to create diverse augmented samples with very few word replacements. Despite being widely utilized in many NLP tasks like text classification, the latter often fails to maintain the labels at the token-level in those paraphrased sentences, thus making it difficult to be applied to NER.We focus on another type of data augmentations called mixup , which was originally proposed in computer vision and performed linear interpolations between randomly sampled image pairs to create virtual training data. ;  adapted the idea to textual domains and have applied it to the preliminary task of text classification. However, unlike classifications where each sentence only has one label, sequence labeling tasks such as NER usually involve multiple interrelated labels in a single sentence. As we found in empirical experiments, it is challenging to directly apply such mixup technique to sequence labeling, and improper interpolations may mislead the model. For instance, random sam-pling in mixup may inject too much noise by interpolating data points far away from each other, hence making it fail on sequence labeling.To fill this gap, we propose a novel method called Local Additivity based Data Augmentation (LADA), in which we constrain the samples to mixup to be close to each other. Our method has two variations: Intra-LADA and Inter-LADA. Intra-LADA interpolates each token's hidden representation with other tokens from the same sentence, which could increase the robustness towards word orderings. Inter-LADA interpolates each token's hidden representation in a sentence with each token from other sentences sampled from a weighted combination of k-nearest neighbors sampling and random sampling, the weight of which controls the delicate trade-off between noise and regularization. To further enhance the performance of learning with limited labeled data, we extend LADA to the semi-supervised setting, i.e., Semi-LADA, by designing a novel consistency loss between unlabeled data and its local augmentations. We conduct experiments on two NER datasets to demonstrate the effectiveness of our LADA based models over state-of-the-art baselines.2 Background  proposed a data augmentation technique called mixup, which trained an image classifier on linear interpolations of randomly sampled image data. Given a pair of data points (x, y) and (x , y ), where x denotes an image in raw pixel space, and y is the label in a one-hot representation, mixup creates a new sample by interpolating images and their corresponding labels:where \u03bb is drawn from a Beta distribution. mixup trains the neural network for image classification by minimizing the loss on the virtual examples. In experiments, the pairs of images data points (x, y) and (x,\u1ef9) are randomly sampled. By assuming all the images are mapped to a low dimension manifold through a neural network, linearly interpolating them creates a virtual vicinity distribution around the original data space, thus improving the generalization performance of the classifier trained on the interpolated samples.Prior work like Snippext , MixText  and AdvAug  generalized the idea to the textual domain by proposing to interpolate in output space , embedding space , or general hidden space  of textual data and applied the technique to NLP tasks such as text classifications and machine translations and achieved significant improvements.Based on the above interpolation based data augmentation techniques, in Section 3.1, we introduced a Local Additivity based Data Augmentation (LADA) for sequence labeling, where creating augmented samples is much more challenging. We continue to describe how to utilize unlabeled data with LADA for semi-supervised NER in Section 3.4.For a given sentence with n tokens x = {x 1 , ..., x n }, denote the corresponding sequence label as y = {y 1 , ..., y n }. In this paper, we use NER as the working example to introduce our model, in which the labels are the entities types. We randomly sample a pair of sentences from the corpus, (x, y) and (x , y ), and then compute the interpolations in the hidden space using a L-layer encoder F(.; \u03b8). The hidden representations of x and x up to the m-th layer are given by:Here h l = {h 1 , ..., h n } refer to the hidden representations at the l-th layer and is the concatenation of token representations at all positions. We use h 0 , h 0 to denote the word embedding of x and x respectively. At the m-th layer, the hidden representations for each token in x are linearly interpolated with each token in x by a ratio \u03bb:where the mixing parameter \u03bb is sampled from a Beta distribution, i.e., \u03bb \u223c Beta(\u03b1, \u03b1). Thenh m is fed to the upper layers:h L can be treated as the hidden representations of a virtual samplex, i.e.,h L = F(x; \u03b8).In the meanwhile, their corresponding labels are linearly added with the same ratio: LADA takes in two sentences, linearly interpolates their hidden states h i and h i at layer m with weight \u03bb intoh i , and then continues forward passing to get encoded representationsh i , which are utilized in downstream tasks where the labels in each task are also mixed with weight \u03bb.The hidden representationsh L are then fed into a classifier p(:, \u03c6) and the loss over all positions is minimized to train the model:Here P mix (x |x) defines the probability of sampling (x , y ) to mix with (x, y). The overall diagram is shown in . Let S = {(x, y)} be the corpus of data samples, then according to ,Note that P mix (x |x) is a uniform distribution that is independent of x. Even though x can be far away from x in the Euclidean space, they are mapped into a low-dimension manifold through a neural network. Interpolating them in the hidden space regularizes the model to perform linearly in the low-dimensional manifold, hence greatly improves tasks such as classification. However, we found empirically in experiments that the above random sampling strategy failed on sequence labeling like NER, leading to worse modeling results than purely supervised learning. Intuitively, sequence labeling is more complicated than sentence classification as it requires learning much more fine-grained information. Labeling a token depends on not only the token itself but also the context. We hypothesize that mixing the sequence x with x changes the context for all tokens and injects too much noise, hence making learning the labels for the tokens challenging. In other words, the relative distance between x and x in the manifold mapped by neural networks is further in sequence labeling than sentence classification (demonstrated in ), which is intuitively understandable as every data point in sentence classification is the pooling over all the tokens in one sentence while every token is a single data point in sequence labeling. Randomly mixing data points far away from each other introduces more noise for sequence labeling. To overcome this problem, we introduce a local additivity based data augmentation approach with two variations, in which we constrain x to be close to x:As stated above, mixing two sequences not only changes the local token representations but also affects the context required to label tokens. To reduce the noises from unrelated sentences, the most direct way is to construct x using the same tokens from x but changing the orders and perform interpolations between them. quence labeling. The dimension of data manifold for sequence labeling is higher than sentence classification, hence the distance between data samples is larger. We constraint x to be close to x in creating interpolated data in LADA.Let Q = Permutations((x, y)) be the set including all possible permutations of x, thenIn this case, each token x i in x is actually interpolated with another token x j in x, while the context is unaltered. By sampling from P Intra , we are essentially turning sequence level interpolation to token level interpolation, thus greatly reducing the complexity of the problem. From another perspective, Intra-LADA generates augmentations with different sentence structures using the same word set, which could potentially increase the model's robustness towards word orderings. Intra-LADA restraints the context from changing, which could be limited in generating diverse augmented data. To overcome that, we propose Inter-LADA, where we sample a different sentence from the training set to perform interpolations.Instead of interpolating within one sentence, Intra-LADA samples a different sentence x from the training set to interpolate with x. To achieve a trade-off between noise and regularization, we sample x through a weighted combination of two strategies: k-nearest neighbors (kNNs) sampling and random sampling:where \u00b5 is the weight of combining two distributions. To get the kNNs, we use sentence-BERT  to map each sentence x into a hidden space, then collect each sentence's kNNs using l 2 distance. For each sentencex, we sample x to mix up from the kNNs with probability \u00b5 and the whole training corpus with a probability 1 \u2212 \u00b5. When x is sampled from the whole training corpus, it may be unrelated to x, introducing large noise but also strong regularization on the model. When x is sampled from the kNNs, x shares similar, albeit different, context with x, thus achieving good signal to noise ratio. By treating \u00b5 as a hyper-parameter, we can control the delicate trade-off between noise and diversity in regularizing the model.To examine why sampling sentences from kNNs decreases the noise and provides meaningful signals to training, we analyze an example with its kNNs in  (1) As it shows, kNNs may contain the same entity words as the original sentence, but in different contexts. The entity types in the neighbor sentences are also changed corresponding to contexts. For example, entity Israel in the third neighbor becomes an organization when surrounded by Radio while it is a location in the original sentence. (2) Contexts from neighbor sentences can help detect the entities of the same type in a given sentence. For example, Lebanon in the second neighbor shares the same type as Israel in the original sentence. Lebanon can resort to the context of the original sentence to detect its entity type. Neighbor sentences may contain the same words but in different forms. For example, the Israeli in the first neighbor sentence is a different form of Israel, which is miscellaneous while Israel is a location in the example sentence. Interpolation with such an example can improve models' ability to recognize words of different forms and their corresponding types.In summary, Inter-LADA can improve both entity learning and context learning by interpolating more diverse data. Note that although we use NER as a working example , LADA can be applied to any sequence labeling models.To further improve the performance of learning with less labeled data, we propose a novel LADAbased approach specifically for unlabeled data. Instead of looking for nearest neighbors, we use backtranslation techniques to generate paraphrases of an unlabeled sentence x u in constructing x u . The paraphrase x u , generated via translating x u to an intermediate language and then translating it back, describes the same content as x u and should be close to x u semantically. However, there is noIsrael plays down fears of war with Syria. Fears of an Israeli operation causes the redistribution of Syrian troops locations in Lebanon . Parliament Speaker Berri: Israel is preparing for war against Syria and Lebanon .Itamar Rabinovich , who as Israel's ambassador to Washington conducted unfruitful negotiations with Syria , told Israel Radio looked like Damascus wanted to talk rather than fight . guarantee that the same entity would appear in the same position in x u and x u . In fact, the number of tokens in x u and x u may not even be the same. For instance, for the sentence \"Rare Hendrix song draft sells for almost $17,000\" and its paraphrased sentence \"A rare Hendrix song design is selling for just under $17,000\", although some words are different, the entity Hendrix keeps unchanged, and there are no extra entities added. That is, both contain one and only one entity (Hendrix) of the same type (Person). Nevertheless, we empirically found that most paraphrases contain the same number of entities (for any specific type) as the original sentence. Inspired by the observation, we propose a new consistency loss to leverage unlabeled data:x u and x u should have the same number of entities for any given entity type. Specifically, for an unlabeled sentence x u and its paraphrase x u , we first guess their token labels with the current model:To avoid predictions being too uniform at the early stage, we sharpen every token prediction y u,i \u2208 y u with a temperature T :where ||.|| 1 denotes the l1-norm. We then add the prediction\u0177 u,i over all tokens in the sentence to denote its total number of entities for each type:Note that\u0177 u,num is the guessed label vector with Cdimensions, where C is the total number of entity types. The i-th element in the\u0177 u,num denotes the total number i-type entity in the sentence.  During training, we use the same procedure to get the number of entities for original and each paraphrase sentence (without sharpening). Assume there are K paraphrases, denote the entity number vector for the k-the paraphrase as\u0177 k u,num . The consistency objective for unlabeled sentence x and its paraphrases is:Here we treat\u0177 u,num as fixed and back-propagate only through\u0177 u,num to train the model. Taking into account the loss objectives for both labeled and unlabeled data (Equation 1 and Equation 5), our Semi-LADA training objective is:where \u03b3 controls the trade-off between the supervised loss term and the unsupervised loss term.We performed experiments on two datasets in different languages: CoNLL 2003  in English and GermEval 2014  in German. The data statistics are shown in . We used the BIO labeling scheme and reported the F1 score. In order to make LADA possible in recent transformerbased models like BERT, we assigned labels to  : The F1 scores on CoNLL 2003 and GermEval 2014 training with varying amounts of the labeled training data (5%, 10%, and 30% of the original training set). There were 10,000 unlabeled data for each dataset which was randomly sampled from the original training set. All the results were averaged over 5 runs. \u2020 denotes our methods.special tokens , , and . Since BERT tokenized a token into one or multiple subtokens, we not only assigned labels to the first subtoken but also to the remaining sub-tokens following the rules: (1) O word: Oxx\u2192OOO, (2) I word: Ixx\u2192III,(3) B word: Bxx\u2192BII, as such kind of assignment will not harm the performance (ablation study was conducted in Section 4.4). During the evaluation, we ignored special tokens and non-first sub-tokens for fair comparisons.In the fully supervised setting, we followed the standard data splits shown in . In the semisupervised setting, we sampled 10,000 sentences in the training set as the unlabeled training data. We adopted FairSeq 1 to implement the back translation. For CoNLL dataset, we utilized German as the intermediate language and English as the intermediate language for GermEval.Our LADA can be applied to any models in standard sequence labeling frameworks. In this work, we applied LADA to two state-of-the-art pre-trained models to show the effectiveness:\u2022 Flair : We used the pretrained Flair embeddings 2 , and a multi-layer BiLSTM-CRF  as the encoder to detect the entities.\u2022 BERT : We loaded the BERT-base-multilingual-cased 3 as the encoder and a linear layer to predict token labels.To demonstrate whether our Semi-LADA works with unlabeled data, we compared it with two recent state-of-the-art semi-supervised NER models:\u2022 VSL-GG-Hier  introduced a hierarchical latent variables models into semi-supervised NER learning.\u2022 MT + Noise  explored different noise strategies including word-dropout, synonym-replace, Gaussian noise and network-dropout in a mean-teacher framework.We also compared our models with another two recent state-of-the-art NER models trained on the whole training set:\u2022 CVT  performed multitask learning and made use of 1 Billion Word Language Model Benchmark as the source of unlabeled data.\u2022 BERT-MRC  formulated the NER as a machine reading comprehension task instead of a sequence labeling problem.For Intra-LADA, as it broke the sentence structures, it cannot be applied to Flair that was based on LSTM-CRF. Thus we only combined it with BERT and only used the labeled data. The mix layer set was {12}. For Inter-LADA, we applied it to Flair and BERT trained with only the labeled data. The mix layer set was {8,9,10}, k in kNNs was 3, and 0.5 was a good start point for tuning \u00b5. Semi-LADA utilized unlabeled data as well. The model was built on BERT. The weight \u03b3 to balance the supervised loss and unsupervised loss was 1.CoNLL GermEval Flair  Token   Multi-task Learning 92.60 -BERT-MRC  Reading Comprehension 93.04 - We evaluated the baselines and our methods using F1-scores on the test set.Utilizing Limited Labeled Data We varied the number of labeled data (made use of 5%, 10%, 30% of labeled sentences in each dataset, which were 700, 1400, 4200 in CoNLL and 1200, 2400, 7200 in GermEval) and the results were shown in . Compared to purely Flair and BERT, applying Intra-LADA and Inter-LADA consistently boosted performances significantly, indicating the effectiveness of creating augmented training data through local linear interpolations. When unlabeled data was introduced, VSL-GG-Hier and MT + Noise performed slightly better than Flair and BERT with 5% labeled data in CoNLL, but pre-trained models (Flair, BERT) still got higher F1 scores when there were more labeled data. Both kinds of BERT + Semi-LADA significantly boosted the F1 scores on CoNLL and GermEval compared to baselines, as Semi-LADA not only utilized LADA on labeled data to avoid overfitting but also combined back translation based data augmentations on unlabeled data for consistent training, which made full use of both labeled data and unlabeled data.Utilizing All the Labeled data  summarized the experimental results on the full training sets (14,987 on CoNLL 2003 and 24,000 on Ger-mEval 2014). Compared to pre-trained Flair and BERT 4 , there were still significant performance gains from utilizing our LADA, which indicated that our proposed data augmentation methods work well even with a large amount of labeled training data (full datasets). We also showed two stateof-the-art NER models' results with different settings, they had better performance mainly due to the multi-task learning with more unlabeled data (CVT) or formulating the NER as reading comprehension problems (BERT + MRC). Note that our LADA was orthogonal to these two models.Loss on the Development Set To illustrate that our LADA could also help the overfitting problem, we plotted the loss on the development set of BERT, BERT + Inter-LADA and BERT + Semi-Inter-LADA on CoNLL and GermEval training with 5% labeled data in . After applying LADA, the loss curve was more stable with training epoch increased, while the loss curve of BERT started increasing after about 10 epochs, indicating that the model might overfit the training data. Such property made LADA a suitable method, especially for semi-supervised learning.Combining Intra&Inter-LADA We further combined Intra-LADA and Inter-LADA with a ratio \u03c0, i.e. data point would be augmented through Intra-LADA with a probability \u03c0 and Inter-LADA with a probability 1 \u2212 \u03c0. In practice, we set the probability 0.3, and kept the settings for each kind of LADA the same. The results are shown in . Through combining two variations, BERT + Intra&Inter-LADA further boosted model performance on both datasets, with an increase of 0.25, 0.04 and 0.19 on CoNLL over BERT + Inter-LADA trained with 5%, 10% and 30% labeled data. We obtained consistent improvement in semi-supervised settings: BERT + Semi-Intra&Inter-LADA improved over BERT + Semi-Inter-LADA trained with 5%, 10% and 30% labeled data on GermEval by +0.05, +0.07 and +0.10. This showed that our Intra-LADA and Inter-LADA can be easily combined by future work to create diverse augmented data to help sequence labeling tasks.Different Sub-token Labeling Strategies To prove that our pre-processing of labeling subtokens for training was reasonable, we compared BERT training with different sub-token labeling strategies in .\"None\" strategy was used in original BERT-Tagger where sub-tokens are ignored during learning. \"Real\" strategy was used in our Inter-LADA where O words' sub-tokens were assigned O (Oxx\u2192OOO), I and B words' sub-tokens were assigned I (Ixx\u2192III, Bxx\u2192BII). \"Repeat\" referred to assigning the original label to each sub-token (Oxx\u2192OOO, Ixx\u2192III, Bxx\u2192BBB). \"O\" means we assigned O to each sub-token (Oxx\u2192OOO, Ixx\u2192IOO, Bxx\u2192BOO). \"Real\" strategy received comparable performances with original BERT models while the other two strategies decreased F1 scores, indicating our strategy mitigated the sub-token labeling issue.Influence of \u00b5 in Inter-LADA We varied the \u00b5 in BERT + Inter-LADA from 0 to 1 to validate that combining kNNs sampling and random sampling in Inter-LADA could achieve the best performance, and the results were plotted in . Note that when \u00b5 = 0, Inter-LADA only did random sampling and it barely improved over BERT largely due to too much noise from interpolations between unrelated sentences. And when \u00b5 = 1, Inter-LADA only did kNNs sampling, and it could get a better F1 score over BERT because of providing mean-  ingful signals to training. BERT + Inter-LADA got the best F1 score with \u00b5 = 0.7 on CoNLL and \u00b5 = 0.5 on GermEval, which indicated the tradeoff between noise and diversity (kNNs sampling with lower noise and random sampling with higher diversity) was necessary for Inter-LADA.5 Related WorkConditional random fields (CRFs)  have been widely used for NER, until recently they have been outperformed by neural networks.  and  are among the first several studies to model sequence labeling using neural networks. Specifically  encoded the input sequence using a unidirectional LSTM  while  instead used a CNN with character level embedding to encode sentences. ;  proposed LSTM-CRFs to combine neural networks with CRFs that aim to leverage both the representation learning capabilities of neural network and structured loss from CRFs. Instead of modeling NER as a sequence modeling problem,  converted NER into a reading comprehension task with an input sentence and a query sentence based on the entity types and achieved competitive performance.There has been extensive previous work  that utilized semi-supervised learning for NER. For instance,  applied variational autoencoders (VAEs) to semi-supervised sequence labeling;  proposed to use discrete labeling sequence as latent variables while  used continuous latent variables in their models. Recently, contextual representations such as ELMO (Peters : F1 score on test set training with 30% labeled data with different \u00b5 in BERT + Inter-LADA. The left Y axis is for CoNLL, and the right Y axis is for Ger-mEval. Dashed lines are the F1 scores of BERT model. et al., 2018b) and BERT  trained on a large amount of unlabeled data have been applied to NER and achieved reasonable performances. Our work is related to research that introduces different data augmentation techniques for NER. For example, Lakshmi Narayan et al. applied noise injection and word dropout and obtained a performance boost,  varied the capitalization of words to increase the robustness to capitalization errors,  augmented traditional models with pretraining on external knowledge bases. In contrast, our work can be viewed as data augmentation in the continuous hidden space without external resources.Mixup  was originally proposed for image classification  as a data augmentation and regularization method , building on which  proposed to interpolate sentences' encoded representations with augmented sentences by tokensubstitutions for text classification. Similarly, Chen et al. (2020a) designed a linguistically informed interpolation of hidden space and demonstrated significant performance increases on several text classification benchmarks.  performed interpolations at the embedding space in sequence-to-sequence learning for machine translations. Different from these previous studies, we sample sentences based on local additivity and utilize mixup for the task of sequence labeling.This paper introduced a local additivity based data augmentation (LADA) methods for Named Entity Recognition (NER) with two different interpolation strategies. To utilize unlabeled data, we introduced a novel consistent training objective combined with LADA. Experiments have been conducted and proved our proposed methods' effectiveness through comparing with several state-ofthe-art models on two NER benchmarks.", "videoStruct": [{"timeStart": "00-00-04", "timeEnd": "00-00-06", "sentence": "phd student from Georgia tech"}, {"timeStart": "00-00-06", "timeEnd": "00-00-09", "sentence": "I'm going to talk about our recent paper"}, {"timeStart": "00-00-10", "timeEnd": "00-00-12", "sentence": "local activity base they talk"}, {"timeStart": "00-00-12", "timeEnd": "00-00-14", "sentence": "for semi supervised and yarn"}, {"timeStart": "00-00-15", "timeEnd": "00-00-19", "sentence": "this work was always strongly wrong and the young from Georgia tech"}, {"timeStart": "00-00-19", "timeEnd": "00-00-23", "sentence": "thank am from google answer toh jaan from city dolls securities"}, {"timeStart": "00-00-24", "timeEnd": "00-00-28", "sentence": "them dancer recognition is one of the most fundamental prayer requests"}, {"timeStart": "00-00-28", "timeEnd": "00-00-30", "sentence": "for Maddie and LP avocations"}, {"timeStart": "00-00-32", "timeEnd": "00-00-34", "sentence": "or even all structured sentence"}, {"timeStart": "00-00-34", "timeEnd": "00-00-36", "sentence": "when you to figure out"}, {"timeStart": "00-00-36", "timeEnd": "00-00-38", "sentence": "but they fix symmetric category"}, {"timeStart": "00-00-39", "timeEnd": "00-00-41", "sentence": "so for example here"}, {"timeStart": "00-00-41", "timeEnd": "00-00-42", "sentence": "Michael Jordan he's a person"}, {"timeStart": "00-00-43", "timeEnd": "00-00-45", "sentence": "Berkeley is a location"}, {"timeStart": "00-00-47", "timeEnd": "00-00-49", "sentence": "most current sand yar models"}, {"timeStart": "00-00-49", "timeEnd": "00-00-51", "sentence": "halfway you rely on a balance of labeled data"}, {"timeStart": "00-00-51", "timeEnd": "00-00-53", "sentence": "to get the state of dark performances"}, {"timeStart": "00-00-54", "timeEnd": "00-00-55", "sentence": "such reliance"}, {"timeStart": "00-00-55", "timeEnd": "00-00-58", "sentence": "heavily prevents them being applied to new settings"}, {"timeStart": "00-00-58", "timeEnd": "00-01-00", "sentence": "like social media contacts"}, {"timeStart": "00-01-00", "timeEnd": "00-01-02", "sentence": "are medical context"}, {"timeStart": "00-01-02", "timeEnd": "00-01-04", "sentence": "or low resource languages"}, {"timeStart": "00-01-05", "timeEnd": "00-01-08", "sentence": "one was was solution is to use the documentation"}, {"timeStart": "00-01-09", "timeEnd": "00-01-11", "sentence": "the limited labeled data set"}, {"timeStart": "00-01-11", "timeEnd": "00-01-13", "sentence": "and get back to performances"}, {"timeStart": "00-01-15", "timeEnd": "00-01-19", "sentence": "there are lots of prior work on tax showed eight augmentation"}, {"timeStart": "00-01-20", "timeEnd": "00-01-23", "sentence": "is using adverse or attacks"}, {"timeStart": "00-01-25", "timeEnd": "00-01-28", "sentence": "however this offer from creating diverse examples"}, {"timeStart": "00-01-29", "timeEnd": "00-01-34", "sentence": "but I don't category is to do the power phrasings at the sentence level"}, {"timeStart": "00-01-35", "timeEnd": "00-01-37", "sentence": "they failed to maintain cooking level labels"}, {"timeStart": "00-01-37", "timeEnd": "00-01-40", "sentence": "which make it impossible to be applied"}, {"timeStart": "00-01-40", "timeEnd": "00-01-42", "sentence": "two sequence level learning"}, {"timeStart": "00-01-44", "timeEnd": "00-01-50", "sentence": "dar why recent categories of the home Nations where they use interpolations in the hidden space"}, {"timeStart": "00-01-50", "timeEnd": "00-01-51", "sentence": "to create virtual samples"}, {"timeStart": "00-01-51", "timeEnd": "00-01-55", "sentence": "I mean this work when focused on such interpolation based methods"}, {"timeStart": "00-01-57", "timeEnd": "00-02-00", "sentence": "using the mix out techniques in textile may"}, {"timeStart": "00-02-00", "timeEnd": "00-02-02", "sentence": "usually randomly sample"}, {"timeStart": "00-02-02", "timeEnd": "00-02-05", "sentence": "examples to be to mix up together"}, {"timeStart": "00-02-05", "timeEnd": "00-02-07", "sentence": "however in this way"}, {"timeStart": "00-02-07", "timeEnd": "00-02-09", "sentence": "it may inject too much noise"}, {"timeStart": "00-02-09", "timeEnd": "00-02-11", "sentence": "or told him that will learn"}, {"timeStart": "00-02-13", "timeEnd": "00-02-16", "sentence": "overcomes limitations we propose Nada"}, {"timeStart": "00-02-16", "timeEnd": "00-02-18", "sentence": "local activity based de ta common"}, {"timeStart": "00-02-18", "timeEnd": "00-02-23", "sentence": "we're reconsidering the samples to mix up to be close to each other"}, {"timeStart": "00-02-23", "timeEnd": "00-02-25", "sentence": "and the way proposed to their racist"}, {"timeStart": "00-02-25", "timeEnd": "00-02-27", "sentence": "internet and the internet"}, {"timeStart": "00-02-28", "timeEnd": "00-02-32", "sentence": "the intro that I refers to interpolating each tokens hit on rotation"}, {"timeStart": "00-02-32", "timeEnd": "00-02-35", "sentence": "with other tokens from the same sentence"}, {"timeStart": "00-02-35", "timeEnd": "00-02-39", "sentence": "and this was achieved by random mutations in the original sentences"}, {"timeStart": "00-02-39", "timeEnd": "00-02-43", "sentence": "internet ACL increase the robustness towards we're ordering"}, {"timeStart": "00-02-44", "timeEnd": "00-02-50", "sentence": "internet refers to enter ploy to each tokens hit on rotation with each token from other synthesis"}, {"timeStart": "00-02-50", "timeEnd": "00-02-53", "sentence": "and doses are some point from"}, {"timeStart": "00-02-53", "timeEnd": "00-02-57", "sentence": "a combination of kenya's neighbours and randomly sample sentences"}, {"timeStart": "00-02-59", "timeEnd": "00-03-04", "sentence": "by sample sentences from their carriers are neighbors we could see"}, {"timeStart": "00-03-04", "timeEnd": "00-03-07", "sentence": "the similar contacts with different aunties"}, {"timeStart": "00-03-07", "timeEnd": "00-03-09", "sentence": "or SE Manti was indifferent"}, {"timeStart": "00-03-12", "timeEnd": "00-03-14", "sentence": "but in different work forms"}, {"timeStart": "00-03-16", "timeEnd": "00-03-18", "sentence": "those informative"}, {"timeStart": "00-03-18", "timeEnd": "00-03-21", "sentence": "labour's cooling hands the year model"}, {"timeStart": "00-03-23", "timeEnd": "00-03-27", "sentence": "furthermore to eat to alleviate the dependence on labelled data"}, {"timeStart": "00-03-27", "timeEnd": "00-03-29", "sentence": "we proposed a semi supervised louder"}, {"timeStart": "00-03-29", "timeEnd": "00-03-33", "sentence": "will we incorporate our label data through consistency chimney"}, {"timeStart": "00-03-33", "timeEnd": "00-03-35", "sentence": "so for our label sentence"}, {"timeStart": "00-03-35", "timeEnd": "00-03-37", "sentence": "my first generate their paraphrase"}, {"timeStart": "00-03-37", "timeEnd": "00-03-40", "sentence": "and then we perform the consistency training"}, {"timeStart": "00-03-41", "timeEnd": "00-03-43", "sentence": "what constraint that"}, {"timeStart": "00-03-43", "timeEnd": "00-03-47", "sentence": "the own label sentences and their paraphrases should have the same number of aunties"}, {"timeStart": "00-03-47", "timeEnd": "00-03-49", "sentence": "for any given empty type"}, {"timeStart": "00-03-53", "timeEnd": "00-03-55", "sentence": "in for now that the internet"}, {"timeStart": "00-03-55", "timeEnd": "00-03-56", "sentence": "are better than"}, {"timeStart": "00-03-56", "timeEnd": "00-03-57", "sentence": "although baseline models"}, {"timeStart": "00-03-57", "timeEnd": "00-03-59", "sentence": "and Sammy supervise the dog"}, {"timeStart": "00-03-59", "timeEnd": "00-04-02", "sentence": "further improve the performance he specially"}, {"timeStart": "00-04-02", "timeEnd": "00-04-03", "sentence": "when they are last labeled data"}, {"timeStart": "00-04-07", "timeEnd": "00-04-10", "sentence": "in this work we propose na da"}, {"timeStart": "00-04-10", "timeEnd": "00-04-15", "sentence": "that can perform interpolations in the hidden space among closely examples to generate augmented data"}, {"timeStart": "00-04-15", "timeEnd": "00-04-18", "sentence": "and we found out sampling strategies of"}, {"timeStart": "00-04-18", "timeEnd": "00-04-21", "sentence": "mixed up for sake of learning are really important"}, {"timeStart": "00-04-21", "timeEnd": "00-04-23", "sentence": "and a re proposed Sammy ladder"}, {"timeStart": "00-04-23", "timeEnd": "00-04-25", "sentence": "designing for"}, {"timeStart": "00-04-25", "timeEnd": "00-04-28", "sentence": "could further improve the performances with limited training data"}, {"timeStart": "00-04-29", "timeEnd": "00-04-32", "sentence": "wait public our code on the gate hub"}, {"timeStart": "00-04-32", "timeEnd": "00-04-34", "sentence": "if you are interested in our work"}, {"timeStart": "00-04-34", "timeEnd": "00-04-38", "sentence": "please check our paper and come to our am no paper in station sessions"}]}, {"title": "[Oral at NeurIPS 2020] A Study on Encodings for Neural Architecture Search (3 min video)", "authors": "Abacus AI", "abstract": "", "publicationOrg": "NeurIPS", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2007.04965.pdf", "pdfPath": "/data/cache/1/PDFs/OralatNeurIPS2020AStudyonEncodingsforNeuralArchitectureSearch3minvideo.pdf", "publicationUrl": "https://arxiv.org/pdf/2007.04965.pdf", "codeUrl": "https://github.com/naszilla/nas-encodings", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/_4K4QDOEZmk", "videoPath": "/data/cache/1/videos/A Study on Encodings for Neural Architecture Search (3 min video).mp4", "pdfText": "In the past few years, the field of neural architecture search (NAS) has seen a steep rise in interest , due to the promise of automatically designing specialized neural architectures for any given problem. Techniques for NAS span evolutionary search, Bayesian optimization, reinforcement learning, gradient-based methods, and neural predictor methods. Many NAS instantiations can be described by the optimization problem min a\u2208A f (a), where A denotes a large set of neural architectures, and f (a) denotes the objective function of interest for a, which is usually a combination of validation accuracy, latency, or number of parameters. A popular approach is to describe each neural architecture a as a labeled directed acyclic graph (DAG), where each node or edge represents an operation.Due to the complexity of DAG structures and the large size of the space, neural architecture search is typically a highly non-convex, challenging optimization problem. A natural consideration when designing a NAS algorithm is therefore, how should we encode the neural architectures to maximize performance? For example, NAS algorithms may involve manipulating or perturbing architectures, or training a model to predict the accuracy of a given architecture; as a consequence, the representation of the DAG-based architectures may significantly change the outcome of these subroutines. The majority of prior work has not explicitly considered this question, opting to use a standard encoding consisting of the adjacency matrix of the DAG along with a list of the operations. Two recent papers have shown that even small changes to the architecture encoding can make a substantial difference in the final performance of the NAS algorithm . It is not obvious how to formally define an encoding for NAS, as prior work defines encodings in different ways, inadvertently using encodings which are incompatible with other NAS algorithms.In this work, we provide the first formal study on NAS encoding schemes, including a theoretical grounding as well as a set of experimental results. We define an encoding as a multi-function from an architecture to a real-valued tensor. We define a number of common encodings from prior work, identifying adjacency matrix-based encodings  and path-based encodings  as two main paradigms. Adjacency matrix approaches represent the architecture as a list of edges and operations, while path-based approaches represent the architecture as a set of paths from the input to the output. We theoretically characterize the scalability of each encoding by quantifying the information loss from truncation. This characterization is particularly interesting for path-based encodings, which we find to exhibit a phase change at r k/n , where r is the number of possible operations, n is the number of nodes, and k is the expected number of edges. In particular, we show that when the size of the path encoding is greater than r 2k/n , barely any information is lost, but below r k/(2n) , nearly all information is lost. We empirically verify these findings.Next, we identify three major encoding-dependent subroutines used in NAS algorithms: sample random architecture, perturb architecture, and train predictor model. We show which of the encodings perform best for each subroutine by testing each encoding within each subroutine for many popular NAS algorithms. Our experiments retroactively provide an ablation study for prior work by disentangling the algorithmic contributions from the encoding-based contributions. We also test the ability of a neural predictor to generalize to new search spaces, using a given encoding. Finally, for encodings in which multiple architectures can map to the same encoding, we evaluate the average standard deviation of accuracies for the equivalence class of architectures defined by each encoding.Overall, our results show that NAS encodings are an important design decision which must be taken into account not only at the algorithmic level, but at the subroutine level, and which can have a significant impact on the final performance. Based on our results, we lay out recommendations for which encodings to use within each NAS subroutine. Our experimental results follow the guidelines in the recently released NAS research checklist . In particular, we experiment on two popular NAS benchmark datasets, and we release our code.Our contributions. We summarize our main contributions below.\u2022 We demonstrate that the choice of encoding is an important, nontrivial question that should be considered not only at the algorithmic level, but at the subroutine level. \u2022 We give a theoretical grounding for NAS encodings, including a characterization of the scalability of each encoding. \u2022 We give an experimental study of architecture encodings for NAS algorithms, disentangling the algorithmic contributions from the encoding-based contributions of prior work, and laying out recommendations for best encodings to use in different settings as guidance for future work.Our work gives a study on encodings for neural architecture search, with the goal of helping future researchers improve their NAS algorithms. Therefore, this work may not have a direct impact on society, since it is two levels of abstraction from real applications, but it can indirectly impact society.As an example, our work may inspire the creation of a new state-of-the-art NAS algorithm, which is then used to improve the performance of various deep learning algorithms, which can have both beneficial and detrimental uses (e.g. optimizers that reduce CO 2 emissions, or deep fake generators). Due to the recent push for the AI community to be more conscious and prescient about the societal impact of its work , we are hoping that future AI models, including ones influenced by our work, will have a positive impact on society.Neural architecture search. NAS has been studied for at least two decades and has received significant attention in recent years . Some of the most popular techniques for NAS include evolutionary algorithms , reinforcement learning , Bayesian optimization , gradient descent , neural predictors , and local search . Recent papers have highlighted the need for fair and reproducible NAS comparisons . See the recent survey  for more information on NAS research.Encoding schemes. Most prior NAS work has used the adjacency matrix encoding, , which consists of the adjacency matrix together with a list of the operations on each node. A continuous-valued variant has been shown to be more effective for some NAS algorithms . The path encoding is a popular choice for neural predictor methods , and it was shown that truncating the path encoding leads to a small information loss . Some prior work uses graph convolutional networks (GCN) as a subroutine in NAS , which requires retraining for each new dataset or search space. Other work has used intermediate encodings to reduce the complexity of the DAG , or added summary statistics to the encoding of feedforward networks . To the best of our knowledge, no paper has conducted a formal study of encodings involving more than two encodings.We denote a set of neural architectures a by A (called a search space), and we define an objective function : A \u2192 R, where (a) is typically a combination of the accuracy and the model complexity. We define a neural architecture encoding as an integer d and a multifunction e : A \u2192 R d from a set of neural architectures A to a d-dimensional Euclidean space R d , and we define a NAS algorithm A as a procedure which takes as input a triple (A, , e), and outputs an architecture a, with the goal that (a) is as close to max a\u2208A (a) as possible. Based on this definition, we consider an encoding e to be a fixed transformation, independent of . In particular, NAS components that use to learn a transformation of an input architecture (such as graph convolutional networks or autoencoders), are considered part of the NAS algorithm rather than the encoding. This is consistent with prior definitions of encodings .   We define eight encodings split into two popular paradigms: adjacency matrix-based and pathbased encodings. We assume that each architecture is represented by a DAG with at most n nodes, at most k edges, at most P paths from input to output, and q choices of operations on each node. We focus on the case where nodes represent operations, though our analysis extends similarly to formulations where edges represent operations. Most of the following encodings have been defined in prior work , and we will see in the next section that each encoding is useful for some part of the NAS pipeline.Adjacency matrix encodings. We first consider a class of encodings that are based on representations of the adjacency matrix. These are the most common types of encodings used in current NAS research.\u2022 The one-hot adjacency matrix encoding is created by row-major vectorizing (i.e. flattening) the architecture adjacency matrix and concatenating it with a list of node operation labels. Each position in the operation list is a single integer-valued feature, where each operation is denoted by a different integer. The total dimension is n(n \u2212 1)/2 + n. See .1.\u2022 In the categorical adjacency matrix encoding, the adjacency matrix is first flattened (similar to the one-hot encoding described previously), and is then defined as a list of the indices each of which specifies one of the n(n \u2212 1)/2 possible edges in the adjacency matrix. To ensure a fixed length encoding, each architecture is represented by k features, where k is the maximum number of possible edges. We again concatenate this representation with a list of operations, yielding a total dimensionality of k + n. See .1.\u2022 Finally, the continuous adjacency matrix encoding is similar to the one-hot encoding, but each of the features for each edge can take on any real value in [0, 1], rather than just {0, 1}. We also add a feature representing the number of edges, 1 \u2264 K \u2264 k. The list of operations is encoded the same way as before. The architecture is created by choosing the K edges with the largest continuous features. The dimension is n(n \u2212 1)/2 + n + 1. The disadvantage of adjacency matrix-based encodings is that nodes are arbitrarily assigned indices in the matrix, which means one architecture can have many different representations (in other words, e \u22121 is not onto). See Path-based encodings. Path-based encodings are representations of a neural architecture that are based on the set of paths from input to output that are present within the architecture DAG.\u2022 The one-hot path encoding is created by giving a binary feature to each possible path from the input node to the output node in the DAG (for example:The truncated one-hot path encoding, simply truncates this encoding to only include paths of length x. The new dimension is x i=0 q i .\u2022 The categorical path encoding, is defined as a list of indices each of which specifies one of the n i=0 q i possible paths. See .1.\u2022 The continuous path encoding consists of a real-valued feature [0, 1] for each potential path, as well as a feature representing the number of paths. Just like the one-hot path encoding, the continuous path encoding can be truncated.Path-based encodings have the advantage that nodes are not arbitrarily assigned indices, and also that isomorphisms are automatically mapped to the same encoding. Path-based encodings have the disadvantage that different architectures can map to the same encoding (e is not onto). See In this section, we discuss the scalability of the NAS encodings with respect to architecture size. We focus on the one-hot variants of the encodings, but our analysis extends to all encodings. We show that the path encoding can be truncated significantly while maintaining its performance, while the adjacency matrix cannot be truncated at all without sacrificing performance, and we back up our theoretical results with experimental observations in the next section. In prior work, the one-hot path encoding has been shown to be effective on smaller benchmark NAS datasets , but it has been questioned whether its exponential \u0398(q n ) length allows it to perform well on very large search spaces . However, a counter-arguement is as follows. The vast majority of features correspond to single line paths using the full set of nodes. This type of architecture is not common during NAS algorithms, nor is it likely to be effective in real applications. Prior work has made the first steps in showing that truncating the path encoding does not harm the performance of NAS algorithms .Consider the popular sample random architecture method: given n, r, and k \u2264 n(n\u22121), (1) choose one of r operations for each node from 1 to n; (2) for all i < j, add an edge from node i to node j with probability 2k n(n\u22121) ; (3) if there is no path from node 1 to node n, goto . Given a random graph G n,k,r outputted by this method, let a n,k, denote the expected number of paths from node 1 to node n of length in G n,k,r . We defineGiven n < k < n(n \u2212 1)/2 and 0 < x < n, b(k, x) represents the expected fraction of paths of length at most x in G n,k,r in expectation. Say that we truncate the path encoding to only include paths of length at most x. If b(k, x) is very close to one, then the truncation will result in very little information loss because nearly all paths in a randomly drawn architecture are length at most x with high probability. However, if b(k, x) is bounded away from 1 by some constant, there may not be enough information in the truncated path encoding to effectively run a NAS algorithm.Prior work has shown that b(k, x) > 1 \u2212 1/n 2 when k < n + O(1) and x > log n . However, no bounds for b(k, x) are known when k is larger than a constant added to n. Now we present our main result for the path encoding, which gives a full characterization of b(k, x) up to constant factors. Interestingly, we show that b(k, x) exhibits a phase transition at x = k/n. What this means is, for the purposes of NAS, truncating the path encoding to length r k/n contains almost exactly the same information as the full path encoding, and it cannot be truncated any smaller. In particular, if k \u2264 n log n, the truncated path encoding can be length n, which is smaller than the one-hot adjacency matrix encoding. We give the full details of the proofs from this section in Appendix A.Theorem 4.1. Given 10 \u2264 n \u2264 k \u2264 n(n\u22121), and c > 3, forProof sketch. Let G n,k,r denote a random graph after step (2) of sample random architecture. Then G n,k,r may not contain a path from node 1 to node n. Let a n,k, denote the expected number of paths of length in G n,k,r . Say that a graph is valid if it contains a path from node 1 to node n. Then a n,k, = 0 \u2022 (1 \u2212 P (G n,k,r is valid)) + a n,k, \u2022 P (G n,k,r is valid), so a n,k, = a n,k, /P (G n,k,r is valid). ThenThis is because on a path from node 1 to n of length , there are n\u22122 \u22121 choices of intermediate nodes from 1 to n. Once the nodes are chosen, we need all edges between the nodes to exist, and each edge exists independently with probability 2 n(n\u22121) \u2022 k. Then we use the well-known binomial inequalities n \u2264 n \u2264 en to finish the claim.To prove the first part of Theorem 4.1, given x > 2ec \u2022 k n , we must upper bound n =x+1 a n,k, and lower bound x =1 a n,k, . To lower bound x =1 a n,k, , we use x > 2ec \u2022 k n with the claim:We also have a n,k,1 = 2k n(n\u22121) because there is just one path of length 1: the edge from the input node to the output node. Therefore, we haveThe proof of the second part of Theorem 4.1 uses similar techniques.In .2, we plot b(k, x) for NASBench-101, which supports Theorem 4.1. Next, we may ask whether the one-hot adjacency matrix encoding can be truncated. However, even removing one bit from the adjacency matrix encoding can be very costly, because each single edge makes the difference between a path from the input node to the output node vs. no path from the input node to the output node. In the next theorem, we show that the probability of a random graph containing any individual edge is at least 2k/(n(n \u2212 1)). Therefore, truncating the adjacency matrix encoding even by a single bit results in significant information loss. In the following theorem, let E n,k,r denote the edge set of G n,k,r . Given 1 \u2264 z \u2264 n(n \u2212 1)/2, we slightly abuse notation by writing z \u2208 E n,k,r if the edge with index z in the adjacency matrix is in E n,k,r .Proof. Recall that sample random architecture adds each edge with probability 2k/(n(n \u2212 1)) and rejects in step (3) if there is no path from the input to the output. Define G n,k,r and valid as in the proof of Theorem 4.1 and let E n,k,r denote the edge set of G n,k,r . Thenwhere the first equality comes from Bayes' theorem, and the inequality follows because there is a natural bijection \u03c6 from graphs with z to graphs without z given by removing z, where G is valid if \u03c6(G) is valid but the reverse does not hold. Therefore,.Our theoretical results show that the path encoding can be heavily truncated, while the adjacency matrix cannot be truncated. In the next section, we verify this experimentally ( .2).In this section, we present our experimental results. All of our experiments follow the Best Practices for NAS checklist . We discuss our adherence to these practices in Appendix B. In particular, we release our code at https://github.com/naszilla/nas-encodings. We run experiments on two NAS benchmark datasets which we describe below.The NASBench-101 dataset  consists of approximately 423,000 neural architectures pretrained on CIFAR-10. The search space is a cell consisting of 7 nodes. The first node is the input, and the last node is the output. The middle five nodes can take one of three choices of operations, and there can be at most 9 edges between the 7 nodes. The NASBench-201 dataset  consists of 15625 neural architectures separately trained on each of CIFAR-10, CIFAR-100, and ImageNet16-120. The search space consists of a cell which is a complete directed acyclic graph with 4 nodes. Each edge takes an operation, and there are five possible operations.We split up our first set of experiments based on the three encoding-dependent subroutines: sample random architecture, perturb architecture, and train predictor model. These three subroutines are the only encoding-dependent building blocks necessary for many NAS algorithms.Sample random architecture. Most NAS algorithms use a subroutine to draw an architecture randomly from the search space. Although this operation is more generally parameterized by a distribution over the search space, it is often instantiated with the choice of architecture encoding. Given an encoding, we define a subroutine by sampling each feature uniformly at random. We also compare to sampling each architecture uniformly at random from the search space (which does not correspond to any encoding). Note that sampling architectures uniformly at random can be very computationally intensive. It is much easier to sample features uniformly at random.Perturb architecture. Another common subroutine in NAS algorithms is to make a small change to a given architecture. The type of modification depends on the encoding. For example, a perturbation might be to change an operation, add or remove an edge, or add or remove a path. Given an encoding and a mutation factor m, we define a perturbation subroutine by resampling each feature of the encoding uniformly at random with a fixed probability, so that m features are modified on average.Train predictor model. Many families of NAS algorithms use a subroutine which learns a model based on previously queried architectures. For example, this can take the form of a Gaussian process within Bayesian optimization (BO), or, more recently, a neural predictor model . In the case of a Gaussian process model, the algorithm uses a distance metric defined on pairs of neural architectures, which is typically chosen as the edit distance between architecture encodings . In the case of a neural predictor, the encodings of the queried architectures are used as training data, and the goal is typically to predict the accuracy of unseen architectures.We run multiple experiments for each encoding-dependent subroutine listed above. Many NAS algorithms use more than one subroutine, so in each experiment, we fix the encodings for all subroutines except for the one we are testing. For each NAS subroutine, we experiment on algorithms that depend on the subroutine. In particular, for random sampling, we run experiments on the Random Search algorithm. For perturb architecture, we run experiments on regularized evolution  and local search . For train predictor model, we run experiments on BO, testing five  encodings that define unique distance functions, as well as NASBOT  (which does not correspond to an encoding). We also train a neural predictor model using six different encodings. Since this runs in every iteration of a NAS algorithm , we plot the mean absolute error on the test set for different sizes of training data. Finally, we run experiments on BANANAS , varying all three subroutines at once. We directly used the open source code for each algorithm. Details on the implementations for each algorithm are discussed in Appendix B. In each experiment, we report the test error of the neural network with the best validation error after time t, for t up to 130 TPU hours. We run at 300 trials for each algorithm and record the mean test errors. See .1 for the results on NASBench-101. We present more experiments for NASBench-201 in Appendix B, seeing largely the same trends.Depending on the subroutine, two encodings might be functionally equivalent, which is why not all encodings appear in each experiment (for example, in local search, there is no difference between one-hot and categorical encodings). There is no overall best encoding; instead, each encoding has varied performance for each subroutine, and the results in .1 act as a guideline for which encodings to use in which subroutines. For example, the one-hot adjacency matrix encoding performs well in most settings, but is quite poor in the neural predictor subroutine. Categorical, one-hot, adjacency-based, path-based, and continuous encodings are all best in certain settings. Some of our findings explain the success of prior algorithms, e.g., regularized evolution using the categorical adjacency encoding, and BANANAS using the path encoding in the meta neural network. We also show that combining the best encodings for each subroutine in BANANAS yields the best performance.In .1, Trunc. Path denotes the path encoding truncated from  the full path encoding is more likely to add uncommon paths that do not improve accuracy. We also evaluate the effect of truncating the one-hot adjacency matrix encoding on regularized evolution, from the full 31 bits (on NASBench-101) to 0 bits, and the path encoding from 31 bits (out of 364) to 0 bits. See .2. The path encoding is much more robust to truncation, consistent with Theorems 4.1 and 4.2.Outside search space experiment. In the set of experiments above, we tested the effect of encodings on a neural predictor model by computing the mean absolute error between the predicted vs. actual errors on the test set, and also by evaluating the performance of BANANAS when changing the encoding of its neural predictor model. The latter experiment tests the predictor model's ability to predict the best architectures, not just all architectures on average. We take this one step further and test the ability of the neural predictor to generalize beyond the search space on which it was trained. We set up the experiment as follows. We define the training search space as a subset of NASBench-101: architectures with at most 6 nodes and 7 edges. We define the disjoint test search space as architectures with 6 nodes and 7 to 9 edges. The neural predictor is trained on 1000 architectures and predicts the validation loss of the 5000 architectures from the test search space. We evaluate the losses of the ten architectures with the highest predicted validation loss. We run 200 trials for each encoding and average the results. See . The adjacency encoding performed the best. An explanation is that for the path encoding, there are features (paths) in architectures from the test set that do not exist in the training set. This is not the case for the adjacency encoding: all features (edges) from architectures in the test set have shown up in the training set.Equivalence class experiments. Recall that the path encoding function e is not onto (see .1). In general, this is not desirable because information is lost when two architectures map to the same encoding. However, if the encoding function only maps architectures with similar accuracies to the same encoding, then the behavior is beneficial. On the NASBench-101 dataset, we compute the path encoding of all 423k architectures, and then we compute the average standard deviation of accuracies among architectures with the same encoding (i.e., we look at the standard deviations within the equivalence classes defined by the encoding). See .2. The result is an average standard deviation of 0.353%, compared to the 5.55% standard deviation over the entire set of architectures.  In this paper, we give the first formal study of encoding schemes for neural architecture search. We define eight different encodings and characterize the scalability of each one. We then identify three encoding-dependent subroutines used by NAS algorithms, sample random architecture, perturb architecture, and train predictor model, and we run experiments to find the best encoding for each subroutine in many popular algorithms. We also conduct experiments on the ability of a neural predictor to generalize beyond the training search space, given each encoding. Our experimental results allow us to disentangle the algorithmic and encoding-based contributions of prior work, and act as a guideline for the encodings to use in future work. Overall, we show that encodings are an important, nontrivial design decision in the field of NAS. Designing and testing new encodings is an exciting next step.", "videoStruct": [{"timeStart": "00-00-00", "timeEnd": "00-00-07", "sentence": "I'm calling wait and I'll be presenting our spotlight paper an encoding for neural architecture search"}, {"timeStart": "00-00-07", "timeEnd": "00-00-11", "sentence": "which is joint work with really nice swinger Sam Nolan and Josh solve on e"}, {"timeStart": "00-00-11", "timeEnd": "00-00-19", "sentence": "most NES algorithms search over a set of Dag based architectures which must be encoded in some way to be used by the algorithm"}, {"timeStart": "00-00-19", "timeEnd": "00-00-24", "sentence": "a couple of recent works have shown that this encoding can have a large effect on the performance of nass"}, {"timeStart": "00-00-24", "timeEnd": "00-00-27", "sentence": "and so he gave the first formal study on nursing coatings"}, {"timeStart": "00-00-28", "timeEnd": "00-00-33", "sentence": "we start by formally defining eight different encoding based on two paradigms"}, {"timeStart": "00-00-33", "timeEnd": "00-00-36", "sentence": "adjacent sea matrix based in coatings and path basement coding"}, {"timeStart": "00-00-36", "timeEnd": "00-00-40", "sentence": "four of them are shown here which are one hot and categorical variance"}, {"timeStart": "00-00-41", "timeEnd": "00-00-45", "sentence": "we note that the path based in coatings are not one to one function"}, {"timeStart": "00-00-45", "timeEnd": "00-00-48", "sentence": "and for the adjacent sea matrix based in coatings"}, {"timeStart": "00-00-48", "timeEnd": "00-00-50", "sentence": "the inverse functions are not one"}, {"timeStart": "00-00-50", "timeEnd": "00-00-52", "sentence": "because of this difference"}, {"timeStart": "00-00-52", "timeEnd": "00-00-55", "sentence": "many prior announce algorithms unknowingly used"}, {"timeStart": "00-00-55", "timeEnd": "00-00-59", "sentence": "annoyingly using cuttings that are not compatible with other neurons"}, {"timeStart": "00-00-59", "timeEnd": "00-01-04", "sentence": "therefore in order to experimentally compare these eight encoding"}, {"timeStart": "00-01-04", "timeEnd": "00-01-06", "sentence": "we must go to this sub routine level"}, {"timeStart": "00-01-06", "timeEnd": "00-01-13", "sentence": "we identified three sub routines that are the only encoding dependent building blocks necessary for many nass algorithms"}, {"timeStart": "00-01-13", "timeEnd": "00-01-17", "sentence": "sample around him architecture are you drawing in architecture at random"}, {"timeStart": "00-01-17", "timeEnd": "00-01-22", "sentence": "perturbed architecture in other words mutating mutating in architecture"}, {"timeStart": "00-01-22", "timeEnd": "00-01-28", "sentence": "and finally a trained predictor model I using a surrogate such as a gas in process or neural network"}, {"timeStart": "00-01-28", "timeEnd": "00-01-31", "sentence": "to predict to the performance of untrained architectures"}, {"timeStart": "00-01-32", "timeEnd": "00-01-38", "sentence": "now for each sub routine we identify a couple of nice algorithms which heavily make use of that sub routine"}, {"timeStart": "00-01-38", "timeEnd": "00-01-42", "sentence": "and then run an experiment trying out all eight encoding for that sub green"}, {"timeStart": "00-01-42", "timeEnd": "00-01-46", "sentence": "for example for the perturb architecture sub routine"}, {"timeStart": "00-01-46", "timeEnd": "00-01-48", "sentence": "regularize devolution and local search"}, {"timeStart": "00-01-48", "timeEnd": "00-01-51", "sentence": "make heavy use of perturbing architectures"}, {"timeStart": "00-01-51", "timeEnd": "00-01-55", "sentence": "we find that the adjacent sea matrix encoding were expressed here"}, {"timeStart": "00-01-55", "timeEnd": "00-01-59", "sentence": "and general we find that path based encoding are best for predictor models"}, {"timeStart": "00-01-59", "timeEnd": "00-02-03", "sentence": "and adjacent sea matrix based in coatings are best for the other two"}, {"timeStart": "00-02-03", "timeEnd": "00-02-06", "sentence": "now a briefly mention the rest of our results"}, {"timeStart": "00-02-06", "timeEnd": "00-02-08", "sentence": "for the path based encoding"}, {"timeStart": "00-02-08", "timeEnd": "00-02-11", "sentence": "not being one to one can actually be a benefit"}, {"timeStart": "00-02-11", "timeEnd": "00-02-13", "sentence": "since architectures with the same encoding"}, {"timeStart": "00-02-13", "timeEnd": "00-02-16", "sentence": "tend to have very similar accuracies"}, {"timeStart": "00-02-16", "timeEnd": "00-02-20", "sentence": "finally we theoretically and empirically show that"}, {"timeStart": "00-02-20", "timeEnd": "00-02-25", "sentence": "that the path basement coatings skilled better than the adjacent sea matrix based in code"}, {"timeStart": "00-02-25", "timeEnd": "00-02-30", "sentence": "overall we find that encoding tsar an important design decision for NES"}, {"timeStart": "00-02-30", "timeEnd": "00-02-34", "sentence": "and we give recommendations for wedding coatings to use for rich suburb"}, {"timeStart": "00-02-34", "timeEnd": "00-02-38", "sentence": "please see our poster or paper for more information"}, {"timeStart": "00-02-38", "timeEnd": "00-02-40", "sentence": "are go to this github link"}]}, {"title": "[NeurIPS 2020] Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis", "authors": "Ye Yuan", "abstract": "", "publicationOrg": "NeurIPS", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2006.07364.pdf", "pdfPath": "/data/cache/1/PDFs/NeurIPS2020ResidualForceControlforAgileHumanBehaviorImitationandExtendedMotionSynthesis.pdf", "publicationUrl": "https://arxiv.org/pdf/2006.07364.pdf", "codeUrl": "https://github.com/Khrylx/RFC", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/XuzH1u78o1Y", "videoPath": "/data/cache/1/videos/(NeurIPS2020)Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis.mp4", "pdfText": "Understanding human behaviors and creating virtual humans that act like real people has been a mesmerizing yet elusive goal in computer vision and graphics. One important step to achieve this goal is human motion synthesis which has broad applications in animation, gaming and virtual reality. With advances in deep learning, data-driven approaches  have achieved remarkable progress in producing realistic motions learned from motion capture data. Among them are physicsbased methods  empowered by reinforcement learning (RL), where a humanoid agent in simulation is trained to imitate reference motions. Physics-based methods have many advantages over their kinematics-based counterparts. For instance, the motions generated with physics are typically free from jitter, foot skidding or geometry penetration as they respect physical constraints. Moreover, the humanoid agent inside simulation can interact with the physical environment and adapt to various terrains and perturbations, generating diverse motion variations.However, physics-based methods have their own challenges. In many cases, the humanoid agent fails to imitate highly agile motions like ballet dance or long-term motions that involve swift transitions between various locomotions. We attribute such difficulty to the dynamics mismatch between the humanoid model and real humans. Humans are very difficult to model because they are very complex : Top: A ballet dancer performing highly agile moves like jet\u00e9, arabesque and pirouette. Bottom: A humanoid agent controlled by a policy augmented with the proposed residual forces (blue arrows) is able to dance like the performer. The motion is best viewed in our supplementary video.creatures with hundreds of bones and muscles. Although prior work has tried to improve the fidelity of the humanoid model , it is nonetheless safe to say that these models are not exact replicas of real humans and the dynamics mismatch still exists. The problem is further complicated when motion capture data comprises a variety of individuals with diverse body types. Due to the dynamics mismatch, motions produced by real humans may not be admissible by the humanoid model, which means no control policy of the humanoid is able to generate those motions.To overcome the dynamics mismatch, we propose an approach termed Residual Force Control (RFC) which can be seamlessly integrated into existing RL-based humanoid control frameworks. Specifically, RFC augments a control policy by introducing external residual forces into the action space. During RL training, the RFC-based policy learns to apply residual forces onto the humanoid to compensate for the dynamics mismatch and achieve better motion imitation. Intuitively, the residual forces can be interpreted as invisible forces that enhance the humanoid's abilities to go beyond the physical limits imposed by the humanoid model. RFC generates a more expressive dynamics that admits a wider range of human motions since the residual forces serve as a learnable time-varying correction to the dynamics of the humanoid model. To validate our approach, we perform motion imitation experiments on a wide range of dynamic human motions including ballet dance and acrobatics. The results demonstrate that RFC outperforms state-of-the-art methods with faster convergence and better motion quality. Notably, we are able to showcase humanoid control policies that are capable of highly agile ballet dance moves like pirouette, arabesque and jet\u00e9 ( ).Another challenge facing physics-based methods is synthesizing multi-modal long-term human motions. Previous work has elicited long-term human motions with hierarchical RL  or user interactive control . However, these approaches still need to define high-level tasks of the agent or require human interaction. We argue that removing these requirements could be critical for applications like automated motion generation and large-scale character animation. Thus, we take a different approach to long-term human motion synthesis by leveraging the temporal dependence of human motion. In particular, we propose a dual-policy control framework where a kinematic policy learns to predict multi-modal future motions based on the past motion and a latent variable used to model human intent, while an RFC-based control policy learns to imitate the output motions of the kinematic policy to produce physically-plausible motions. Experiments on a large-scale human motion dataset, Human3.6M , show that our approach with RFC and dual policy control can synthesize stable long-term human motions without any task guidance or user input.The main contributions of this work are as follows:  We address the dynamics mismatch in motion imitation by introducing the idea of RFC which can be readily integrated into RL-based humanoid control frameworks.  We propose a dual-policy control framework to synthesize multi-modal long-term human motions without the need for task guidance or user input. (3) Extensive experiments show that our approach outperforms state-of-the-art methods in terms of learning speed and motion quality. It also enables imitating highly agile motions like ballet dance that evade prior work. With RFC and dual-policy control, we present the first humanoid control method that successfully learns from a large-scale human motion dataset (Human3.6M) and generates diverse long-term motions.Kinematics-based models for human motion synthesis have been extensively studied by the computer graphics community. Early approaches construct motion graphs from large motion datasets and design controllers to navigate through the graph to generate novel motions . Alternatively, prior work has explored learning a low-dimensional embedding space to synthesize motions continuously . Advances in deep learning have enabled methods that use deep neural networks to design generative models of human motions . While the graphics community focuses on user control, computer vision researchers have been increasingly interested in predicting future human motions. A vast body of work has used recurrent neural networks to predict a deterministic future motion from the past motion . To address the uncertainty of future, stochastic approaches develop deep generative models to predict multi-modal future motions . The major drawback of kinematics-based approaches is that they are prone to generating physically-invalid motions with artifacts like jitter, foot skidding and geometry (e.g., body, ground) penetration.Physics-based methods for motion synthesis address the limitation of kinematics-based models by enforcing physical constraints. Early work has adopted model-based methods for tracking reference motions . Recently, deep RL has achieved great success in imitating human motions with manually-designed rewards . GAIL  based approaches have been proposed to eliminate the need for reward engineering . RL-based humanoid control has also been applied to estimating physically-plausible human poses from videos . To synthesize long-term human motions, prior work has resorted to hierarchical RL with predefined high-level task objectives . Alternatively, recent works use deep RL to learn controllable polices to generate long-term motions with user input . Different from previous work, our dual-policy control framework exploits the temporal dependence of human motion and synthesizes multi-modal long-term motions by forecasting diverse futures, which can be used to replace manual task guidance or user input. Furthermore, our proposed residual force control addresses the dynamics mismatch in humanoid control and enables imitating agile motions like ballet dance that evade prior work.Inferring external forces from human motion has been an active research area in biomechanics. Researchers have developed models that regress ground reaction forces from human motion using supervised learning . These approaches require expensive force data collected in laboratory settings to train the models. On the other hand, machine learning researchers have proposed differentiable physics engines that enable learning forces to control simple simulated systems . Trajectory optimization based approaches  have also been used to optimize external contact forces to synthesize human motions. A recent work  predicts forces acting on rigid objects in simulation to match image evidence with contact point supervision. Unlike prior work, we use deep RL to learn residual forces that complement contact forces to improve motion imitation without the supervision of forces or contact points.The task of humanoid control-based motion imitation can be formulated as a Markov decision process (MDP), which is defined by a tuple M = (S, A, T , R, \u03b3) of states, actions, transition dynamics, a reward function, and a discount factor. A humanoid agent interacts with a physically-simulated environment according to a policy \u03c0(a|s), which models the conditional distribution of choosing an action a \u2208 A given the current state s \u2208 S. Starting from some initial state s 0 , the agent iteratively samples an action a t from the policy \u03c0 and the simulation environment with transition dynamics T (s t+1 |s t , a t ) generates the next state s t+1 and gives the agent a reward r t . The reward is assigned based on how the agent's motion aligns with a given reference motion. The agent's goal is to learn an optimal policy \u03c0 * that maximizes its expected return J(\u03c0) = E \u03c0 [ t \u03b3 t r t ]. To solve for the optimal policy, one can apply one's favorite reinforcement learning algorithm (e.g., PPO ). In the following, we will give a more detailed description of the states, actions, policy and rewards to show how motion imitation fits in the standard reinforcement learning (RL) framework.States. The state s is formed by the humanoid state x = (q,q) which includes all degrees of freedom (DoFs) q of the humanoid and their corresponding velocitiesq. Specifically, the DoFs q = (q r , q nr ) include 6 root DoFs q r (global position and orientation) as well as the angles of other joints q nr . We transform q r to the root's local coordinate to remove dependency on global states.Actions. As noticed in previous work , using proportional derivative (PD) controllers at each joint yields more robust policies than directly outputting joint torques. Thus, the action a consists of the target angles u of the PD controllers mounted at non-root joint DoFs q nr (root DoFs q r are not actuated). The joint torques \u03c4 can then be computed aswhere k p and k d are manually-specified gains and \u2022 denotes element-wise multiplication.Policy. As the action a is continuous, we use a parametrized Gaussian policy \u03c0 \u03b8 (a|s) = N (\u00b5 \u03b8 , \u03a3) where the mean \u00b5 \u03b8 is output by a neural network with parameters \u03b8 and \u03a3 is a fixed diagonal covariance matrix. At test time, instead of sampling we use the mean action to achieve best performance.Rewards. Given a reference motion x 0:, we need to design a reward function to incentivize the humanoid agent to imitate x 0:T . To this end, the reward r t = r im t is defined by an imitation reward r im t that encourages the state x t of the humanoid agent to match the reference state x t . The detailed definition of the imitation reward r im t can be found in Appendix B. During RL training, the agent's initial state is intialized to a random frame from the reference motion x 0:T . The episode ends when the agent falls to the ground or the episode horizon H is reached.As demonstrated in prior work , we can apply the motion imitation framework described in Sec. 3 to successfully learn control policies that imitate human locomotions (e.g., walking, running, crouching) or acrobatics (e.g, backflips, cartwheels, jump kicks). However, the motion imitation framework has its limit on the range of motions that the agent is able to imitate. In our experiments, we often find the framework unable to learn more complex motions that require sophisticated foot interaction with the ground (e.g., ballet dance) or long-term motions that involve swift transitions between different modes of locomotion. We posit that the difficulty in learning such highly agile motions can be attributed to the dynamics mismatch between the humanoid model and real humans, i.e., the humanoid transition dynamics T (s t+1 |s t , a t ) is different from the real human dynamics. Thus, due to the dynamics mismatch, a reference motion x 0:T generated by a real human may not be admissible by the transition dynamics T , which means no policy under T can generate x 0:T .To overcome the dynamics mismatch, our goal is to come up with a new transition dynamics T that admits a wider range of motions. The new transition dynamics T should ideally satisfy two properties: (1) T needs to be expressive and overcome the limitations of the current dynamics T ;(2) T needs to be physically-valid and respect physical constraints (e.g., contacts), which implies that kinematics-based approaches such as directly manipulating the resulting state s t+1 by adding some residual \u03b4s are not viable as they may violate physical constraints.Based on the above considerations, we propose residual force control (RFC), that considers a more general form of dynamics T (s t+1 |s t , a t , a t ) where we introduce a corrective control action a t (i.e., external residual forces acting on the humanoid) alongside the original humanoid control action a t . We also introduce a corresponding RFC-based composite policy \u03c0 \u03b8 (a t , a t |s t ) which can be decomposed into two policies: (1) the original policy \u03c0 \u03b81 (a t |s t ) with parameters \u03b8 1 for humanoid control and (2) a residual force policy \u03c0 \u03b82 ( a t |s t ) with parameters \u03b8 2 for corrective control. The RFC-based dynamics and policy are more general as the original policy \u03c0 \u03b81 (a t |s t ) \u2261 \u03c0 \u03b8 (a t , 0|s t ) corresponds to a policy \u03c0 \u03b8 that always outputs zero residual forces. Similarly, the original dynamics T (s t+1 |s t , a t ) \u2261 T (s t+1 |s t , a t , 0) corresponds to the dynamics T with zero residual forces. During RL training, the RFC-based policy \u03c0 \u03b8 (a t , a t |s t ) learns to apply proper residual forces a t to the humanoid to compensate for the dynamics mismatch and better imitate the reference motion. Since a t is sampled from \u03c0 \u03b82 ( a t |s t ), the dynamics of the original policy \u03c0 \u03b81 (a t |s t ) is parametrized by \u03b8 2 as T \u03b82 (s t+1 |s t , a t ) \u2261 T (s t+1 |s t , a t , a t ). From this perspective, a t are learnable time-varying dynamics correction forces governed by \u03c0 \u03b82 . Thus, by optimizing the composite policy \u03c0 \u03b8 (a t , a t |s t ), we are in fact jointly optimizing the original humanoid control action a t and the dynamics correction (residual forces) a t . In the following, we propose two types of RFC, each with its own advantages.One way to implement RFC is to explicitly model the corrective action a t as a set of residual force vectors {\u03be 1 , . . . , \u03be M } and their respective contact points {e 1 , . . . , e M }. As the humanoid model is formed by a set of rigid bodies, the residual forces are applied to M bodies of the humanoid, where \u03be j and e j are represented in the local body frame. To reduce the size of the corrective action space, one can apply residual forces to a limited number of bodies such as the hip or feet. In RFC-Explicit, the corrective action of the policy \u03c0 \u03b8 (a, a|s) is defined as a = (\u03be 1 , . . . , \u03be M , e 1 , . . . , e M ) and the humanoid control action is a = u as before (Sec. 3). We can describe the humanoid motion using the equation of motion for multibody systems  augmented with the proposed residual forces:where we have made the residual forces term explicit. Eq. is an ordinary differential equation (ODE), and by solving it with an ODE solver we obtain the aforementioned RFC-based dynamics T (s t+1 |s t , a t , a t ). On the left hand sideq, B, C, g are the joint accelerations, the inertial matrix, the matrix of Coriolis and centrifugal terms, and the gravity vector, respectively. On the right hand side, the first term contains the torques \u03c4 computed from a (Sec. 3) applied to the non-root joint DoFs q nr and 0 corresponds to the 6 non-actuated root DoFs q r . The second term involves existing contact forces h i on the humanoid (usually exerted by the ground plane) and the contact points v i of h i , which are determined by the simulation environment. Here, J vi = dv i /dq is the Jacobian matrix that describes how the contact point v i changes with the joint DoFs q. By multiplying J T vi , the contact force h i is transformed from the world space to the joint space, which can be understood using the principle of virtual work, i.e., the virtual work in the joint space equals that in the world space or (J T vi h i ) T dq = h T i dv i . Unlike the contact forces h i which are determined by the environment, the policy can control the corrective action a which includes the residual forces \u03be j and their contact points e j in the proposed third term. The Jacobian matrix J ej = de j /dq is similarly defined as J vi . During RL training, the policy will learn to adjust \u03be j and e j to better imitate the reference motion. Most popular physics engines (e.g., MuJoCo , Bullet ) use a similar equation of motion to Eq. (2) (without residual forces), which makes our approach easy to integrate.As the residual forces are designed to be a correction mechanism to the original humanoid dynamics T , we need to regularize the residual forces so that the policy only invokes the residual forces when necessary. Consequently, the regularization keeps the new dynamics T close to the original dynamics T . Formally, we change the RL reward function by adding a regularizing reward r reg t :where w reg , k f and k cp are weighting factors. The regularization constrains the residual force \u03be j to be as small as possible and pushes the contact point e j to be close to the local body origin.One drawback of RFC-explicit is that one must specify the number of residual forces and the contact points. To address this issue, we also propose an implicit version of RFC where we directly model the total joint torques \u03b7 = J T ej \u03be j of the residual forces. In this way, we do not need to specify the number of residual forces or the contact points. We can decompose \u03b7 into two parts (\u03b7 r , \u03b7 nr ) that correspond to the root and non-root DoFs respectively. We can merge \u03b7 with the first term on the right of Eq. (2) as they are both controlled by the policy, which yields the new equation of motion:where we further remove \u03b7 nr (crossed out) because the torques applied at non-root DoFs are already modeled by the policy \u03c0 \u03b8 (a, a|s) through \u03c4 which can absorb \u03b7 nr . In RFC-Implicit, the corrective action of the policy is defined as a = \u03b7 r . To regularize \u03b7 r , we use a similar reward to Eq. (3):where k r is a weighting factor. While RFC-Explicit provides more interpretable results by exposing the residual forces and their contact points, RFC-Implicit is computationally more efficient as it only increases the action dimensions by 6 which is far less than that of RFC-Explicit and it does not require Jacobian computation. Furthermore, RFC-Implicit does not make any underlying assumptions about the number of residual forces or their contact points.So far our focus has been on imitating a given reference motion, which in practice is typically a short and segmented motion capture sequence (e.g., within 10 seconds). In some applications (e.g., behavior simulation, large-scale animation), we want the humanoid agent to autonomously exhibit long-term behaviors that consist of a sequence of diverse agile motions. Instead of guiding the humanoid using manually-designed tasks or direct user input, our goal is to let the humanoid learn long-term behaviors directly from data. To achieve this, we need to develop an approach that (i) infers future motions from the past and (ii) captures the multi-modal distribution of the future.As multi-modal behaviors are usually difficult to model in the control space due to non-differentiable dynamics, we first model human behaviors in the kinematic space. We propose a dual-policy control framework that consists of a kinematic policy \u03ba \u03c8 and an RFC-based control policy \u03c0 \u03b8 . The \u03c8parametrized kinematic policy \u03ba \u03c8 (x t:t+f |x t\u2212p:t , z) models the conditional distribution over a f -step future motion x t:t+f , given a p-step past motion x t\u2212p:t and a latent variable z used to model human intent. We learn the kinematic policy \u03ba \u03c8 with a conditional variational autoencoder (CVAE ), where we optimize the evidence lower bound (ELBO):where q \u03c6 (z|x t\u2212p:t , x t:t+f ) is a \u03c6-parametrized approximate posterior (encoder) distribution and p(z) is a Gaussian prior. The kinematic policy \u03ba \u03c8 and encoder q \u03c6 are instantiated as Gaussian distributions whose parameters are generated by two recurrent neural networks (RNNs) respectively. The detailed architectures for \u03ba \u03c8 and q \u03c6 are given in Appendix C.2.Once the kinematic policy \u03ba \u03c8 is learned, we can generate multi-modal future motions x t:t+f from the past motion x t\u2212p:t by sampling z \u223c p(z) and decoding z with \u03ba \u03c8 . To produce physically-plausible motions, we use an RFC-based control policy \u03c0 \u03b8 (a, a|x, x, z) to imitate the output motion x t:t+f of \u03ba \u03c8 by treating x t:t+f as the reference motion in the motion imitation framework (Sec. 3 and 4). The state s of the policy now includes the state x of the humanoid, the reference state x from \u03ba \u03c8 , and the latent code z. To fully leverage the reference state x, we use the non-root joint angles q nr inside x to serve as bases for the target joint angles u of the PD controllers. For this purpose, we change the humanoid control action a t from u to residual angles \u03b4u, and u can be computed as u = q nr + \u03b4u. This additive action will improve policy learning because q nr provides a good guess for u. while D is not full do 6:x 0:p \u2190 random motion from X 7:x p\u22121 \u2190 x p\u22121 initialize humanoid state 8:if (t \u2212 p) mod f = 0 then if reaching end of reference motion segment :11:x t:t+f \u2190 \u03ba \u03c8 ( x t: 14:x t \u2190 next state from simulation with a t and a tr t \u2190 reward from Eq. (3) or (5) :  The learning procedure for the control policy \u03c0 \u03b8 is outlined in Alg. 1. In each RL episode, we autoregressively apply the kinematic policy n times to generate reference motions x p:p+nf of nf steps, and the agent with policy \u03c0 \u03b8 is rewarded for imitating x p:p+nf . The reason for autoregressively generating n segments of future motions is to let the policy \u03c0 \u03b8 learn stable transitions through adjacent motion segments (e.g., x p:p+f and x p+f :p+2f ). At test time, we use the kinematic policy \u03ba \u03c8 and control policy \u03c0 \u03b8 jointly to synthesize infinite-horizon human motions by continuously forecasting futures with \u03ba \u03c8 and physically tracking the forecasted motions with \u03c0 \u03b8 .Our experiments consist of two parts: (1) Motion imitation, where we examine whether the proposed RFC can help overcome the dynamics mismatch and enable the humanoid to learn more agile behaviors from reference motions; (2) Extended motion synthesis, where we evaluate the effectiveness of the proposed dual-policy control along with RFC in synthesizing long-term human motions.Reference Motions. We use the CMU motion capture (MoCap) database (link) to provide reference motions for imitation. Specifically, we deliberately select eight clips of highly agile motions to increase the difficulty. We use clips of ballet dance with signature moves like pirouette, arabesque and jet\u00e9, which have sophisticated foot-ground interaction. We also include clips of acrobatics such as handsprings, backflips, cartwheels, jump kicks and side flips, which involve dynamic body rotations. Implementation Details. We use MuJoCo  as the physics engine. We construct the humanoid model from the skeleton of subject 8 in the CMU Mocap database while the reference motions we use are from various subjects. The humanoid model has 38 DoFs and 20 rigid bodies with properly assigned geometries. Following prior work , we add the motion phase to the state of the humanoid agent. We also use the stable PD controller  to compute joint torques. The simulation runs at 450Hz and the policy operates at 30Hz. We use PPO  to train the policy for 2000 epochs, each with 50,000 policy steps. Each policy takes about 1 day to train on a 20-core machine with an NVIDIA RTX 2080 Ti. More implementation details can be found in Appendix C.Comparisons. We compare the two variants -RFC-Explicit and RFC-Implicit -of our approach against the state-of-the-art method for motion imitation, DeepMimic . For fair comparison, the only differences between our RFC models and the DeepMimic baseline are the residual forces and the regularizing reward.  shows the learning curves of our models and DeepMimic, where we plot the average return per episode against the training epochs for all eight reference motions. We train three models with different initial seeds for each method and each reference motion. The return is computed using only the motion imitation reward and excludes the regularizing reward. We can see that both variants of RFC converge faster than DeepMimic consistently. Moreover, our RFC models always converge to better motion policies as indicated by the higher final returns. One can also observe that RFC-Explicit and RFC-Implicit perform similarly, suggesting that they are equally capable of imitating agile motions. Since the motion quality of learned policies is best seen in videos, we encourage the reader to refer to the supplementary video 1 for qualitative comparisons. One will observe that RFC can successfully imitate the sophisticated ballet dance skills while DeepMimic fails to reproduce them. We believe the failure of DeepMimic is due to the dynamics mismatch between the humanoid model and real humans, which results in the humanoid unable to generate the external forces needed to produce the motions. On the other hand, RFC overcomes the dynamics mismatch by augmenting the original humanoid dynamics with learnable residual forces, which enables a more flexible new dynamics that admits a wider range of agile motions. We note that the comparisons presented here are only for simulation domains (e.g., animation and motion synthesis) since external residual forces are not directly applicable to real robots. However, we do believe that RFC could be extended to a warm-up technique to accelerate the learning of complex policies for real robots, and the residual forces needed to overcome the dynamics mismatch could be used to guide agent design.Datasets. Our experiments are performed with two motion capture datasets: Human3.6M  and EgoMocap . Human3.6M is a large-scale dataset with 11 subjects (7 labeled) and 3.6 million total video frames. Each subject performs 15 actions in 30 takes where each take lasts from 1 to 5 minutes. We consider two evaluation protocols: (1) Mix, where we train and test on all 7 labeled subjects but using different takes; (2) Cross, where we train on 5 subjects (S1, S5, S6, S7, S8) and test on 2 subjects (S9 and S11). We train a model for each action for all methods. The other dataset, EgoMocap, is a relatively small dataset including 5 subjects and around 1 hour of motions. We train the models using the default train/test split in the mixed subject setting. Both datasets are resampled to 30Hz to conform to the policy.Implementation Details. The simulation setup is the same as the motion imitation task. We build two humanoids, one with 52 DoFs and 18 rigid bodies for Human3.6M and the other one with 59 DoFs and 20 rigid bodies for EgoMocap. For both datasets, the kinematic policy \u03ba \u03c8 observes motions of p = 30 steps (1s) to forecast motions of f = 60 steps (2s). When training the control policy \u03c0 \u03b8 , we generate n = 5 segments of future motions with \u03ba \u03c8 . Please refer to Appendix C for additional implementation details.Baselines and Metrics. We compare our approach against two well-known kinematics-based motion synthesis methods, ERD  and acLSTM , as well as a physics-based motion synthesis method that does not require task guidance or user input, EgoPose . We use two metrics, mean angle error (MAE) and final angle error (FAE). MAE computes the average Euclidean distance between predicted poses and ground truth poses in angle space, while FAE computes the distance for the final frame. Both metrics are computed with a forecasting horizon of 2s. For stochastic methods, we generate 10 future motion samples to compute the mean of the metrics. Results. In , we show quantitative results of all models for motion forecasting over the 2s horizon, which evaluates the methods' ability to infer future motions from the past. For all datasets and evaluation protocols, our RFC models with dual-policy control outperform the baselines consistently in both metrics. We hypothesize that the performance gain over the other physics-based method, EgoPose, can be attributed to the use of kine-matic policy and the residual forces. To verify this hypothesis, we conduct an ablation study in the Human3.6M (Mix) setting. We train model variants of RFC-Implicit by removing the residual forces (ResForce) or the additive action (AddAct) that uses the kinematic policy's output.  demonstrates that, in either case, the performance decreases for both metrics, which supports our previous hypothesis. Unlike prior physics-based methods, our approach also enables synthesizing sitting motions even when the chair is not modeled in the physics environment, because the learned residual forces can provide the contact forces need to support the humanoid. Furthermore, our model allows infinite-horizon stable motion synthesis by autoregressively applying dual policy control. As motions are best seen in videos, please refer to the supplementary video for qualitative results.In this work, we proposed residual force control (RFC), a novel and simple method to address the dynamics mismatch between the humanoid model and real humans. RFC uses external residual forces to provide a learnable time-varying correction to the dynamics of the humanoid model, which results in a more flexible new dynamics that admits a wider range of agile motions. Experiments showed that RFC outperforms state-of-the-art motion imitation methods in terms of convergence speed and motion quality. RFC also enabled the humanoid to learn sophisticated skills like ballet dance which have eluded prior work. Furthermore, we proposed a dual-policy control framework to synthesize multi-modal infinite-horizon human motions without any task guidance or user input, which opened up new avenues for automated motion generation and large-scale character animation. We hope our exploration of the two aspects of human motion, dynamics and kinematics, can encourage more work to view the two from a unified perspective. One limitation of the RFC framework is that it can only be applied to simulation domains (e.g., animation, motion synthesis, pose estimation) in its current form, as real robots cannot generate external residual forces. However, we do believe that RFC could be applied as a warm-up technique to accelerate the learning of complex policies for real robots. Further, the magnitude of residual forces needed to overcome the dynamics mismatch could also be used to inform and optimize agent design. These are all interesting avenues for future work.", "videoStruct": []}, {"title": "Solving Physics Puzzles by Reasoning about Paths (NeurIPS 2020) - PHYRE", "authors": "Andrew Melnik", "abstract": "", "publicationOrg": "NeurIPS", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2011.07357.pdf", "pdfPath": "/data/cache/1/PDFs/SolvingPhysicsPuzzlesbyReasoningaboutPathsNeurIPS2020PHYRE.pdf", "publicationUrl": "https://arxiv.org/pdf/2011.07357.pdf", "codeUrl": "https://github.com/ndrwmlnk/PHYRE-Reasoning-about-Paths", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/X30QGeIEXRs", "videoPath": "/data/cache/1/videos/Solving Physics Puzzles by Reasoning about Paths (NeurIPS 2020) - PHYRE.mp4", "pdfText": "PHYRE benchmark    offers a large set of physics challenges in the described format. The ability of humans to solve even intricate instances of such puzzles reflects a high level of physics cognition that so far is unparalleled in machines. : Top: Action generation pipeline. NNs modules are highlighted with green rectangles. The task's initial scene is presented to the agent as five bitmap channels; one channel for each object class: Green target-object, blue dynamic goal-object, blue static goal-object, dynamic grey objects, static black objects. Bottom left: Model prediction examples. All examples of the generated final action in the figure solve the corresponding tasks. Bottom right: Model architecture details: Every Conv2d and ConvTransposed2d Layer has a kernel size of 4x4, stride of 2 and padding of 1.Here we describe the building blocks of our NN model 2 outlined in , and compare them to the introspection of players trying to solve such a puzzle:1. Base Net. Humans can get useful information by imagining how the scene will develop without the solving intervention. Therefore, we train a Base Net neural network (NN) using the PHYRE simulator to predict the path (Base Path) of the green target-object ( ) when there is no interaction with the red action-ball. The Base Net input consists of the five channel bitmap of the initial scene. Target output is the probability density of 2D-points predicted to be traversed by the Base Path. The training procedure is described in detail in the section 2.3 Training.2. Target Net. Human players can imagine the path the green target-object should follow in order to solve the puzzle on a set of acquired intuitions: Getting closer to the goal is better as well as following the direction of gravity and obeying interaction constraints. Therefore, we train a Target Net ) to generate possible solution paths (Target Path) of the green target-object without information about the red action-ball. This gives the Target Net the freedom to \"dream up\" Target Paths that appear \"interaction incomplete\" since they become physically valid only through some, so far unknown, interaction with the red action-ball. Input to the Target Net is the initial scene 5-channel bitmap. Target output now is the probability map (2D-density of traversed points) of the Target Path.3. Action Net 1. Humans are able to reason and imagine which trajectories the red action-ball has to take to \"add\" the missing interaction to turn a Base Path into a potential Target Path. Here humans might heuristically start from the point where Base Path and Target Path diverge. Action Net 1 generates possible Action-Ball Paths. NN input: Initial scene 5-channel bitmap and 2-channel probability maps of the Base Path from step 1 and the Target Path from step 2. NN output: Probability map of the Action-Ball Path.Action Net 2 generates a probability map of the initial red action-ball position ). NN input: Initial scene 5-channel bitmap and probability maps of the Target Path from step 2 and the Action Path from step 3.5. Convert the red action-ball probability maps from step 4 to a 3-dim action vector, comprised of x, y and radius values. This is done with a non-learning algorithm, which randomly selects initial radius and initial position proposals (x, y) from pixels that are over a certain threshold value. Then it iteratively tries improve the overlap of the red action-ball rendered from a new action vector, and the probability map by selecting position and radius values within a close neighborhood. We sample 5 different initial positions and radius value and update each of them 5 times. We take these as the first 5 actions for solving the task. If all fail, we randomly sample from them and add Gaussian noise until the task is solved or the limit of 100 tries is reached.The artificial agent needs knowledge about the whole scene for reasoning about paths: Object interactions might be understood with local information but to propose a path which connects the green target-object and blue goal-object the whole scene needs to be considered. This motivated the following architecture for Base Net, Target Net, Action Net 1, and Action Net 2, illustrated in . A stack of convolutional layers 'folds' the input channels into a 256-dimensional encoding of the complete scene. Then a similar stack of transposed convolutional layers 'unfolds' this global encoding into the desired number of output channels, each having the same dimensionality as the input channels. We use ReLU as the activation function for hidden layers and the sigmoid function for the output layer to allow smooth prediction.All 4 NNs of the model are trained jointly in a supervised way; each NN receives its own learning signal but learning signals are also backpropagated through the entire architecture. We collect BasePath, TargetPath, ActionPath and the initial red action-ball bitmap channels from rollouts in the PHYRE simulator and use them to impose a cross-entropy loss between every pixel of the NNs output and the corresponding ground truth bitmaps. Action Net 1 and Action Net 2 receive predicted BasePath and TargetPath channels from BaseNet and TargetNet NNs. For training we use a data set that containes 10 solving rollouts per task, 80 tasks per template, and 25 PHYRE-B templates. We randomly shuffle the samples and split them into 625 batches with a batch size of 32 and train the NNs for 10 epochs.3 ResultsPHYRE uses the auccess metric to score performance: Agents can try up to 100 attempts per task and the area under the logarithmically scaled percentage of tasks solved curve is the auccess value. See  for more details. As described in 2.1, we generate 5 action proposals for each task based on the action ball prediction from Action Net 2. If the proposed actions led to unsuccessful attempts, we sample further action vectors from a multivariate normal distribution centered at the original action proposals. We slowly increase its standard deviation during the 100 tasks (starting from 0.02), leading to increased deviation from the original action for later attempts. There are 25 templates in the PHYRE-B (BALL) problem . In the within-template setting the model is evaluated on tasks that share the same template with training tasks. Such tasks differ in the placement and size of scene objects, but not in their otherwise structure. In the cross-template setting, the model is evaluated on tasks from templates that were not shown during training. Bakhtin et al.  use a \"process that deterministically splits the tasks into 10 folds containing a training, validation, and test set\" allowing fair comparison between agents and studies.  shows auccess values of our trained NN model which are collected individually for each template in each fold and then averaged over all 10 folds. The baseline DQN only uses a deep reinforcement learning setup to guess the success value of a batch of 10000 possible combinations of action parameters (x, y, radius) with a certain level of grid discretization, while our model performs a basic reasoning about developments in the scene and interactions about objects, thus capable of generating an informed action proposal in an explainable fashion. Our model can also be thought of as a kind of a jointly trained autoencoder for multi-modal fusion .Forward prediction for physical reasoning , an error-based dynamic model learning , or continues-action-space policy gradient algorithms  are possible ways to improve performance and generalize learning. Allen et al.  introduced the \"Virtual Tools\" game to measure the capacity of human beings for flexible, creative tool use. The study proposes that the flexibility of human physical problem solving rests on an ability to imagine the effects of hypothesized actions. Our deep learning architecture ) can serve as a mechanism to imagine the effects of such hypothesized actions. Kurutach et al.  demonstrated on a rope manipulation example that a plausible sequence of observations evolving from its current configuration to a desired goal state can be used as a reference trajectory for control. In contrast, the PHYRE type problems have an additional constraint of one control-action per episode, and therefore requires a different NN architecture.Melnik et al.  described a set of functional modules for specific types of interaction primitives, which are common to a broad range of arcade ball-game environments. Results of this case study in different Atari ball-games suggest that human-level performance can be achieved by a learning agent within a human amount of game experience (10-15 minutes game time) when a proper decomposition of an environment or a task is provided. However, automatization of such decomposition remains a challenging problem.", "videoStruct": [{"timeStart": "00-00-00", "timeEnd": "00-00-04", "sentence": "I am my name is Augustine hotter and am happy to present you our work"}, {"timeStart": "00-00-04", "timeEnd": "00-00-07", "sentence": "solving physics passers by reasoning about paths"}, {"timeStart": "00-00-08", "timeEnd": "00-00-12", "sentence": "we propose a new deep learning model for gold driven tasks"}, {"timeStart": "00-00-12", "timeEnd": "00-00-15", "sentence": "that require intuitive physical reasoning"}, {"timeStart": "00-00-15", "timeEnd": "00-00-17", "sentence": "intervention the scene"}, {"timeStart": "00-00-17", "timeEnd": "00-00-19", "sentence": "to achieve a desired end goal"}, {"timeStart": "00-00-19", "timeEnd": "00-00-22", "sentence": "to evaluate the model we use fire"}, {"timeStart": "00-00-22", "timeEnd": "00-00-25", "sentence": "a benchmark test for goal driven physical reasoning"}, {"timeStart": "00-00-25", "timeEnd": "00-00-27", "sentence": "into the mechanics passes"}, {"timeStart": "00-00-27", "timeEnd": "00-00-29", "sentence": "these are the twenty five templates"}, {"timeStart": "00-00-29", "timeEnd": "00-00-31", "sentence": "of the fire ball"}, {"timeStart": "00-00-31", "timeEnd": "00-00-35", "sentence": "we present the mother that learns to solve all of them"}, {"timeStart": "00-00-36", "timeEnd": "00-00-38", "sentence": "yes an example"}, {"timeStart": "00-00-38", "timeEnd": "00-00-39", "sentence": "this is the initial scene"}, {"timeStart": "00-00-39", "timeEnd": "00-00-41", "sentence": "and not a task is to place the red boy"}, {"timeStart": "00-00-41", "timeEnd": "00-00-44", "sentence": "that will make the green object touched the blue object"}, {"timeStart": "00-00-47", "timeEnd": "00-00-49", "sentence": "the pipeline starts on the left"}, {"timeStart": "00-00-50", "timeEnd": "00-00-54", "sentence": "initial see it is represented as a five channel bit map"}, {"timeStart": "00-00-54", "timeEnd": "00-00-56", "sentence": "one channel for each type of object"}, {"timeStart": "00-00-57", "timeEnd": "00-00-59", "sentence": "the base map predicts the path"}, {"timeStart": "00-00-59", "timeEnd": "00-01-01", "sentence": "the green object would take"}, {"timeStart": "00-01-01", "timeEnd": "00-01-04", "sentence": "without interaction with the red action bar"}, {"timeStart": "00-01-04", "timeEnd": "00-01-08", "sentence": "target net predicted dream path the green object should take"}, {"timeStart": "00-01-08", "timeEnd": "00-01-10", "sentence": "in order to solve the task"}, {"timeStart": "00-01-10", "timeEnd": "00-01-14", "sentence": "the accident one then predicts the red action ball path"}, {"timeStart": "00-01-14", "timeEnd": "00-01-16", "sentence": "that would solve the task"}, {"timeStart": "00-01-17", "timeEnd": "00-01-21", "sentence": "exner to then predicts the initial read ball possession"}, {"timeStart": "00-01-22", "timeEnd": "00-01-26", "sentence": "this prediction is done converted into a three dimensional action vector"}, {"timeStart": "00-01-26", "timeEnd": "00-01-30", "sentence": "consisting of exposition y position and radios value"}, {"timeStart": "00-01-31", "timeEnd": "00-01-36", "sentence": "we then use this action actor our first attempt to solve the task"}, {"timeStart": "00-01-37", "timeEnd": "00-01-42", "sentence": "if it fades we increasingly at gaussian noise until we get to solving action"}, {"timeStart": "00-01-43", "timeEnd": "00-01-46", "sentence": "for a basement target and both acts nuts"}, {"timeStart": "00-01-46", "timeEnd": "00-01-50", "sentence": "we use an encoder decoder architecture"}, {"timeStart": "00-01-50", "timeEnd": "00-01-51", "sentence": "that you can see here"}, {"timeStart": "00-01-51", "timeEnd": "00-01-56", "sentence": "the bottleneck is a two hundred fifty six dimensional encoding of the whole scene"}, {"timeStart": "00-01-57", "timeEnd": "00-02-03", "sentence": "here can see three examples with visualization of the pipeline steps"}, {"timeStart": "00-02-03", "timeEnd": "00-02-07", "sentence": "we can see that it learns solution ambiguity"}, {"timeStart": "00-02-07", "timeEnd": "00-02-12", "sentence": "and that it is able to generalize over different types of objects like a ball and bow"}, {"timeStart": "00-02-13", "timeEnd": "00-02-18", "sentence": "after ten attempts our moto sites on average sixty six percent of tasks"}, {"timeStart": "00-02-18", "timeEnd": "00-02-21", "sentence": "but you can see a difference between templates"}, {"timeStart": "00-02-21", "timeEnd": "00-02-25", "sentence": "template three is almost always solved after the first attempt"}, {"timeStart": "00-02-26", "timeEnd": "00-02-28", "sentence": "why Templar twenty three"}, {"timeStart": "00-02-28", "timeEnd": "00-02-32", "sentence": "it's soft after ten attempts in about half of the time"}, {"timeStart": "00-02-32", "timeEnd": "00-02-36", "sentence": "you can find our paper and cult implementation with the following link"}, {"timeStart": "00-02-37", "timeEnd": "00-02-38", "sentence": "thankfully attention"}]}, {"title": "[Spotlight at NeurIPS 2020] Probabilistic Linear Solvers for Machine Learning", "authors": "T\u00fcbingen Machine Learning", "abstract": "", "publicationOrg": "NeurIPS", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2010.09691.pdf", "pdfPath": "/data/cache/1/PDFs/SpotlightatNeurIPS2020ProbabilisticLinearSolversforMachineLearning.pdf", "publicationUrl": "https://arxiv.org/pdf/2010.09691.pdf", "codeUrl": "https://github.com/JonathanWenger/probabilistic-linear-solvers-for-ml", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/3_1JE91g_3E", "videoPath": "/data/cache/1/videos/NeurIPS 2020- Probabilistic Linear Solvers for Machine Learning.mp4", "pdfText": "Arguably one of the most fundamental problems in machine learning, statistics and scientific computation at large is the solution of linear systems of the form Ax * = b, where A \u2208 R n\u00d7n sym is a symmetric positive definite matrix . Such matrices usually arise in the context of second-order or quadratic optimization problems and as Gram matrices. Some of the numerous application areas in machine learning and related fields are least-squares regression , kernel methods , Kalman filtering , Gaussian (process) inference , spectral graph theory , (linear) differential equations  and (stochastic) second-order methods .Linear systems in machine learning are typically large-scale, have characteristic structure arising from generative processes, and are subject to noise. These distinctive features call for linear solvers that can explicitly make use of such structural information. While classic solvers are highly optimized for general problems, they lack key functionality for machine learning. In particular, they do not consider generative prior information about the matrix.An important example are kernel Gram matrices, which exhibit specific sparsity structure and spectral properties, depending on the kernel choice and the generative process of the data. Exploiting such prior information is a prime application for probabilistic linear solvers, which aim to quantify numerical uncertainty arising from limited computational resources. Another key challenge, which we will not yet address here, are noisy matrix evaluations arising from data subsampling. Ultimately, linear algebra for machine learning should integrate all sources of uncertainty in a computational pipeline -aleatoric, epistemic and numerical -into one coherent probabilistic framework. : Illustration of a probabilistic linear solver. Given a prior for A or H modelling the linear operator A and its inverse A \u22121 , posterior beliefs are inferred via observations y i = As i . This induces a distribution on the solution x * , quantifying numerical uncertainty arising from finite computation. The plot shows k = 3 iterations of Algorithm 1 on a toy problem of dimension n = 5.interpreting linear solvers. Further, we propose a prior covariance class which recovers the method of conjugate gradients as its posterior mean and uses prior spectral information for uncertainty calibration, one of the primary shortcomings of probabilistic linear solvers. We conclude by presenting simplified examples of promising applications of such solvers within machine learning.Let Ax * = b be a linear system with A \u2208 R n\u00d7n sym positive definite and b \u2208 R n . Probabilistic linear solvers (PLS)  iteratively build a model for the linear operator A, its inverse H = A \u22121 or the solution x * , represented by random variables A, H or x. In the framework of probabilistic numerics  such solvers can be seen as Bayesian agents performing inference via linear observations Y = [y 1 , . . . , y k ] \u2208 R n\u00d7k resulting from actions S = [s 1 , . . . , s k ] \u2208 R n\u00d7k given by an internal policy \u03c0(s | A, H, x, A, b). For a matrix-variate prior p(A) or p(H) encoding prior (generative) information, our solver computes posterior beliefs over the matrix, its inverse and the solution of the linear system. An illustration of a probabilistic linear solver is given in .Desiderata We begin by stipulating a fundamental set of desiderata for probabilistic linear solvers.To our knowledge such a list has not been collated before. Connecting previously disjoint threads, the following presents a roadmap for the development of these methods. Probabilistic linear solvers modelling A and A \u22121 must assume matrix-variate distributions which are expressive enough to capture structure and generative prior information either for A or its inverse. The distribution choice must also allow computationally efficient sampling and density evaluation. It should encode symmetry and positive definiteness and must be closed under positive linear combinations. Further, the two models for the system matrix or its inverse should be translatable into and consistent with each other. Actions s i of a PLS should be model-based and induce a tractable distribution on linear observations y i = As i . Since probabilistic linear solvers are low-level procedures, their inference procedure must be computationally lightweight. Given (noise-corrupted) observations this requires tractable posteriors over A, H and x, which are calibrated in the sense that at convergence the true solution x * represents a draw from the posterior p(x | Y , S). Finally, such solvers need to allow preconditioning of the problem and ideally should return beliefs over non-linear properties of the system matrix extending the functionality of classic methods. These desiderata are summarized concisely in .Guided by these desiderata, we will now outline the inference framework for A, H and x forming the base of the algorithm. The choice of a matrix-variate prior distribution is severely limited by the desideratum that conditioning on linear observations y i = As i must be tractable. This reduces the choice to stable distributions  and thus excludes candidates such as the Wishart, which has measure zero outside the cone of symmetric positive semi-definite matrices. For symmetric matrices, this essentially forces use of the symmetric matrix-variate normal distribution, introduced in this context by Hennig . Given A 0 , W A 0 \u2208 R n\u00d7n sym , assume a prior distribution p(A) = N (A; A 0 , W A 0 W A 0 ), : Desired properties of probabilistic linear solvers. Symbols ( , \u223c, ) indicate which properties are encoded in our proposed solver (see  and to what degree. where denotes the symmetric Kronecker product .  The symmetric matrix-variate Gaussian induces a Gaussian distribution on linear observations. While it has non-zero measure only for symmetric matrices, its support is not the positive definite cone. However, positive definiteness can still be enforced post-hoc (see Proposition 1). We assume noise-free linear observations of the formThe posterior distribution follows from the properties of Gaussians  and has been investigated in detail in previous work . It is given by p(where \u2206 A 0 = Y \u2212 A 0 S and U = W A 0 S(S W A 0 S) \u22121 . We aim to construct a probabilistic model H for the inverse H = A \u22121 consistent with the model A as well. However, not even in the scalar case does the inverse of a Gaussian have finite mean. We ask instead what Gaussian model for H is as consistent as possible with our observational model for A. whereIn Section 3 we will derive a covariance class, which establishes correspondence between the two Gaussian viewpoints for the linear operator and its inverse and is consistent with our desiderata.The above inference procedure leads to Algorithm 1. The degree to which the desiderata are encoded in our formulation of a PLS can be found in . We will now go into more detail about the policy, the choice of step size, stopping criteria and the implementation.Step Size In each iteration our solver collects information about the linear operator A via actions s i determined by the policy \u03c0(s | A, H, x, A, b). The next actionchosen based on the current belief about the inverse. If E[H] = A \u22121 , i.e. if the solver's estimate for the inverse equals the true inverse, then Algorithm 1 converges in a single step sinceThe step size minimizing the quadratic q(Stopping Criteria Classic linear solvers typically use stopping criteria based on the current residual of the form Ax i \u2212 b 2 \u2264 max(\u03b4 rtol b 2 , \u03b4 atol ) for relative and absolute tolerances \u03b4 rtol and \u03b4 atol . However, this residual may oscillate or even increase in all but the last step even if the error x * \u2212 x i 2 is monotonically decreasing . From a probabilistic point of view, we should stop if our posterior uncertainty is sufficiently small. Assuming the posterior covariance is calibrated, it. Hence given calibration, we can bound the expected (relative) error between our estimate and the true solution by terminating when tr(Cov[x]) \u2264 max(\u03b4 rtol b 2 , \u03b4 atol ). A probabilistic criterion is also necessary for an extension to the noisy setting, where classic convergence criteria become stochastic. However, probabilistic linear solvers typically suffer from miscalibration , an issue we will address in Section 3.We provide an open-source implementation of Algorithm 1 as part of PROBNUM, a Python package implementing probabilistic numerical methods, in an online code repository:https://github.com/probabilistic-numerics/probnumThe mean and covariance up-and downdates in Section 2.1 when performed iteratively are of low rank. In order to maintain numerical stability these updates can instead be performed for their respective Cholesky factors . This also enables computationally efficient sampling or evaluation of probability density functions downstream.This section details some theoretical properties of our method such as its convergence behavior and computational complexity. In particular we demonstrate that for a specific prior choice Algorithm 1 recovers the method of conjugate gradients as its solution estimate. All proofs of results in this section and the next can be found in the supplementary material. We begin by establishing that our solver is a conjugate directions method and therefore converges in at most n steps in exact arithmetic. We can obtain a better convergence rate by placing stronger conditions on the prior covariance class as outlined in Section 3. Given these assumptions, Algorithm 1 recovers the iterates of (preconditioned) CG and thus inherits its favorable convergence behavior (overviews in ). Theorem 2 (Connection to the Conjugate Gradient Method) Given a scalar prior mean A 0 = H \u22121 0 = \u03b1I with \u03b1 > 0, assume (1) and (2) hold, then the iterates x i of Algorithm 1 are identical to the ones produced by the conjugate gradient method.A common phenomenon observed when implementing conjugate gradient methods is that due to cancellation in the computation of the residuals, the search directions s i lose A-conjugacy . In fact, they can become independent up to working precision for i large enough . One way to combat this is to perform complete reorthogonalization of the search directions in each iteration as originally suggested by Lanczos . Algorithm 1 does this implicitly via its choice of policy which depends on all previous search directions as opposed to just s i\u22121 for (naive) CG.Computational Complexity The solver has time complexity O(kn 2 ) for k iterations without uncertainty calibration. Compared to CG, inferring the posteriors in Section 2.1 adds an overhead of four outer products and four matrix-vector products per iteration, given (1) and . Uncertainty calibration outlined in Section 3 adds between O(1) and O(k 3 ) per iteration depending on the sophistication of the scheme. Already for moderate n this is dominated by the iteration cost. In practice, means and covariances do not need to be formed in memory. Instead they can be evaluated lazily as linear operators v \u2192 Lv, if S and Y are stored. This results in space complexity O(kn).Numerical methods for the solution of linear systems have been studied in great detail since the last century. Standard texts  give an in-depth overview. The conjugate gradient method recovered by our algorithm for a specific choice of prior was introduced by Hestenes and Stiefel . Recently, randomization has been exploited to develop improved algorithms for large-scale problems arising from machine learning . The key difference to our approach is that we do not rely on sampling to approximate large-scale matrices, but instead perform probabilistic inference. Our approach is based on the framework of probabilistic numerics  and is a natural continuation of previous work on probabilistic linear solvers. In historical order, Hennig and Kiefel  provided a probabilistic interpretation of Quasi-Newton methods, which was expanded upon in . This work also relied on the symmetric matrix-variate Gaussian as used in our paper. Bartels and Hennig  estimate numerical error in approximate least-squares solutions by using a probabilistic model. More recently, Cockayne et al.  proposed a Bayesian conjugate gradient method performing inference on the solution of the system. This was connected to the matrix-based view by Bartels et al. .Having outlined the proposed algorithm, this section derives a prior covariance class which satisfies nearly all desiderata, connects the two modes of prior information and allows for calibration of uncertainty by appropriately choosing remaining degrees of freedom in the covariance. The third desideratum posited that A and H should be almost surely positive definite. This evidently does not hold for the matrix-variate Gaussian. However, we can restrict the choice of admissable W A 0 to act like A on span(S). This in turn induces a positive definite posterior mean. Proposition 1 (Hereditary Positive Definiteness ) Let A 0 \u2208 R n\u00d7n sym be positive definite. Assume the actions S are A-conjugate and W A 0 S = Y , then for i \u2208 {0, . . . , k \u2212 1} it holds that A i+1 is symmetric positive definite.Prior information about the linear system usually concerns the matrix A itself and not its inverse, but the inverse is needed to infer the solution x * of the linear problem. So a way to translate between a Gaussian distribution on A and H is crucial. Previous works generally committed to either one view or the other, potentially discarding available information. Below, we show that the two correspond, if we allow ourselves to constrain the space of possible models. We impose the following condition. Definition 1 Let A i and H i be the means of A and H at step i. We say a prior induces posterior correspondence ifThe following theorem establishes a sufficient condition for weak posterior correspondence. For an asymmetric prior model one can establish the stronger notion of posterior correspondence. A proof is included in the supplements. Theorem 3 (Weak Posterior Correspondence) Let W H 0 \u2208 R n\u00d7n sym be positive definite. Assume H 0 = A \u22121 0 , and thatthen weak posterior correspondence holds for the symmetric Kronecker covariance.Given the above, let A 0 be a symmetric positive definite prior mean and H 0 = A \u22121 0 . Define the orthogonal projection matrices P S \u22a5 = I \u2212S(S S) \u22121 S \u2208 R n\u00d7n sym and Psym mapping to the spaces span(S) \u22a5 and span(Y ) \u22a5 . We propose the following prior covariance class given by the prior covariance factors of the A and H viewwhere \u03a6 \u2208 R n\u00d7n and \u03a8 \u2208 R n\u00d7n are degrees of freedom. This choice of covariance class satisfies Theorem 1, Proposition 1, Theorem 3 and for a scalar mean also Theorem 2. Therefore, it produces symmetric realizations, has symmetric positive semi-definite means, it links the matrix and the inverse view and at any given time only needs access to v \u2192 Av not A itself. It is also compatible with a preconditioner by simply transforming the given linear problem.This class can be interpreted as follows. The derived covariance factor W A 0 acts like A on the space span(S) explored by the algorithm. On the remaining space its uncertainty is determined by the degrees of freedom in \u03a6. Likewise, our best guess for A \u22121 is A \u22121 0 on the space spanned by Y . On the orthogonal space span(Y ) \u22a5 the uncertainty is determined by \u03a8. Note that the prior depends on actions and observations collected during a run of Algorithm 1, hence one might call this an empirical Bayesian approach. This begs the question how the algorithm is realizable for the proposed prior (3) given its dependence on future data. Notice that the posterior mean in Section 2.1 only depends on W A 0 S = Y not on W A 0 alone. Using eq. (3), at iteration i we have W A 0 S 1:i = Y 1:i , i.e. the observations made up to this point. Similar reasoning applies for the inverse. Now, the posterior covariances do depend on W A 0 , respectively W H 0 alone, but prior to convergence we only require tr(Cov[x]) for the stopping criterion. We show in Section S4.3 under the assumptions of Theorem 2 how to compute this at any iteration i independent of future actions and observations.which is a well-known behavior of CG (see eqn. 5.29 in ). In part, since this dynamic of the underlying Krylov subspace method is not encoded in the prior, the solver in its current form is typically miscalibrated (see also ). While this non-linear information is challenging to include in the Gaussian framework, we can choose \u03a6 and \u03a8 in (3) to empirically calibrate uncertainty. This can be interpreted as a form of hyperparameter optimization similar to optimization of kernel parameters in GP regression.We would like to encode prior knowledge about the way A and H act in the respective orthogonal spaces span(S) \u22a5 and span(Y ) \u22a5 . For the Rayleigh quotient. Hence for vectors v lying in the respective null spaces of S and Y our uncertainty should be determined by the not yet explored eigenvalues \u03bb k+1 , . . . , \u03bb n of A and H. Without prior information about the eigenspaces, we choose \u03a6 = \u03c6I and \u03a8 = \u03c8I. If a priori we know the respective spectra, a straightforward choice isIn the absence of prior spectral information we can make use of already collected quantities during a run of Algorithm 1. We build a one-dimensional regression model p(ln R i | Y , S) for the ln-Rayleigh quotient ln R(A, s i ) given actions s i . Such a model can then encode the well studied behaviour of CG, whose Rayleigh coefficients rapidly decay at first, followed by a slower continuous  decay .  illustrates this approach using a GP regression model. At convergence, we use the prediction of the Rayleigh quotient for the remaining n \u2212 k dimensions by choosingi.e. uncertainty about actions in span(S) \u22a5 is calibrated to be the average Rayleigh quotient as an approximation to the spectrum. Depending on the application a simple or more complex model may be useful. For large problems, where generally k n, more sophisticated schemes become computationally feasible. However, these do not necessarily need to be computationally demanding due to the simple nature of this one-dimensional regression problem with few data. For example, approximate  or even exact GP regression  is possible in O(k) using a Kalman filter.This section demonstrates the functionality of Algorithm 1. We choose some -deliberately simpleexample problems from machine learning and scientific computation, where the solver can be used to quantify uncertainty induced by finite computation, solve multiple consecutive linear systems, and propagate information between problems.Gaussian Process Regression GP regression  infers a latent function f :, where X \u2208 R n\u00d7N and y \u2208 R n . Given a prior p(f ) = GP(f ; 0, k) with kernel k for the unknown function f , the posterior mean and marginal variance atis the Gram matrix of the kernel andk = k(X,x) \u2208 R n\u00d7m . The bulk of computation during prediction arises from solving the linear system (K + \u03b5 2 I)z = b for some right-hand side b \u2208 R n repeatedly. When using a probabilistic linear solver for this task, we can quantify the uncertainty arising from finite computation as well as the belief of the solver about the shape of the GP at a set of not yet computed inputs.  illustrates this. In fact, we can estimate the marginal variance of the GP without solving the linear system again by multiplyingk with the estimated inverse of K + \u03b5 2 I. In large-scale applications, we can trade off computational expense for increased uncertainty arising from the numerical approximation and quantified by the probabilistic linear solver. By assessing the numerical uncertainty arising from not exploring the full space, we can judge the quality of the estimated GP mean and marginal variance.Kernel Gram Matrix Inversion Consider a linear problem Kx * = b, where K is generated by a Mercer kernel. For a \u03bd-times continuously differentiable kernel the eigenvalues \u03bb n (K) decay approximately as |\u03bb n | \u2208 O(n \u2212\u03bd\u2212 1 2 ) . We can make use of this generative prior informationFigure 3: Numerical uncertainty in GP inference. Computing posterior mean and covariance of a GP regression using a PLS. Top: GP mean for a toy data set (n = 16) computed with increasing number of iterations k of Algorithm 1. The numerical estimate of the GP mean approaches the true mean. Note that the numerical variance is different from the marginal variance of the GP. Bottom: GP variance and estimate of GP variance with numerical uncertainty. The GP variance estimate is computed using the estimated inverse from computing E[f ] without any additional solver iterations.by specifying a parametrized prior mean \u00b5(n) = ln \u03b8 0 n \u2212\u03b81 = \u03b8 0 \u2212 \u03b8 1 ln(n) for the ln-Rayleigh quotient model. Typically, such Gram matrices are ill-conditioned and therefore K = K + \u03b5 2 I is used instead, implying \u03bb(K ) i \u2265 \u03b5 2 . In order to assess calibration we apply various differentiable kernels to the airline delay dataset from January 2020 . We compute the ln-ratio statistic2 ) for no calibration, calibration via Rayleigh quotient GP regression using \u00b5(n) as a prior mean, calibration by setting \u03c6 = \u03b5 2 and calibration using the average spectrum \u03c6 = \u03bb k+1:n . The averagew for 10 5 /n randomly sampled test problems is shown in .  Without any calibration the solver is generally overconfident. All tested calibration procedures reverse this, resulting in more cautious uncertainty estimates. We observe that Rayleigh quotient regression overcorrects for larger problems. This is due to the fact that its model correctly predicts K to be numerically singular from the dominant Rayleigh quotients, however it misses the information that the spectrum of K is bounded from below by \u03b5 2 . If we know the (average) of the remaining spectrum, significantly better calibration can be achieved, but often this information is not available. Nonetheless, since in this setting the majority of eigenvalues satisfy \u03bb(K ) i \u2248 \u03b5 2 by choosing \u03c6 = \u03c8 \u22121 = \u03b5 2 , we can get to the same degree of calibration. Therefore, we can improve the solver's uncertainty calibration at constant cost O(1) per iteration. For more general problems involving Gram matrices without damping we may want to rely on Rayleigh regression instead.Galerkin's Method for PDEs In the spirit of applying machine learning approaches to problems in the physical sciences and vice versa , we use Algorithm 1 for the approximate solution of a PDE via Galerkin's method . Consider the Dirichlet problem for the Poisson equation given bywhere \u2126 is a connected open region with sufficiently regular boundary and u \u2202\u2126 : \u2202\u2126 \u2192 R defines the boundary conditions. One obtains an approximate solution by projecting the weak formulation of the PDE to a finite dimensional subspace. This results in the Galerkin equation Au = f , i.e. a linear system where A is the Gram matrix of the associated bilinear form.  shows the induced uncertainty on the solution of the Dirichlet problem for f (x, y) = 15 and u \u2202\u2126 (x, y) = (x 2 \u2212 2y)  (1 + sin(2\u03c0x)). The mesh and corresponding Gram matrix were computed using FENICS . We can exploit two properties of Algorithm 1 in this setting. First, if we need to solve multiple related problems (A j , f j ) j , by solving a single problem we obtain an estimate of the solution to all other problems. We can successively use the posterior over the inverse as a prior for the next problem. This approach is closely related to subspace recycling in numerical linear algebra . Second, suppose we first compute a solution in a low-dimensional subspace corresponding to a coarse   shows that the approximation is better near the top boundary of \u2126. Given perfect uncertainty calibration,  represents a sample from N (0, I). The apparent structure in the plot and smaller than expected deviations in the upper part of \u2126 indicate the conservative confidence estimate of the solver.discretization for computational efficiency. We can then leverage the estimated solution to extrapolate to an (adaptively) refined discretization based on the posterior uncertainty. In machine learning lingo these two approaches can be viewed as forms of transfer learning.In this work, we condensed a line of previous research on probabilistic linear algebra into a selfcontained algorithm for the solution of linear problems in machine learning. We proposed first principles to constrain the space of possible generative models and derived a suitable covariance class. In particular, our proposed framework incorporates prior knowledge on the system matrix or its inverse and performs inference for both in a consistent fashion. Within our framework we identified parameter choices that recover the iterates of conjugate gradients in the mean, but add calibrated uncertainty around them in a computationally lightweight manner. To our knowledge our solver, available as part of the PROBNUM package, is the first practical implementation of this kind. In the final parts of this paper we showcased applications like kernel matrix inversion, where prior spectral information can be used for uncertainty calibration and outlined example use-cases for propagation of numerical uncertainty through computations. Naturally, there are also limitations remaining. While our theoretical framework can incorporate noisy matrix-vector product evaluations into its inference procedure via a Gaussian likelihood, practically tractable inference in the inverse model is more challenging. Our solver also opens up new research directions. In particular, our outlined regression model on the Rayleigh quotient may lead to a probabilistic model of the eigenspectrum. Finally, the matrix-based view of probabilistic linear solvers could inform probabilistic approaches to matrix decompositions, analogous to the way Lanczos methods are used in the classical setting.", "videoStruct": [{"timeStart": "00-00-12", "timeEnd": "00-00-15", "sentence": "systems in machine learning are typically large scale"}, {"timeStart": "00-00-15", "timeEnd": "00-00-19", "sentence": "they have corporate structure which arises from generative processes"}, {"timeStart": "00-00-19", "timeEnd": "00-00-20", "sentence": "and they may be subject to noise"}, {"timeStart": "00-00-21", "timeEnd": "00-00-23", "sentence": "think for example of a prediction model of regression models"}, {"timeStart": "00-00-23", "timeEnd": "00-00-27", "sentence": "we're a very large parameter space such as in deep learning"}, {"timeStart": "00-00-27", "timeEnd": "00-00-32", "sentence": "then if you do second order optimization the hessian matrix has a dimension of the parameter space"}, {"timeStart": "00-00-33", "timeEnd": "00-00-35", "sentence": "or in kernel methods"}, {"timeStart": "00-00-35", "timeEnd": "00-00-39", "sentence": "colonel actually induces certain spectral properties"}, {"timeStart": "00-00-39", "timeEnd": "00-00-41", "sentence": "of the gram matrix arising from the colonel"}, {"timeStart": "00-00-41", "timeEnd": "00-00-44", "sentence": "ideally would like to make use of this"}, {"timeStart": "00-00-44", "timeEnd": "00-00-45", "sentence": "generative information"}, {"timeStart": "00-00-45", "timeEnd": "00-00-50", "sentence": "and sometimes an empirical risk musician if you do batch optimization"}, {"timeStart": "00-00-50", "timeEnd": "00-00-53", "sentence": "evaluations will be subject to substantial noise"}, {"timeStart": "00-00-54", "timeEnd": "00-01-00", "sentence": "in this work we introduced a so-called pro ballistic linear solver which is a linear solver"}, {"timeStart": "00-01-00", "timeEnd": "00-01-02", "sentence": "that treats the task of solving the linear system"}, {"timeStart": "00-01-02", "timeEnd": "00-01-04", "sentence": "as an inference problem"}, {"timeStart": "00-01-04", "timeEnd": "00-01-09", "sentence": "so we proceed by putting a prior"}, {"timeStart": "00-01-09", "timeEnd": "00-01-10", "sentence": "either the matrix or the inverse"}, {"timeStart": "00-01-10", "timeEnd": "00-01-13", "sentence": "and the sole generative information about the matrix"}, {"timeStart": "00-01-15", "timeEnd": "00-01-18", "sentence": "and then you collect matrix vector product observations"}, {"timeStart": "00-01-19", "timeEnd": "00-01-23", "sentence": "you don't actually need to form the full matrix in memory suffices if you can multiply with a"}, {"timeStart": "00-01-24", "timeEnd": "00-01-27", "sentence": "that then results in a procedure solution over the matrix"}, {"timeStart": "00-01-28", "timeEnd": "00-01-31", "sentence": "who's mean is typically simple matrix plus low rank"}, {"timeStart": "00-01-32", "timeEnd": "00-01-34", "sentence": "apple Siri distribution over the inverse"}, {"timeStart": "00-01-34", "timeEnd": "00-01-36", "sentence": "and over the solution"}, {"timeStart": "00-01-36", "timeEnd": "00-01-39", "sentence": "now on a certain assumptions on the co variance"}, {"timeStart": "00-01-39", "timeEnd": "00-01-41", "sentence": "class that you use for this solder"}, {"timeStart": "00-01-41", "timeEnd": "00-01-46", "sentence": "you can actually recover the method of conjugate gradient switches state of the art for these types of very large scale linear systems"}, {"timeStart": "00-01-46", "timeEnd": "00-01-49", "sentence": "and we also obtain the same complexity class"}, {"timeStart": "00-01-51", "timeEnd": "00-01-57", "sentence": "when primary shortcoming of these types of producing linear solvers us that they are typically miss calibrated"}, {"timeStart": "00-01-57", "timeEnd": "00-01-59", "sentence": "mean that they don't faithfully represent"}, {"timeStart": "00-01-59", "timeEnd": "00-02-02", "sentence": "the error to the true solution in their uncertainty"}, {"timeStart": "00-02-03", "timeEnd": "00-02-09", "sentence": "we began by outlining a set of first principles for these types of solvers"}, {"timeStart": "00-02-09", "timeEnd": "00-02-11", "sentence": "and then derived a coherence class foot from them"}, {"timeStart": "00-02-11", "timeEnd": "00-02-13", "sentence": "which fulfilled as many properties as possible"}, {"timeStart": "00-02-14", "timeEnd": "00-02-19", "sentence": "now it turns out that this coherence class still has degrees of freedom which are analogous to hyper parameters"}, {"timeStart": "00-02-19", "timeEnd": "00-02-22", "sentence": "in a kernel for example in"}, {"timeStart": "00-02-22", "timeEnd": "00-02-23", "sentence": "gods process regression"}, {"timeStart": "00-02-24", "timeEnd": "00-02-28", "sentence": "and can use these reviews of freedom for uncertainty calibration"}, {"timeStart": "00-02-28", "timeEnd": "00-02-31", "sentence": "and the sport was show how you can use either prior spectral information to do this"}, {"timeStart": "00-02-31", "timeEnd": "00-02-36", "sentence": "or actually use already collected quantities during a run of the algorithm"}, {"timeStart": "00-02-36", "timeEnd": "00-02-38", "sentence": "to calibrate on certainty"}, {"timeStart": "00-02-40", "timeEnd": "00-02-45", "sentence": "promote sickliness all of us aren't instance of a larger class of methods so-called proposed numerical methods"}, {"timeStart": "00-02-45", "timeEnd": "00-02-48", "sentence": "such methods treat numerical tasks"}, {"timeStart": "00-02-48", "timeEnd": "00-02-49", "sentence": "as infants problems"}, {"timeStart": "00-02-49", "timeEnd": "00-02-52", "sentence": "this has multiple benefits such as in coding structure as we do here as well"}, {"timeStart": "00-02-52", "timeEnd": "00-02-59", "sentence": "or treating parameters of a new York medical methods from the principal pro ballistic perspective"}, {"timeStart": "00-03-01", "timeEnd": "00-03-08", "sentence": "this solver is implemented as part of a larger framework that implement such producing numerical methods called problem"}, {"timeStart": "00-03-08", "timeEnd": "00-03-12", "sentence": "and the future we aim to use the solver for things like black and methods for"}, {"timeStart": "00-03-12", "timeEnd": "00-03-14", "sentence": "dispositions of part of the French equations"}, {"timeStart": "00-03-14", "timeEnd": "00-03-18", "sentence": "or for empirical risk ms ation problems were noise plays role"}, {"timeStart": "00-03-22", "timeEnd": "00-03-24", "sentence": "Lena systems and machine learning"}, {"timeStart": "00-03-24", "timeEnd": "00-03-28", "sentence": "are large scale and have characteristic structure which arises from generative processes"}, {"timeStart": "00-03-28", "timeEnd": "00-03-33", "sentence": "this type of generous information can be encoded in the prayer of a pro ballistic linear solver"}, {"timeStart": "00-03-33", "timeEnd": "00-03-36", "sentence": "which then after certain number of iterations"}, {"timeStart": "00-03-36", "timeEnd": "00-03-39", "sentence": "using much smaller than the theoretical convergence rate of these methods"}, {"timeStart": "00-03-39", "timeEnd": "00-03-42", "sentence": "outputs a posterior over the solution"}, {"timeStart": "00-03-43", "timeEnd": "00-03-46", "sentence": "I'm certainty in codes their remaining"}, {"timeStart": "00-03-46", "timeEnd": "00-03-48", "sentence": "America uncertainty about the solution"}, {"timeStart": "00-03-48", "timeEnd": "00-03-50", "sentence": "because of the finite number of iterations"}, {"timeStart": "00-03-51", "timeEnd": "00-03-55", "sentence": "typical convergence rate you can find the paper on archives"}, {"timeStart": "00-03-55", "timeEnd": "00-03-58", "sentence": "can check out our experiments and reproduce them on github"}, {"timeStart": "00-03-58", "timeEnd": "00-04-01", "sentence": "or check out problem the code repository"}, {"timeStart": "00-04-01", "timeEnd": "00-04-02", "sentence": "where the method is implemented"}]}, {"title": "Intra-Processing Methods for Debiasing Neural Networks", "authors": "Abacus AI", "abstract": "", "publicationOrg": "NeurIPS", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2006.08564.pdf", "pdfPath": "/data/cache/1/PDFs/IntraProcessingMethodsforDebiasingNeuralNetworks.pdf", "publicationUrl": "https://arxiv.org/pdf/2006.08564.pdf", "codeUrl": "https://github.com/realityengines/post_hoc_debiasing", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/PcCj91K7jO0", "videoPath": "/data/cache/1/videos/Intra-Processing Methods for Debiasing Neural Networks.mp4", "pdfText": "The last decade has seen a huge increase in applications of machine learning in a wide variety of domains such as credit scoring, fraud detection, hiring decisions, criminal recidivism, loan repayment, face recognition, and so on . The outcome of these algorithms are impacting the lives of people more than ever. There are clear advantages in the automation of classification tasks, as machines can quickly process thousands of datapoints with many features. However, algorithms are susceptible to bias towards individuals or groups of people from a variety of sources .For example, facial recognition algorithms are currently being used by the US government to match application photos from people applying for visas and immigration benefits, to match mugshots, and to match photos as people cross the border into the USA . However, recent studies showed that many of these algorithms exhibit bias based on race and gender . For example, some of the algorithms were 10 or 100 times more likely to have false positives for Asian or Black people, compared to white people. When used for law enforcement, it means that a Black or Asian person is more likely to be arrested and detained for a crime they didn't commit .Motivated by the discovery of biased models in real-life applications, the last few years has seen a huge growth in the area of fairness in machine learning. Dozens of formal definitions of fairness have been proposed , and many algorithmic techniques have been developed for debiasing according to these definitions . Many debiasing algorithms fit into one of three categories: pre-processing, in-processing, or post-processing . Pre-processing techniques make changes to the data itself, in-processing techniques are methods for training machine learning models tailored to making fairer models, and post-processing techniques modify the final predictions outputted by a (biased) model. However, as datasets become larger and training becomes more computationally intensive, especially in the case of computer vision and natural language processing, it is becoming increasingly more common in applications to start with a very large pretrained model, and then fine-tune for the specific use-case . In fact, PyTorch offers several pretrained models, all of which have been trained for dozens of GPU hours on ImageNet . Pre-, in-, and post-processing debiasing methods are of little help here: pre-and in-processing methods would require retraining the entire model from scratch, and post-processing methods would not make use of the full power of the model.In this work, we initiate the study of intra-processing methods for debiasing neural networks. An intra-processing method is defined as an algorithm which has access to a trained model and a dataset (which typically differs from the original training dataset), and outputs a new model which gives debiased predictions on the target task (typically by updating or augmenting the weights of the original model). To see an overview of all the fairness debiasing algorithm classes, see . We propose three different intra-processing baseline algorithms, and we also show how to repurpose a popular in-processing algorithm  to the intra-processing setting. All of the algorithms we study work for any group fairness measure and any objective which trades off accuracy with bias.Our first baseline is a simple random perturbation algorithm, which iteratively adds multiplicative noise to the weights of the neural network and then picks the perturbation which maximizes the chosen objective. Our next baseline optimizes the weights of each layer using GBRT . Finally, we propose adversarial methods for fine-tuning. Adversarial training was recently used as an inprocessing method for debiasing , by training a critic model to predict the protected attribute of datapoints, to ensure that the predictions are not correlated with the protected attribute. We modify this approach to fit the intra-processing setting, and we also propose a new, more direct approach which trains a critic to directly measure the bias of the model weights, which gives us a differentiable proxy for bias, enabling the use of gradient descent for debiasing.We compare the four above techniques with three post-processing algorithms from prior work: reject option classification , equalized odds post-processing , and calibrated equalized odds post-processing . We run experiments with three fairness datasets from AIF360 , as well as the CelebA dataset , with three popular fairness definitions. We show that intra-processing is much more effective than post-processing for the fine-tuning use case. We also show that the difficulty of intra-processing and post-processing debiasing is highly dependent on the initial conditions of the original model. In particular, given a neural network trained to optimize accuracy, the variance in the amount of bias of the trained model is much higher than the variance in the accuracy, with respect to the random seed used for initializing the weights of the original model. Fairness research (and machine learning research as a whole) has seen a huge increase in popularity, and recent papers have highlighted the need for fair and reproducible results . To facilitate best practices, we run our experiments on the AIF360 toolkit  and open source all of our code.Our contributions. We summarize our main contributions below.  ). The classical classes are the pre-processing, the in-processing and the post-processing algorithms. We introduce the novel intra-processing algorithm class.\u2022 We initiate the study of intra-processing algorithms for debiasing ML models. This framework sits in between in-processing and post-processing algorithms, and is realistic for many finetuning use cases.\u2022 We study the nature of intra-processing techniques for debiasing neural networks, showing that the problem is sensitive to the initial conditions of the original model.\u2022 We propose three intra-processing algorithms, and we show how to repurpose popular inprocessing algorithms into the intra-processing setting. We compare all algorithms across a variety of group fairness constraints and datasets against three post-processing algorithms.Debiasing overview. There is a surging body of research on bias and fairness in machine learning. There are dozens of types of bias that can arise , and many formal definitions of fairness have been proposed . Popular definitions include statistical parity/demographic parity , equal opportunity (a subset of equalized odds) , and average absolute odds . For an overview of fairness definitions and techniques, see . Recently, the AIF360 toolkit was established to facilitate best practices in debiasing experiments . A meta-algorithm was also recently developed for in-processing debiasing by reducing fairness measures to convex optimization problems . Another work treats debiasing as an empirical risk minimization problem . Yet another work adds the fairness constraints as regularizers in the machine learning models . A recent work adaptively samples the training dataset during optimization in order to reduce disparate performance . There is also prior work using adversarial learning to debias algorithms . To the best of our knowledge, no prior work has designed an intra-processing algorithm using adversarial learning.Post-processing methods. Many post-processing debiasing algorithms have been proposed, which all assume black-box access to a biased classifier . We describe a few of these techniques in Section 6. Currently, most of these techniques have only been established for specific fairness measures. In natural language processing, there is recent work which decreases the bias present in pretrained models such as BERT or ELMo , however, these techniques were not shown to work on tabular or computer vision datasets.Debiasing in computer vision. A recent work studied methods for debiasing CelebA with respect to gender . However, the study was limited to in-processing techniques focusing on achieving the highest accuracy in light of distribution shift with respect to correlations present in the data. Another work considered counterfactual sensitivity analysis for detecting bias on the CelebA dataset, using the latent space in GANs . There is also recent work in measuring and mitigating bias in facial recognition software and related computer vision tasks .Deep learning algorithms are more prevalent now than ever before. The technology is becoming integrated into our society, and is being used in high-stakes applications such as criminal recidivism, loan repayment, and hiring decisions . It is also becoming increasingly evident that many of these algorithms are biased from various sources . Using technologies that makes prejudiced decisions for life-changing events will only deepen the divides that already exist in society. The need to address these issues is higher than ever . Our work seeks to decrease the negative effects that biased deep learning algorithms have on society. Intra-processing methods, which work for any group fairness measure, will be applicable to large existing deep learning models, since the networks need not be retrained from scratch. We present simple techniques (random perturbation) as well as more complex and strong techniques (adversarial fine-tuning) that work with most existing deep learning architectures. Our study on the nature of our proposed intra-processing debiasing methods compared to prior work, may also facilitate future work on related debiasing algorithms.Algorithms to mitigate bias such as the ones in our paper are an important part of creating inclusive and unbiased technology. However, as pointed out in other work , bias mitigation techniques must be part of a larger, socially contextualized project to examine the ethical considerations in deploying facial analysis models.In this section, we give notation and definitions used throughout the paper. Given a dataset split into three parts,features including one binary protected attribute A (e.g., identifying as female or not identifying as female), and Y i \u2208 {0, 1} is the label. Denote the value of the protected feature for x x x i as a i . We denote a trained neural network by a function f \u03b8 :, where \u03b8 denotes the trained weights. We often denote f \u03b8 (x x x i ) =\u0176 i , the output predicted probability for datapoint x x x i . Finally, we refer to the list of labels in a dataset D as Y.Fairness measures. We now give an overview of group fairness measures used in this work. Given a dataset D with labels Y, protected attribute A, and a list offrom some neural network f \u03b8 , we define the true positive and false positive rates asStatistical Parity Difference (SPD), or demographic parity difference , measures the difference in the probability of a positive outcome between the protected and unprotected groups. Formally,Equal opportunity difference (EOD)  measures the difference in TPR for the protected and unprotected groups. Equal opportunity is identical to equalized odds in the case where the protected feature and labels are binary. Formally, we haveAverage Odds Difference (AOD)  is defined as the average of the difference in the false positive rates and true positive rates for unprivileged and privileged groups. Formally,Optimization techniques. Zeroth order (non-differentiable) optimization is used when the objective function is not differentiable (as is the case for most definitions of group fairness). This is also called black-box optimization. Given an input space W and an objective function \u00b5, zeroth order optimization seeks to compute w * = arg min w\u2208W \u00b5(w). Leading methods for zeroth order optimization when function queries are expensive (such as optimizing a deep network) include gradient-boosted regression trees (GBRT)  and Bayesian optimization (BO) , however BO struggles with high-dimensional data. In contrast, first-order optimization is used when it is possible to take the derivative of the objective function. Gradient descent is an example of a first-order optimization technique.In this section, we describe three new intra-processing algorithms for debiasing neural networks. First we give more notation and formally define the different types of debiasing algorithms. Given a neural network f \u03b8 , where \u03b8 represents the weights, we sometimes drop the subscript \u03b8 when it is clear from context. We denote the last layer of f by f ( ) , and we assume thatwhere f is all but the last layer of the neural network. One of our three algorithms, layer-wise optimization, assumes that f is feed-forward, that is,  for functions f  , f  , . . . , f ( ) . The performance of the model is given by a performance measure \u03c1. For a set of data points D, given the list of true labels Y and the list of We also define a bias measure \u00b5, given as \u00b5(D,\u0176, A) \u2208 [0, 1], such as one defined in Section 4.The goal of any debiasing algorithm is to increase the performance \u03c1, while constraining the bias \u00b5. Many prior works have observed that fairness comes at the price of accuracy for many datasets, even when using large models such as deep networks . Therefore, a common technique is to maximize the performance subject to a constraint on the bias, e.g., \u00b5 < 0.05. Concretely, we define an objective function as follows.An in-processing debiasing algorithm takes as input the training and validation datasets and outputs a model f which seeks to maximize \u03c6 \u00b5,\u03c1, . An intra-processing algorithm takes in the validation dataset and a trained model f with weights \u03b8 (typically f was trained to optimize the performance \u03c1 on the training dataset), and outputs fine-tuned weights \u03b8 such that f \u03b8 maximizes the objective \u03c6 \u00b5,\u03c1, . A post-processing debiasing algorithm takes as input the validation dataset as well as a set of predictions\u0176 on the validation dataset (typically coming from a model f which was optimized for \u03c1 on the training dataset), and outputs a post-processing function h : [0, 1] \u2192 {0, 1} which is applied to the final output of the model so that the final predictions optimize \u03c6 \u00b5,\u03c1, . Note that intra-processing and post-processing debiasing algorithms are useful in different settings. Post-processing algorithms are useful when there is no access to the original model. Intra-processing algorithms are useful when there is access to the original model, or when the prediction is over a continuous feature. Now we present three new intra-processing techniques.Random perturbation. Our first intra-processing algorithm is a simple iterative random procedure, random perturbation. In every iteration, each weight in the neural network is multiplied by a Gaussian random variable with mean 1 and standard deviation 0.1. In case the model f outputs probabilities, we find the threshold \u03c4 such that\u0176 \u03c4 = [I{\u0176 i > \u03c4 }|\u0176 i \u2208\u0176] maximizes \u03c6 \u00b5,\u03c1, (Y,\u0176 \u03c4 , A). We run T iterations and output the perturbed weights that maximize \u03c6 \u00b5,\u03c1, on the validation set. See Algorithm 1. We show in the next section that despite its simplicity, this model performs well on many datasets and fairness measures, and therefore we recommend this algorithm as a baseline in future intra-processing debiasing applications. A natural follow-up question is whether we can do even better by using an optimization algorithm instead of random search. This is the motivation for our next approach.Layer-wise optimization. Our next method fine-tunes the model by debiasing individual layers using zeroth order optimization. Intuitively, an optimization procedure will be much more effective Select threshold \u03c4 \u2208 [0, 1] which maximizes the objective \u03c6 \u00b5,\u03c1, on the validation set 7:If val > val * , set val * = val, \u03b8 * = \u03b8 , and \u03c4 * = \u03c4 . 9: end for 10: Output: \u03b8 * , \u03c4 * than random perturbations. However, zeroth order optimization can be computationally expensive and does not scale well, so instead we only run the optimization on individual layers. Given a model, assume the model can be decomposed into several functions  For example, a feed-forward neural network with layers can be decomposed in this way. We denote the trained weights of each component by \u03b8 1 , . . . , \u03b8 , respectively. Now assume that we have access to a zeroth order optimizer A, which takes as input a model  , weights \u03b8 = (\u03b8 1 , . . . , \u03b8 ), dataset D valid , and an index i. The optimizer returns weights \u03b8 i , optimized with respect to \u03c6 \u00b5,\u03c1, . In Algorithm 2, we set the optimizer to be gradient-boosted regression trees (GBRT) , a leading technique for black box optimization which converts shallow regression trees into strong learners. GBRT iteratively constructs a posterior predictive model using the weights to make predictions and uncertainty estimates for each potential set of weights \u03b8 . To trade off exploration and exploitation, the next set of weights to try is chosen using lower confidence bounds (LCB), a popular acquisition function (e.g., ). Formally, \u03c6 LCB (\u03b8 ) =\u03b8 \u2212 \u03b2\u03c3, in which we assume our model's posterior predictive density follows a normal distribution with mean\u03b8 and standard deviation\u03c3. \u03b2 is a tradeoff parameter that can be tuned. See Algorithm 2. Note that this algorithm can be easily generalized to optimize multiple layers at once, but this comes at the price of runtime. For example, running GBRT on the entire neural network would be more powerful than the random permutation algorithm but is prohibitively expensive.  with weights \u03b8 1 , . . . , \u03b8 , objective \u03c6 \u00b5,\u03c1, , black-box optimizer A 2: Set \u03b8 * = \u2205, val * = \u2212\u221e, and \u03c4 * = 0 3: for i = 1 to doRun optimizer A to optimize weights \u03b8 i to \u03b8 i with respect to \u03c6 \u00b5,\u03c1, .Select threshold \u03c4 \u2208 [0, 1] which maximizes objective \u03c6 \u00b5,\u03c1, :If val > val * set val * = val, \u03b8 * = \u03b8 , and \u03c4 * = \u03c4 . 8: end for 9: Output: \u03b8 * , \u03c4 * Adversarial fine-tuning. The previous two methods rely on zeroth order optimization techniques because most group fairness measures such as statistical parity difference and equalized odds are non-differentiable. Our last technique casts the problem of debiasing as first-order optimization by using adversarial learning. The idea behind the adversarial method is that we train a critic model to predict the amount of bias in a minibatch. We sample the datapoints in a minibatch randomly and with replacement. This statistical bootstrapping approach to creating a minibatch means that if the critic can predict the bias in a minibatch accurately, then it can predict the bias in the model with respect to the validation set reasonably well. Therefore, the critic effectively acts as a differentiable proxy for bias, which makes it possible to debias the original model using back-propagation.The adversarial algorithm works by alternately iterating between training the critic model g using the predictions from f , and fine-tuning the predictive model f with respect to a custom function designed to be differentiable while still maximizing the non-differentiable objective function \u03c6 \u00b5,\u03c1, using the bias proxy\u03bc from g. The custom function we use to emulate the objective function while still being differentiable is given by max{1, \u03bb(|\u03bc| \u2212 + \u03b4) + 1} \u2022 Loss(y,\u0177) where\u0177 is the predicted label, y is the real label, \u03bb, \u03b4 are hyperparameters, and Loss is a generic loss function such as binary cross-entropy. This function multiplies the task specific loss by a coefficient that is 1 if the absolute bias is less than \u2212 \u03b4, otherwise the coefficient is \u03bb(|\u03bc| \u2212 + \u03b4) + 1. Intuitively, this custom function optimizes the task specific loss subject to the absolute bias less than \u2212 \u03b4. The hyperparameter \u03bb describes how strict the bias constraint should be, and the hyperparameter \u03b4 describes the margin of error in the bias we want the algorithm to maintain. Note that the g concatenates the examples in the minibatch and returns a single number that estimates the bias of the minibatch as the final output. See Algorithm 3. Note that BCELoss denotes the standard binary cross-entropy loss. for j = 1 to m doSample a minibatch (X X X j , Y Y Y j ) with replacement from D validEvaluate the bias in the minibatch,\u03bc \u2190 \u00b5((X X X j , Y Y Y j ), f (X X X j )).Update the critic model g by updating its stochastic gradientend for 9:for j = 1 to m doSample a minibatch (X X X j , Y Y Y j ) with replacement from D validUpdate the original model by updating its stochastic gradientend forSelect threshold \u03c4 \u2208 [0, 1] that minimizes the objective \u03c6 \u00b5,\u03c1, 14: end for 15: Output: Debiased model f , threshold \u03c4 Converting in-processing into intra-processing. Since both in-processing and intra-processing algorithms optimize the weights of the neural network while training, it may be possible to convert existing in-processing algorithms into intra-processing algorithms. However, the in-processing algorithm needs to be able to run on a generic neural architecture, and there cannot be any specific weight-initialization step (since it is given a pretrained model as the starting point). Furthermore, the hyperparameters of the in-processing algorithm may need to be modified to better fit the fine-tuning paradigm. For example, the learning rate should be lowered, and the optimizer's influence on the earlier layers that are unlikely to contribute as much to the final result should be limited.As an instructive example, we modify a popular in-processing fairness algorithm  to convert it to the intra-processing setting. This algorithm relies on a similar adversarial paradigm to our adversarial fine-tuning algorithm. The fundamental difference between the algorithm of  and our Algorithm 3 is that their algorithm uses the critic to predict the protected attribute and not to directly predict the bias for the minibatch. As a result, we modify our adversarial fine-tuning algorithm so the critic predicts the protected attribute, and we change the optimization procedure to the one provided by the original work. Finally we modify the hyperparameters so that they are better suited to the fine-tuning use case. For instance, we use a lower learning rate when fine-tuning the model.In this section, we experimentally evaluate the techniques laid out in Section 5 compared to baselines, on four datasets and with multiple fairness measures. To promote reproducibility, we release our code at https://github.com/abacusai/intraprocessing_debiasing and we use datasets from the AIF360 toolkit  and a popular image dataset. Each dataset contains one or more binary protected feature(s) and a binary label. We briefly describe them below.Tabular datasets. The COMPAS dataset is a commonly used dataset in fairness research, consisting of over 10,000 defendants with 402 features . The goal is to predict the recidivism likelihood for an individual . We run separate experiments using race and also gender as protected attributes. The Adult Census Income (ACI) dataset is a binary classification dataset from the 1994 USA Census bureau database in which the goal is to predict whether a person earns above $50,000 . There are over 40,000 data points with 15 features. We use gender and race as the protected attribute. The Bank Marketing (BM) dataset is from the phone marketing campaign of a Portuguese bank. There are over 48,000 datapoints consisting of 17 categorical and quantitative features. The goal is to predict whether a customer will subscribe to a product . The protected feature is whether or not the customer is older than 25.The CelebA dataset. The CelebA dataset  is a popular image dataset used in computer science research. This dataset consists of over 200,000 images of celebrity head-shots, along with binary attributes such as \"smiling\", \"young\", and \"gender\". As pointed out in other papers , the binary categorization of attributes such as gender, hair color, and age, does not reflect true human diversity, and is problematic . Furthermore, some binary attributes present in CelebA such as \"attractive\" and \"chubby\" involve pejorative judgements which may cause harm .In our experiments on CelebA, we choose to focus on two models. One model predicts whether or not the person is classified as young, and the other predicts whether the person is classified as smiling. As pointed out in prior work, smiling detection has a host of potential positive applications with limited negative applications .We set the protected attribute to Fitzpatrick skin tones in range , as done in prior work investigating inequity in computer vision models . The Fitzpatrick skin type scale  consists of six types of skin tones, and generally tones 4-6 are darker than 1-3. To label all 200,000 attributes, we used a pretrained classifier . We removed all images which were not predicted as type 1-3 or type 4-6 with at least 70% probability, which left us with roughly 180000 images. Finally, we manually verified the correct classification of 1000 random images.The need for neural networks. First, we run a quick experiment to demonstrate the need for neural networks on the tabular datasets above. Deep learning has become a very popular approach in the field of machine learning , however, for tabular datasets with fewer than 20 features, it is worth checking whether logistic regression or random forest techniques perform as well as neural networks . We construct a neural network with 10 fully-connected layers, BatchNorm for regularization, and a dropout rate of 0.2, and we compare this to logistic regression and a random forest model on the ACI dataset. We see that a neural network achieves accuracy and area under the receiver operating characteristic curve (AUC ROC) scores which are 2% higher than the other models. See Appendix A for the full results. Therefore, we focus on debiasing the neural network implementation.Bias sensitivity to initial model conditions. We run experiments to compute the amount of variance in the bias scores of the initial models. Neural networks have a large number of local minima . Hyperparameters such as the optimizer and learning rate, and even the initial random seed, cause the model to converge to different local minima . Techniques such as the Adam optimizer and early stopping with patience have been designed to allow neural networks to consistently reach local minima with high accuracies . However, there is no guarantee on the consistency of bias across the local minima. In particular, the local minima found by neural networks may have large differences in the amount of bias, and therefore, there may be very high variance on the amount of bias exhibited by neural networks just because of the random seed. Every local optima corresponds to a different set of weights. If the weights of the model at a specific local optimum rely heavily on the protected feature, removing the bias from such a model by updating the weights would be harder than removing the bias from a model whose weights do not rely on the protected feature as heavily. We compute the mean and the standard deviation of three fairness bias measures, as well as accuracy, for a neural network trained with 10 different initial random seeds, across three datasets. See . We see that the standard deviation of the bias score is an order of magnitude higher than the standard deviation of the accuracy. In Appendix A, we also plot the contribution of each individual weight to the bias score, for a neural network. We show that the contribution of the weights to the bias score are sensitive to the initial random seed which means we cannot know the weights that are most likely to contribute to the bias before training the model even if we have another identical model trained using a different seed.Now we present our main experimental study by comparing four intra-processing and three postprocessing debiasing methods across four datasets and three fairness measures. This includes one  : Results for tabular datasets over 5 trials. We plot the mean bias with std error bars (top), and the median value of the objective function in Equation 1 (bottom). Note that we report the median because Equation 1 has a large discontinuity.  in-processing algorithm that we have adapted to the intra-processing setting. First we briefly describe the baseline post-processing algorithms that we tested.The reject option classification post-processing algorithm  defines a critical region of points in the protected group whose predicted probability is near 0.5, and flips these labels. This algorithm is designed to minimize statistical parity difference. The equalized odds post-processing algorithm  defines a convex hull based on the bias rates of different groups, and then flips the label of data points that fall inside the convex hull. This algorithm is designed to minimize equal opportunity difference. The calibrated equalized odds post-processing algorithm  defines a base rate of bias for each group, and then adds randomness based on the group into the classifier until the bias rates converge. This algorithm is also designed to minimize equal opportunity difference. For these algorithms, we use the implementations in the AIF360 repository . Now we explain the experimental setup for the tabular datasets. Our initial model consists of a feed-forward neural network with 10 fully-connected layers of size 32, with a BatchNorm layer between each fully-connected layer, and a dropout fraction of 0.2. The model is trained with the Adam optimizer and an early-stopping patience of 100 epochs. The loss function is the binary cross-entropy loss. We use the validation data as the input for the intra-processing methods, with the objective function set to Equation 1 with = 0.05. We modified the hyperparameters so that each method took roughly 30 minutes. We run each algorithm on 5 neural networks initialized with different random seeds and aggregate the results. We publish the median objective scores for the tabular datasets as the mean would not accurately portray the expected results since some runs may return an objective score of 0.0. We also publish the mean and std error bars for the bias scores. We limit the plots to those for which the default biased algorithm did not achieve a positive objective score. See .Finally, we run experiments on the CelebA dataset. We use a ResNet18 architecture  pretrained on ImageNet from the PyTorch library  as our initial model. Then we run 10 epochs of fine-tuning the architecture to predict the \"young\" attribute (or the \"smiling\" attribute) from the CelebA dataset using 40,000 random images. To perform fine-tuning, we freeze all of the convolutional layers, only updating the final fully-connected layers. Then we run each of the debiasing algorithms. We consider the default model, as well as the three post-processing algorithms and the three novel intra-processing algorithms. As in the previous section, we set the objective function to Equation 1 with = 0.05. See . In order to give a qualitative comparison, we also publish a series of images along with the predicted probability that the celebrity is smiling before and after applying the adversarial fine-tuning model. See .Discussion. We see that the intra-processing methods significantly outperform the post-processing methods, sometimes even on the fairness metric for which the post-processing method was designed. We note that there are two caveats. First, the three intra-processing methods had access to the objective function in Equation 1, while the post-processing methods are only designed to minimize their respective fairness measures. However, as seen in , sometimes the intra-processing methods simultaneously achieve higher objective scores and lower bias compared to the postprocessing methods, making the intra-processing methods dominate them pareto-optimally. Second, intra-processing methods are more powerful than post-processing methods, since post-processing methods do not modify the weights of the original model. Post-processing methods are more appropriate when the model weights are unavailable or when computation time is constrained, and intra-processing methods are more appropriate when higher performance is desired. We find that all the intra-processing algorithms tend to do well on the tabular datasets. However making sure that their bias scores remain below the threshold on the test is not as easy as there were not enough rows. Using regularization techniques helped to ensure that the scores remained consistent even over the test set. We find that when the dataset and the model become more complex as is the case with the CelebA dataset and the ResNet model the more complex algorithms like adversarial fine-tuning tend to perform better than the random perturbation and layerwise optimization algorithm. This indicates that when dealing with more complex datasets and models, using complex intra-processing models like adversarial fine-tuning may be a better fit for the problem.In this work, we initiate the study of a new paradigm in debiasing research, intra-processing, which sits between in-processing and post-processing methods, and is designed for fairly fine-tuning large models. We define three new intra-processing algorithms: random perturbation, adversarial finetuning, and layer-wise optimization, and we repurpose a popular in-processing algorithm to work for intra-processing. In our experimental study, first we show that the amount of bias is sensitive to the initial conditions of the original neural network. Then we give an extensive comparison of four intra-processing methods and three post-processing methods across three tabular datasets, one image dataset, and three popular fairness measures. We show that the intra-processing algorithms outperform the post-processing methods.  : Results for coefficient analysis.the stacked matrix with singular value decomposition. The singular values of the matrix measure the degree of linear independence between the coefficients for the 10 linear models. As we see from  (right), the singular values are all close to 1. This indicates that the coefficients are all relatively different from one another. This means that the parameters of the 10 neural networks that correspond to the bias are different for each network indicating that each time we train a model, even if it has the same architecture, the parameters that contribute to bias are different.Tabular Intra-Processing Debiasing Experiments. Now we give more results for the intraprocessing debiasing experiments from Section 6. The experimental setting is the same as in the tabular data experiments from Section 6 ( ). We give even more combinations of dataset, bias measures, and protected attributes. See . Note that the results in this figure are a superset of the results from . These additional results show that different debiasing measures work better in different situations.   , with even more combinations of dataset, bias measure, and protected attribute. Over 5 runs with different seeds, we report mean bias with std error bars (top) and the median of the objective function in Equation 1 (bottom). Note that we report the median because Equation 1 has a large discontinuity.", "videoStruct": [{"timeStart": "00-00-00", "timeEnd": "00-00-06", "sentence": "hi I'm your so funny and I'll be presenting our paper on inter processing methods for device in neural networks"}, {"timeStart": "00-00-06", "timeEnd": "00-00-10", "sentence": "this is joint work that call in lights and the beams undergo into Roger"}, {"timeStart": "00-00-10", "timeEnd": "00-00-15", "sentence": "contemporary fairness algorithms are usually divided into three distinct classes"}, {"timeStart": "00-00-15", "timeEnd": "00-00-19", "sentence": "pre processing algorithms transform the data used to train the model"}, {"timeStart": "00-00-19", "timeEnd": "00-00-24", "sentence": "in processing algorithms modify the model architecture and training details"}, {"timeStart": "00-00-24", "timeEnd": "00-00-28", "sentence": "and post processing algorithms also the outputs of the free trade model"}, {"timeStart": "00-00-29", "timeEnd": "00-00-37", "sentence": "these approaches either need access to the entire model training pipeline or rely on a separate model built on top of our black box pre train model"}, {"timeStart": "00-00-37", "timeEnd": "00-00-42", "sentence": "none of these algorithms are optimized to directly adjust the parameters of a preteen model"}, {"timeStart": "00-00-42", "timeEnd": "00-00-48", "sentence": "we introduce a fourth class of fairness algorithm is that we call inter processing algorithms"}, {"timeStart": "00-00-48", "timeEnd": "00-00-52", "sentence": "these algorithms sit in between the in processing and post processing algorithms"}, {"timeStart": "00-00-52", "timeEnd": "00-00-59", "sentence": "inter processing algorithms finds unit pre train network on a separate data set to maximize an objective function"}, {"timeStart": "00-00-59", "timeEnd": "00-01-05", "sentence": "the objective function requires that the bias be below a pre specified Epsilon threshold"}, {"timeStart": "00-01-05", "timeEnd": "00-01-09", "sentence": "we argue that these algorithms are very relevant to contemporary machine learning technologies"}, {"timeStart": "00-01-09", "timeEnd": "00-01-18", "sentence": "as he acknowledges tender around large pre training deep learning models that cannot be retrained to minimize bias due to the productive cost involved"}, {"timeStart": "00-01-19", "timeEnd": "00-01-27", "sentence": "we introduced three inter processing three novel inter processing algorithms and modify one popular in processing algorithm to make it in"}, {"timeStart": "00-01-27", "timeEnd": "00-01-31", "sentence": "the first novel inter processing algorithm is the random perturbation algorithm"}, {"timeStart": "00-01-31", "timeEnd": "00-01-35", "sentence": "this algorithm randomly perturbed the weights of a pre train network"}, {"timeStart": "00-01-35", "timeEnd": "00-01-39", "sentence": "and selects the perturbation that maximizes the objective"}, {"timeStart": "00-01-39", "timeEnd": "00-01-44", "sentence": "the second novel inter processing method is the layer wise optimization device algorithm"}, {"timeStart": "00-01-44", "timeEnd": "00-01-52", "sentence": "this algorithm relies on a black box of the miser to adjust the weights of a specific layer in the neural network that maximizes the objective"}, {"timeStart": "00-01-53", "timeEnd": "00-01-58", "sentence": "the final no linter processing algorithm is the adversarial device in algorithm"}, {"timeStart": "00-01-58", "timeEnd": "00-02-01", "sentence": "this algorithm employs two auxiliary network heads"}, {"timeStart": "00-02-01", "timeEnd": "00-02-05", "sentence": "one evaluates the bias of the main network on a mini match"}, {"timeStart": "00-02-05", "timeEnd": "00-02-09", "sentence": "and the second head evaluates the task specific loss of the net network"}, {"timeStart": "00-02-09", "timeEnd": "00-02-16", "sentence": "the model is subsequently trained using back propagation to maximize the objective function using the output from both heads"}, {"timeStart": "00-02-16", "timeEnd": "00-02-22", "sentence": "we also modify a popular in processing algorithm based on work from song at all to make it into processing"}, {"timeStart": "00-02-23", "timeEnd": "00-02-31", "sentence": "we evaluate all these inter processing methods along with three popular post processing algorithms on a series of data sets"}, {"timeStart": "00-02-31", "timeEnd": "00-02-32", "sentence": "based on"}, {"timeStart": "00-02-32", "timeEnd": "00-02-35", "sentence": "the work from after six thinking bae sing toolkit"}, {"timeStart": "00-02-36", "timeEnd": "00-02-43", "sentence": "we also evaluate all but the modifying processing algorithm on an image dataset"}, {"timeStart": "00-02-43", "timeEnd": "00-02-46", "sentence": "built around the popular show that a day is set"}, {"timeStart": "00-02-46", "timeEnd": "00-02-54", "sentence": "overall we find that using inter processing methods can significantly improve the fairness results when fine tuning pre train models"}, {"timeStart": "00-02-54", "timeEnd": "00-02-58", "sentence": "please see our poster or paper for more information"}]}, {"title": "[CVPR 2020 Award Nominee] Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Sele...", "authors": "ComputerVisionFoundation Videos", "abstract": "", "publicationOrg": "CVPR", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/1912.02424.pdf", "pdfPath": "/data/cache/1/PDFs/CVPR2020AwardNomineeBridgingtheGapBetweenAnchorBasedandAnchorFreeDetectionviaAdaptiveTrainingSampleSele.pdf", "publicationUrl": "https://arxiv.org/pdf/1912.02424.pdf", "codeUrl": "https://github.com/sfzhang15/ATSS.", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/CnchEvVhI3c", "videoPath": "/data/cache/1/videos/Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Sele....mp4", "pdfText": "Object detection is a long-standing topic in the field of computer vision, aiming to detect objects of predefined categories. Accurate object detection would have far reaching impact on various applications including image recognition and video surveillance. In recent years, with the development of convolutional neural network (CNN), object detection has been dominated by anchor-based detectors, which can be generally divided into one-stage methods  and two-stage methods . Both of them first tile a large number of preset anchors on the image, then predict the category and refine the coordinates of these anchors by one or several times, finally output these refined anchors as detection results. Because two-stage methods refine anchors several times more than one-stage methods, the former one has more accurate results while the latter one has higher computational efficiency. State-of-the-art results on common detection benchmarks are still held by anchor-based detectors.Recent academic attention has been geared toward anchor-free detectors due to the emergence of FPN  and Focal Loss . Anchor-free detectors directly find objects without preset anchors in two different ways. One way is to first locate several pre-defined or self-learned keypoints and then bound the spatial extent of objects. We call this type of anchor-free detectors as keypoint-based methods . Another way is to use the center point or region of objects to define positives and then predict the four distances from positives to the object boundary. We call this kind of anchor-free detectors as center-based methods . These anchor-free detectors are able to eliminate those hyperparameters related to anchors and have achieved similar performance with anchor-based detectors, making them more potential in terms of generalization ability.Among these two types of anchor-free detectors, keypoint-based methods follow the standard keypoint estimation pipeline that is different from anchor-based detectors. However, center-based detectors are similar to anchorbased detectors, which treat points as preset samples instead of anchor boxes. Take the one-stage anchor-based detector RetinaNet  and the center-based anchor-free detector FCOS  as an example, there are three main differences between them: (1) The number of anchors tiled per location. RetinaNet tiles several anchor boxes per location, while FCOS tiles one anchor point 1 per location.  The definition of positive and negative samples. RetinaNet resorts to the Intersection over Union (IoU) for positives and negatives, while FCOS utilizes spatial and scale constraints to select samples.  The regression starting status. Reti-naNet regresses the object bounding box from the preset anchor box, while FCOS locates the object from the anchor point. As reported in , the anchor-free FCOS achieves much better performance than the anchor-based RetinaNet, it is worth studying which of these three differences are essential factors for the performance gap.In this paper, we investigate the differences between anchor-based and anchor-free methods in a fair way by strictly ruling out all the implementation inconsistencies between them. It can be concluded from experiment results that the essential difference between these two kind of methods is the definition of positive and negative training samples, which results in the performance gap between them. If they select the same positive and negative samples during training, there is no obvious gap in the final performance, no matter regressing from a box or a point. Therefore, how to select positive and negative training samples deserves further study. Inspired by that, we propose a new Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples based on object characteristics. It bridges the gap between anchorbased and anchor-free detectors. Besides, through a series of experiments, a conclusion can be drawn that tiling multiple anchors per location on the image to detect objects is not necessary. Extensive experiments on the MS COCO  dataset support our analysis and conclusions. State-of-theart AP 50.7% is achieved by applying the newly introduced ATSS without introducing any overhead. The main contributions of this work can be summarized as:\u2022 Indicating the essential difference between anchorbased and anchor-free detectors is actually how to define positive and negative training samples.\u2022 Proposing an adaptive training sample selection to automatically select positive and negative training samples according to statistical characteristics of object.\u2022 Demonstrating that tiling multiple anchors per location on the image to detect objects is a useless operation.\u2022 Achieving state-of-the-art performance on MS COCO without introducing any additional overhead.Current CNN-based object detection consists of anchorbased and anchor-free detectors. The former one can be divided into two-stage and one-stage methods, while the latter one falls into keypoint-based and center-based methods.Two-stage method. The emergence of Faster R-CNN  establishes the dominant position of two-stage anchorbased detectors. Faster R-CNN consists of a separate region proposal network (RPN) and a region-wise prediction network (R-CNN)  to detect objects. After that, lots of algorithms are proposed to improve its performance, including architecture redesign and reform , context and attention mechanism , multiscale training and testing , training strategy and loss function , feature fusion and enhancement , better proposal and balance . Nowadays, state-of-the-art results are still held by two-stage anchorbased methods on standard detection benchmarks. One-stage method. With the advent of SSD , onestage anchor-based detectors have attracted much attention because of their high computational efficiency. SSD spreads out anchor boxes on multi-scale layers within a ConvNet to directly predict object category and anchor box offsets. Thereafter, plenty of works are presented to boost its performance in different aspects, such as fusing context information from different layers , training from scratch , introducing new loss function , anchor refinement and matching , architecture redesign , feature enrichment and alignment . At present, one-stage anchor-based methods can achieve very close performance with two-stage anchor-based methods at a faster inference speed.Keypoint-based method. This type of anchor-free method first locates several pre-defined or self-learned keypoints, and then generates bounding boxes to detect objects. Cor-nerNet  detects an object bounding box as a pair of keypoints (top-left corner and bottom-right corner) and CornerNet-Lite  introduces CornerNet-Saccade and CornerNet-Squeeze to improve its speed. The second stage of Grid R-CNN  locates objects via predicting grid points with the position sensitive merits of FCN and then determining the bounding box guided by the grid. Ex-tremeNet  detects four extreme points (top-most, leftmost, bottom-most, right-most) and one center point to generate the object bounding box. Zhu et al.  use keypoint estimation to find center point of objects and regress to all other properties including size, 3D location, orientation and pose. CenterNet  extends CornetNet as a triplet rather than a pair of keypoints to improve both precision and recall. RepPoints  represents objects as a set of sample points and learns to arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. Center-based method. This kind of anchor-free method regards the center (e.g., the center point or part) of object as foreground to define positives, and then predicts the distances from positives to the four sides of the object bounding box for detection. YOLO  divides the image into an S \u00d7 S grid, and the grid cell that contains the center of an object is responsible for detecting this object. DenseBox  uses a filled circle located in the center of the object to define positives and then predicts the four distances from positives to the bound of the object bounding box for location. GA-RPN  defines the pixels in the center region of the object as positives to predict the location, width and height of object proposals for Faster R-CNN. FSAF  attaches an anchor-free branch with online feature selection to RetinaNet. The newly added branch defines the center region of the object as positives to locate it via predicting four distances to its bounds. FCOS  regards all the locations inside the object bounding box as positives with four distances and a novel centerness score to detect objects. CSP  only defines the center point of the object box as positives to detect pedestrians with fixed aspect ratio. Fove-aBox  regards the locations in the middle part of object as positives with four distances to perform detection.Without loss of generality, the representative anchorbased RetinaNet  and anchor-free FCOS  are adopted to dissect their differences. In this section, we focus on the last two differences: the positive/negative sample definition and the regression starting status. The remaining one difference: the number of anchors tiled per location, will be discussed in subsequent section. Thus, we just tile one square anchor per location for RetinaNet, which is quite similar to FCOS. In the remaining part, we first introduce the experiment settings, then rule out all the implementation inconsistencies, finally point out the essential difference between anchor-based and anchor-free detectors.Dataset. All experiments are conducted on the challenging MS COCO  dataset that includes 80 object classes. Following the common practice , all 115K images in the trainval35k split is used for training, and all 5K images in the minival split is used as validation for analysis study. We also submit our main results to the evaluation server for the final performance on the test-dev split. Training Detail. We use the ImageNet  pretrained ResNet-50  with 5-level feature pyramid structure as the backbone. The newly added layers are initialized in the same way as in . For RetinaNet, each layer in the 5-level feature pyramid is associated with one square anchor with 8S scale, where S is the total stride size. During training, we resize the input images to keep their shorter side being 800 and their longer side less or equal to 1, 333. The whole network is trained using the Stochastic Gradient Descent (SGD) algorithm for 90K iterations with 0.9 momentum, 0.0001 weight decay and 16 batch size. We set the initial learning rate as 0.01 and decay it by 0.1 at iteration 60K : Analysis of implementation inconsistencies between RetinaNet and FCOS on MS COCO minival set. \"#A=1\" means there is one square anchor box per location. Inference Detail. During the inference phase, we resize the input image in the same way as in the training phase, and then forward it through the whole network to output the predicted bounding boxes with a predicted class. After that, we use the preset score 0.05 to filter out plenty of background bounding boxes, and then output the top 1000 detections per feature pyramid. Finally, the Non-Maximum Suppression (NMS) is applied with the IoU threshold 0.6 per class to generate final top 100 confident detections per image.We mark the anchor-based detector RetinaNet with only one square anchor box per location as RetinaNet (#A=1), which is almost the same as the anchor-free detector FCOS. However, as reported in , FCOS outperforms RetinaNet (#A=1) by a large margin in AP performance on the MS COCO minival subset, i.e., 37.1% vs. 32.5%. Furthermore, some new improvements have been made for FCOS including moving centerness to regression branch, using GIoU loss function and normalizing regression targets by corresponding strides. These improvements boost the AP performance of FCOS from 37.1% to 37.8% 2 , making the gap even bigger. However, part of the AP gap between the anchor-based detector (32.5%) and the anchor-free detector (37.8%) results from some universal improvements that are proposed or used in FCOS, such as adding GroupNorm  in heads, using the GIoU  regression loss function, limiting positive samples in the ground-truth box , introducing the centerness branch  and adding a trainable scalar  for each level feature pyramid. These improvements can also be applied to anchor-based detectors, therefore they are not the essential differences between anchorbased and anchor-free methods. We apply them to Reti-naNet (#A=1) one by one so as to rule out these implementation inconsistencies. As listed in , these irrelevant differences improve the anchor-based RetinaNet to 37.0%, which still has a gap of 0.8% to the anchor-free FCOS. By now, after removing all the irrelevant differences, we can explore the essential differences between anchor-based and anchor-free detectors in a quite fair way.After applying those universal improvements, these are only two differences between the anchor-based RetinaNet (#A=1) and the anchor-free FCOS. One is about the classification sub-task in detection, i.e., the way to define positive and negative samples. Another one is about the regression sub-task, i.e., the regression starting from an anchor box or an anchor point. Classification. As shown in (a), RetinaNet utilizes IoU to divide the anchor boxes from different pyramid levels into positives and negatives. It first labels the best anchor box of each object and the anchor boxes with IoU > \u03b8 p as positives, then regards the anchor boxes with IoU < \u03b8 n as negatives, finally other anchor boxes are ignored during training. As shown in (b), FCOS uses spatial and scale constraints to divide the anchor points from different pyramid levels. It first considers the anchor points within the ground-truth box as candidate positive samples, then selects the final positive samples from candidates based on the scale range defined for each pyramid level  , finally those unselected anchor points are negative samples.As shown in , FCOS first uses the spatial constraint to find candidate positives in the spatial dimension, then uses the scale constraint to select final positives in the scale dimension. In contrast, RetinaNet utilizes IoU to directly select the final positives in the spatial and scale dimension simultaneously. These two different sample selec-  tion strategies produce different positive and negative samples. As listed in the first column of  for RetinaNet (#A=1), using the spatial and scale constraint strategy instead of the IoU strategy improves the AP performance from 37.0% to 37.8%. As for FCOS, if it uses the IoU strategy to select positive samples, the AP performance decreases from 37.8% to 36.9% as listed in the second column of . These results demonstrate that the definition of positive and negative samples is an essential difference between anchor-based and anchor-free detectors.Regression. After positive and negative samples are determined, the location of object is regressed from positive samples as shown in (a). RetinaNet regresses from the anchor box with four offsets between the anchor box and the object box as shown in (b), while FCOS regresses from the anchor point with four distances to the bound of object as shown in (c). It means that for a positive sample, the regression starting status of RetinaNet is a box while FCOS is a point. However, as shown in the first and second rows of , when RetinaNet and FCOS adopt the same sample selection strategy to have consistent positive/negative samples, there is no obvious difference in final performance, no matter regressing starting from a point or a box, i.e., 37.0% vs. 36.9% and 37.8% vs. 37.8%. These results indicate that the regression starting status is an irrelevant difference rather than an essential difference.Conclusion. According to these experiments conducted in a fair way, we indicate that the essential difference between one-stage anchor-based detectors and center-based anchorfree detectors is actually how to define positive and negative training samples, which is important for current object detection and deserves further study.When training an object detector, we first need to define positive and negative samples for classification, and then use positive samples for regression. According to the previous analysis, the former one is crucial and the anchorfree detector FCOS improves this step. It introduces a new way to define positives and negatives, which achieves better performance than the traditional IoU-based strategy. Inspired by this, we delve into the most basic issue in object detection: how to define positive and negative training samples, and propose an Adaptive Training Sample Selection (ATSS). Compared with these traditional strategies, our method almost has no hyperparameters and is robust to different settings.Previous sample selection strategies have some sensitive hyperparameters, such as IoU thresholds in anchor-based detectors and scale ranges in anchor-free detectors. After these hyperparameters are set, all ground-truth boxes must select their positive samples based on the fixed rules, which are suitable for most objects, but some outer objects will be neglected. Thus, different settings of these hyperparameters will have very different results.To this end, we propose the ATSS method that automatically divides positive and negative samples according to statistical characteristics of object almost without any hyperparameter. Algorithm 1 describes how the proposed method works for an input image. For each ground-truth box g on the image, we first find out its candidate positive samples. As described in Line 3 to 6, on each pyramid level, we select k anchor boxes whose center are closest to the center of g based on L2 distance. Supposing there are L feature pyramid levels, the ground-truth box g will have k \u00d7 L candidate positive samples. After that, we compute the IoU between these candidates and the ground-truth g as D g in Line 7, whose mean and standard deviation are computed as m g and v g in Line 8 and Line 9. With these statistics, the IoU threshold for this ground-truth g is obtained as t g = m g +v g in Line 10. Finally, we select these candidates whose IoU are greater than or equal to the threshold t g as final positive samples in Line 11 to 15. Notably, we also limit the positive samples' center to the ground-truth box as shown in Line 12. Besides, if an anchor box is assigned to multiple ground-truth boxes, the one with the highest IoU will be selected. The rest are negative samples. Some motivations behind our method are explained as follows. Selecting candidates based on the center distance between anchor box and object. For RetinaNet, the IoU is larger when the center of anchor box is closer to the center of object. For FCOS, the closer anchor point to the center of object will produce higher-quality detections. Thus, the closer anchor to the center of object is the better candidate.Input:G is a set of ground-truth boxes on the image L is the number of feature pyramid levels Ai is a set of anchor boxes from the i th pyramid levels A is a set of all anchor boxes k is a quite robust hyperparameter with a default value of 9 Output:P is a set of positive samples N is a set of negative samples 1: for each ground-truth g \u2208 G dobuild an empty set for candidate positive samples of the ground-truth g: Cg \u2190 \u2205;3:Si \u2190 select k anchors from Ai whose center are closest to the center of ground-truth g based on L2 distance;5:end for compute IoU threshold for ground-truth g: tg = mg + vg;11:for each candidate c \u2208 Cg doif IoU (c, g) \u2265 tg and center of c in g then Using the sum of mean and standard deviation as the IoU threshold. The IoU mean m g of an object is a measure of the suitability of the preset anchors for this object. A high m g as shown in (a) indicates it has high-quality candidates and the IoU threshold is supposed to be high. A low m g as shown in (b) indicates that most of its candidates are low-quality and the IoU threshold should be low. Besides, the IoU standard deviation v g of an object is a measure of which layers are suitable to detect this object. A high v g as shown in (a) means there is a pyramid level specifically suitable for this object, adding v g to m g obtains a high threshold to select positives only from that level. A low v g as shown in (b) means that there are several pyramid levels suitable for this object, adding v g to m g obtains a low threshold to select appropriate positives from these levels. Using the sum of mean m g and standard deviation v g as the IoU threshold t g can adaptively select enough positives for each object from appropriate pyramid levels in accordance of statistical characteristics of object. Limiting the positive samples' center to object. The anchor with a center outside object is a poor candidate and will be predicted by the features outside the object, which is not conducive to training and should be excluded. Maintaining fairness between different objects. According to the statistical theory  , about 16% of samples are in the confidence interval [m g + v g , 1] in theory. Although the IoU of candidates is not a standard normal distribution, the statistical results show that each object has about 0.2 * kL positive samples, which is invariant to its scale, aspect ratio and location. In contrast, strategies of RetinaNet and FCOS tend to have much more positive samples for larger objects, leading to unfairness between different objects. Keeping almost hyperparameter-free. Our method only has one hyperparameter k. Subsequent experiments prove that it is quite insensitive to the variations of k and the proposed ATSS can be considered almost hyperparameter-free.Anchor-based RetinaNet. To verify the effectiveness of our adaptive training sample selection for anchor-based detectors, we use it to replace the traditional strategy in the improved RetinaNet (#A=1). As shown in , it consistently boosts the performance by 2.3% on AP, 2.4% on AP 50 , 2.9% for AP 75 , 2.9% for AP S , 2.1% for AP M and 2.7% for AP L . These improvements are mainly due to the adaptive selection of positive samples for each ground-truth based on its statistical characteristics. Since our method only redefines positive and negative samples without incurring any additional overhead, these improvements can be considered cost-free. Anchor-free FCOS. The proposed method can also be applied to the anchor-free FCOS in two different versions: the lite and full version. For the lite version, we apply some ideas of the proposed ATSS to FCOS, i.e., replacing its way to select candidate positives with the way in our method. FCOS considers anchor points in the object box as candidates, which results in plenty of low-quality positives. In contrast, our method selects top k = 9 candidates per pyramid level for each ground-truth. The lite version of our method has been merged to the official code of FCOS as the center sampling, which improves FCOS from 37.8% to 38.6% on AP as listed in . However, the hyperparameters of scale ranges still exist in the lite version. For the full version, we let the anchor point in FCOS become the anchor box with 8S scale to define positive and negative samples, then still regress these positive samples to objects from the anchor point like FCOS. As shown in , it significantly increases the performance by 1.4% for AP, by 1.7% for AP 50 , by 1.7% for AP 75 , by 0.6% for AP S , by 1.3% for AP M and by 2.7% for AP L . Notably, these two versions have the same candidates selected in the spatial dimension, but different ways to select final positives from candidates along the scale dimension. As listed in the last two rows of , the full version (ATSS) outperforms the lite version (center sampling) across different metrics by a large margin. These results indicate that the adaptive way in our method is better than the fixed way in FCOS to select positives from candidates along the scale dimension.Training an object detector with the proposed adaptive training sample selection only involves one hyperparameter k and one related setting of anchor boxes. This subsection analyzes them one after another. Hyperparameter k. We conduct several experiments to study the robustness of the hyperparameter k, which is used to select the candidate positive samples from each pyramid level. As shown in , different values of k in  are used to train the detector. We observe that the proposed method is quite insensitive to the variations of k from 7 to 17. Too large k (e.g.,  will result in too many low-quality candidates that slightly decreases the performance. Too small k (e.g., 3) causes a noticeable drop in accuracy, because too few candidate positive samples will cause statistical instability. Overall, the only hyperparameter k is quite robust and the proposed ATSS can be nearly regarded as hyperparameter-free.   Anchor Size. The introduced method resorts to the anchor boxes to define positives and we also study the effect of the anchor size. In the previous experiments, one square anchor with 8S (S indicates the total stride size of the pyramid level) is tiled per location. As shown in , we conduct some experiments with different scales of the square anchor in  and the performances are quite stable. Besides, several experiments with different aspect ratios of the 8S anchor box are performed as shown in . The performances are also insensitive to this variation. These results indicate that the proposed method is robust to different anchor settings.We compare our final models on the MS COCO test-dev subset in  with other state-of-the-art object detectors. Following previous works , the multiscale training strategy is adopted for these experiments, i.e., randomly selecting a scale between 640 to 800 to resize the shorter side of images during training. Besides, we double the total number of iterations to 180K and the learning rate reduction points to 120K and 160K correspondingly. Other settings are consistent with those mentioned before.As shown in , our method with ResNet-101 achieves 43.6% AP without any bells and whistles, which is better than all the methods with the same backbone including Cascade R-CNN  (42.8% AP), C-Mask RCNN  (42.0% AP), RetinaNet  (39.1% AP) and RefineDet  (36.4% AP). We can further improve the AP accuracy of the proposed method to 45.1% and 45.6% by using larger backbone networks ResNeXt-32x8d-101 and ResNeXt-64x4d-101 , respectively. The 45.6% AP result surpasses all the anchor-free and anchor-based detectors except only 0.1% lower than SNIP  (45.7% AP), which introduces the improved multi-scale training Previous experiments are based on RetinaNet with only one anchor per location. There is still a difference between anchor-based and anchor-free detectors that is not explored: the number of anchors tiled per location. Actually, the original RetinaNet tiles 9 anchors (3 scales \u00d7 3 aspect ratios) per location (marked as RetinaNet (#A=9)) that achieves 36.3% AP as listed in the first row of . In addition, those universal improvements in  can also be used to RetinaNet (#A=9), boosting the AP performance from 36.3% to 38.4%. Without using the proposed ATSS, the improved RetinaNet (#A=9) has better performance than Reti-naNet (#A=1), i.e., 38.4% in  vs. 37.0% in . These results indicate that under the traditional IoU-based sample selection strategy, tiling more anchor boxer per location is effective.However, after using our proposed method, the opposite conclusion will be drawn. To be specific, the proposed ATSS also improves RetinaNet (#A=9) by 0.8% on AP, 1.4% on AP 50 and 1.1% on AP 75 , achieving similar performances to RetinaNet (#A=1) as listed in the third and sixth rows of . Besides, when we change the number of anchor scales or aspect ratios from 3 to 1, the results are almost unchanged as listed in the fourth and fifth rows of . In other words, as long as the positive samples are selected appropriately, no matter how many anchors are tiled at each location, the results are the same. We argue that tiling multiple anchors per location is a useless operation under our proposed method and it needs further study to discover its right role. In this work, we point out that the essential difference between one-stage anchor-based and center-based anchor-free detectors is actually the definition of positive and negative training samples. It indicates that how to select positive and negative samples during object detection training is critical. Inspired by that, we delve into this basic issue and propose the adaptive training sample selection, which automatically divides positive and negative training samples according to statistical characteristics of object, hence bridging the gap between anchor-based and anchor-free detectors. We also discuss the necessity of tiling multiple anchors per location and show that it may not be a so useful operation under current situations. Extensive experiments on the challenging benchmarks MS COCO illustrate that the proposed method can achieve state-of-the-art performances without introducing any additional overhead.", "videoStruct": [{"timeStart": "00-00-00", "timeEnd": "00-00-05", "sentence": "hello everyone my name is sue phone down from CSI glad to share our work"}, {"timeStart": "00-00-05", "timeEnd": "00-00-11", "sentence": "reaching the gap between anchor bay sand anchor free detection via adaptive training sample selection to you"}, {"timeStart": "00-00-12", "timeEnd": "00-00-15", "sentence": "and the first author and the other authors are changed"}, {"timeStart": "00-00-15", "timeEnd": "00-00-18", "sentence": "y'all hang yard only as then leave"}, {"timeStart": "00-00-19", "timeEnd": "00-00-25", "sentence": "Corinthian vase objective factors consist of angry base and ANC are three methods and good days"}, {"timeStart": "00-00-25", "timeEnd": "00-00-30", "sentence": "can be divided into one stage and two stage methods why and her free"}, {"timeStart": "00-00-30", "timeEnd": "00-00-35", "sentence": "can be also divided into center base and keep on base methods"}, {"timeStart": "00-00-35", "timeEnd": "00-00-38", "sentence": "among them the one stage and her grace"}, {"timeStart": "00-00-38", "timeEnd": "00-00-40", "sentence": "approach is very similar to their"}, {"timeStart": "00-00-40", "timeEnd": "00-00-42", "sentence": "and could free central base approach"}, {"timeStart": "00-00-42", "timeEnd": "00-00-47", "sentence": "hey and come base routine and anchor free as CEO as example"}, {"timeStart": "00-00-47", "timeEnd": "00-00-49", "sentence": "they have very similar framework"}, {"timeStart": "00-00-49", "timeEnd": "00-00-53", "sentence": "our motivation is to explore the essential difference"}, {"timeStart": "00-00-53", "timeEnd": "00-00-58", "sentence": "there are three differences between the Tina net and CEO"}, {"timeStart": "00-01-11", "timeEnd": "00-01-14", "sentence": "the second one is the regression starting status"}, {"timeStart": "00-01-14", "timeEnd": "00-01-22", "sentence": "routine a natural regressive the object bounding boxes from the present and convulse while I feel as from the anchor point"}, {"timeStart": "00-01-22", "timeEnd": "00-01-26", "sentence": "the third one is the number of anchors tire location"}, {"timeStart": "00-01-26", "timeEnd": "00-01-32", "sentence": "Tina Mack Stiles night and her boss is white x your as tiles one and her Pointe location"}, {"timeStart": "00-01-33", "timeEnd": "00-01-37", "sentence": "we want to explore which of them are essential differences"}, {"timeStart": "00-01-50", "timeEnd": "00-01-54", "sentence": "we can see that routine Annette with one anchor only achieved"}, {"timeStart": "00-01-54", "timeEnd": "00-02-00", "sentence": "thirty two point five AP on KO KO five one three points worse than that pos"}, {"timeStart": "00-02-00", "timeEnd": "00-02-05", "sentence": "however part of this AP gap results from some universal improvement"}, {"timeStart": "00-02-05", "timeEnd": "00-02-10", "sentence": "they also can improve routine net from thirty two point five to thirty seven"}, {"timeStart": "00-02-11", "timeEnd": "00-02-12", "sentence": "but retain a nap"}, {"timeStart": "00-02-12", "timeEnd": "00-02-16", "sentence": "still zero points eight points worse than I feel"}, {"timeStart": "00-02-16", "timeEnd": "00-02-19", "sentence": "buy now after removing inconsistency"}, {"timeStart": "00-02-19", "timeEnd": "00-02-23", "sentence": "we can explore their essential differences in a fair way"}, {"timeStart": "00-02-23", "timeEnd": "00-02-33", "sentence": "when routine and and absolute useless thing definition of training samples and different regression starting status they have similar results as the first"}, {"timeStart": "00-02-33", "timeEnd": "00-02-34", "sentence": "and second rows"}, {"timeStart": "00-02-35", "timeEnd": "00-02-44", "sentence": "where routine and nets and I feel as used the same regression starting daddies and different definitions of training samples they have a differing results at the"}, {"timeStart": "00-02-44", "timeEnd": "00-02-46", "sentence": "the first and second coldest"}, {"timeStart": "00-02-46", "timeEnd": "00-02-54", "sentence": "passed the definition of positive and negative samples is an essential difference why the regression starts dollars"}, {"timeStart": "00-02-55", "timeEnd": "00-02-58", "sentence": "inspired by this with Tao into the most basic"}, {"timeStart": "00-02-58", "timeEnd": "00-03-01", "sentence": "object detection"}, {"timeStart": "00-03-01", "timeEnd": "00-03-06", "sentence": "how did they find positive and negative training samples and composed a t f s"}, {"timeStart": "00-03-18", "timeEnd": "00-03-21", "sentence": "this object will have forty five candidates"}, {"timeStart": "00-03-21", "timeEnd": "00-03-26", "sentence": "then we compute the I O you between these candidates and this object"}, {"timeStart": "00-03-26", "timeEnd": "00-03-29", "sentence": "as well as its me and"}, {"timeStart": "00-03-29", "timeEnd": "00-03-31", "sentence": "with this statistic"}, {"timeStart": "00-03-31", "timeEnd": "00-03-34", "sentence": "oh you rass hold for this object"}, {"timeStart": "00-03-34", "timeEnd": "00-03-36", "sentence": "to sum up me and s t d"}, {"timeStart": "00-03-36", "timeEnd": "00-03-44", "sentence": "and we feel like this candidates whose oh you are greater than the threshold as final positive samples"}, {"timeStart": "00-03-58", "timeEnd": "00-04-02", "sentence": "there are only one hyper parameter in our methods"}, {"timeStart": "00-04-02", "timeEnd": "00-04-04", "sentence": "which is quite robust in a luxury"}, {"timeStart": "00-04-05", "timeEnd": "00-04-12", "sentence": "besides our matter uses one and her both to define positive it is robust to different anchor standings"}, {"timeStart": "00-04-13", "timeEnd": "00-04-16", "sentence": "finally with disgust the last difference"}, {"timeStart": "00-04-16", "timeEnd": "00-04-25", "sentence": "following my anchor boss's location in routine QS improves the performance from thirty seventeen thirty eight which means"}, {"timeStart": "00-04-25", "timeEnd": "00-04-27", "sentence": "that Moore anchor both of publication is it"}, {"timeStart": "00-04-27", "timeEnd": "00-04-31", "sentence": "under the I O you bass sample selection strategy"}, {"timeStart": "00-04-32", "timeEnd": "00-04-40", "sentence": "however it is a useless operation under our a t s s because different numbers of anchors location has the same result"}, {"timeStart": "00-04-40", "timeEnd": "00-04-50", "sentence": "therefore we conclude that after using appropriate sample selection strategy telling multiple anchors vacation may not be a useful operation"}, {"timeStart": "00-04-50", "timeEnd": "00-04-54", "sentence": "further study is needed to discover it's right"}, {"timeStart": "00-04-54", "timeEnd": "00-04-59", "sentence": "you can find the release coat on my homepage feel free to use our method"}]}, {"title": "[CVPR 2020 Award Nominee] High-Performance Long-Term Tracking With Meta-Updater", "authors": "ComputerVisionFoundation Videos", "abstract": "", "publicationOrg": "CVPR", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2004.00305.pdf", "pdfPath": "/data/cache/1/PDFs/CVPR2020AwardNomineeHighPerformanceLongTermTrackingWithMetaUpdater.pdf", "publicationUrl": "https://arxiv.org/pdf/2004.00305.pdf", "codeUrl": "https://github.com/Daikenan/LTMU.", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/0wgL79ZvXi4", "videoPath": "/data/cache/1/videos/High-Performance Long-Term Tracking With Meta-Updater.mp4", "pdfText": "The study of visual tracking has begun to shift from short-term tracking to large-scale long-term tracking, roughly due to two reasons. First, long-term tracking is much closer to practical applications than short-term tracking. The average length of sequences in short-term tracking benchmarks (OTB , VOT2018 , TC128 , to name a few) is often at the second level, whereas the av- . \"ATOM*\" is our local tracker based on ATOM , \"Ours\" denotes our long-term tracker with meta-update. \"ATOM* LT\" means \"Ours\" without metaupdater. \"CLGS\" and \"SiamDW LT\" are the second and third best trackers on VOT2019LT. Please see Sections 3 and 4 for more details.erage frame length in long-term tracking datasets (such as VOT2018LT , VOT2019LT , and OxUvALT ) is at least at the minute level. Second, the long-term tracking task additionally requires the tracker having the capability to handle frequent disappearance and reappearance (i.e., having a strong re-detection capability) 1 . Deep-learning-based methods have dominated the shortterm tracking field , from the perspective of either one-shot learning  or online learning . Usually, the latter methods (e.g., ECO , ATOM ) are more accurate (with less training data) but slower than the former ones (e.g., SiamFC , SiamRPN ). A curious phenomenon is that few leading long-term trackers exploit online-updated short-term trackers to conduct local tracking. MBMD , the winner of VOT2018LT, exploits an offline-trained regression network to directly regress the target's bounding box in a local region, and uses an onlinelearned verifier to make the tracker switch between local tracking and global re-detection. The recent SPLT  method utilizes the same SiamRPN model in  for local tracking. SiamFC+R , the best method in the OxU-vALT report, equips the original SiamFC  with a simple re-detection scheme. An important reason is that online update is a double-edged sword for tracking. Online update captures appearance variations from both target and background, but inevitably pollutes the model with noisy samples. The risk of online update is amplified for long-term tracking, due to long-term uncertain observations.Motivated by the aforementioned analysis, this work attempts to improve the long-term tracking performance from two aspects. First, we design a long-term tracking framework that exploits an online-updated tracker for local tracking. As seen in , the tracking performance is remarkably improved by extending ATOM* to a long-term tracker (ATOM* LT), but it remains worse than the CLGS and SiamDW LT methods. Second, we propose a novel meta-updater to effectively guide the tracker's update.  1 shows that after adding our meta-updater, the proposed tracker achieves very promising tracking results.Our main contributions can be summarized as follows.\u2022 A novel offline-trained meta-updater is proposed to address an important but unsolved problem: Is the tracker ready for updating in the current frame? The proposed meta-updater effectively guide the update of the online tracker, not only facilitating the proposed tracker but also having good generalization ability. \u2022 A long-term tracking framework is introduced on the basis of a SiamRPN-based re-detector, an online verifier, and an online local tracker with our meta-updater. Compared with other methods, our long-term tracking framework can benefit from the strength of onlineupdated short-term tracker at low risk. \u2022 Numerous experimental results on the VOT2018LT, VOT2019LT, OxUvALT, TLP and LaSOT long-term benchmarks show that the proposed method outperforms the state-of-the-art trackers by a large margin.Although large-scale long-term tracking benchmarks  began to emerge since 2018, researchers have attached importance to the long-term tracking task for a long time (such as keypoint-based , proposalbased , detector-based , and other methods). A classical algorithm is the tracking-learning-detection (TLD) method , which addresses long-term tracking as a combination of a local tracker (with forward-backward optical flow) and a global re-detector (with an ensemble of weak classifiers). Following this idea, many researchers  attempt to handle the long-term tracking problem with different local trackers and different global re-detectors. Among them, the local tracker and global re-detectors can also adopt the same powerful model , being equipped with a re-detection scheme (e.g., random search and sliding window). A crucial problem of these trackers is how to switch the tracker between the local tracker and the global re-detector. Usually, they use the outputs of local trackers to conduct self-evaluation, i.e., to determine whether the tracker losses the target or not. This manner has a high risk since the outputs of local trackers are not always reliable and unexpectedly mislead the switcher sometimes. The MBMD method , the winner of VOT2018LT, conducts local and global switching with an additional online-updated deep classifier. This tracker exploits a SiamPRN-based network to regress the target in a local search region or every sliding window when re-detection. The recent SPLT method  utilizes the same SiamPRN in  for tracking and re-detection, replaces the online verifier in  with an offline trained matching network, and speeds up the tracker by using their proposed skimming module. A curious phenomenon is that most top-ranked long-term trackers (such as MBMD , SPLT , and SiamRPN++ ), have not adopted excellent online-updated trackers (e.g., ECO , ATOM ) to conduct local tracking. One of the underlying reasons is that the risk of online update is amplified for long-term tracking, caused by long-term uncertain observations. In this work, we attempt to address this dilemma by designing a high-performance long-term tracker with a meta-updater.For visual tracking, online update acts as a vital role to capture appearance variations from both target and its surrounding background during the tracking process. Numerous schemes have been designed to achieve this goal by using template update , incremental subspace learning , online learning classifiers , to name a few. However, online update is a double-edged sword in balancing the dynamical information description and unexpected noise introduction. Accumulating errors over a long time, collecting inappropriate samples or overfitting to available data when the target disappears can easily degrade the tracker and lead to tracking drift, especially for long-term tracking. To deal with this dilemma, many efforts have been done at least from two aspects. The first one aims to distill the online collected samples by recovering or clustering noisy observations . Another effective attempt is to design some criteria for evaluating the reliability of the current tracking result, to remove the unreliable samples or reject the inappropriate update. These criteria include the confidence score , the maximum (MAX) response , peak-to-sidelobe rate (PSR) , average peak-to-correlation energy , and MAX-PSR . These methods usually utilize the tracker's output to selfevaluate this reliability. But the self-evaluation of the trackers' reliability with its outputs has inevitable risks, especially when the tracker experiences the long-term uncertain and noisy observations. In this work, we propose a novel offline-trained meta-updater to integrate multiple cues in a sequential manner. The meta-updater outputs a binary score to indicate whether the tracker should be updated or not in the current frame, which not only remarkably improves the performance of our long-term tracker but also is easy to be embedded into other online-updated trackers. Recently, some meta-learning-based methods  have been presented. All these methods focus on addressing the \"how to update\" problem (i.e., efficiently and/or effectively updating the trackers' appearance models). By contrast, our meta-updater is designed to deal with the \"when to update\" problem, and it can be combined with many \"how to update\" algorithms to further improve the tracking performance.  The overall framework is presented in . In each frame, the local tracker takes the local search region as input, and outputs the bounding box of the tracked object. Then, the verifier evaluates the correctness of the current tracking result. If the output verification score is larger than a predefined threshold, the tracker will continue to conduct local tracking in the next frame. If the score is smaller than the threshold, we use the faster R-CNN detector  to detect all possible candidates in the next frame and crop the local search region regarding each candidate. Then, a SiamPRN model  takes each region as input and outputs corresponding candidate boxes. These bounding boxes are sent to the verifier for identifying whether there exists the target or not. When the verifier finds the target, the local tracker will be reset to adapt to the current target appearance. Before entering into the next frame, all historic information is collected and sent into the proposed meta-updater. Finally, the meta-updater guides the online trackers' update.In this work, we implement an improved ATOM tracker (denoted as ATOM * ) as our local tracker, which applies the classification branch of the ATOM method  for localization and exploits the SiamMask method  for scale estimation  . We use the RTMDNet method  as our verifier, and its verification threshold is set to 0. Strength and Imperfection. Compared with recent topranked long-term trackers (such as MBMD  and SPLT ), the major strength of our framework lies in embedding an online-updated local tracker into the long-term tracking framework. This idea makes the long-term tracking solution benefit from the progress of short-term trackers, and unifies the short-term and long-term tracking problems as much as possible. One imperfection is that the risk of online update is amplified due to the long-term uncertain observations (since the results of any frame except for the first one have no absolute accuracy during tracking). Thus, we propose a novel Meta-Updater to handle this problem and obtain more robust tracking performance.It is essential to update the tracker for capturing appearance variations from both target and its surrounding background. However, the inappropriate update will inevitably make the tracker degrade and cause tracking drift. To address this dilemma, we attempt to answer an important but unsolved question: Is the tracker ready for updating in the current frame? To be specific, we propose a Meta-Updater to determine whether the tracker should be updated or not in the present moment, by integrating historical tracking results. These historical results include geometric, discriminative, and appearance cues in a sequential manner. We introduce our meta-updater on the basis of an online tracker outputting a response map in each frame (e.g., ECO , ATOM ). It is easy to generalize our metaupdater for other types of trackers (such as MDNet ).Given an online tracker T , in the t-th frame, we denote the output response map as R t , the output bounding box as b t , and the result image (cropped according to b t ) as I t , respectively. The target template in the first frame is denoted as I 0 . An intuitive explanation is illustrated in .   We develop our meta-updater by mining the sequential information, integrating geometric, discriminative, and appearance cues within a given time slice. Geometric Cue. In the t-th frame, the tracker outputs a bounding box b t = [x t , y t , w t , h t ] as the tracking state, where (x, y) denote the horizontal and vertical coordinates of the up-left corner and (w, h) are the width and height of the target. This bounding box itself merely reflects the geometric shape of the tracked object in the current frame. However, a series of bounding boxes from consecutive frames contain the important motion information regarding the target, such as velocity, acceleration, and scale change. Discriminative Cue. Visual tracking can be considered as a classification task to distinguish the target from its surrounding background, thus, an online tracker should have good discriminative ability itself. We define a confidence score s C t as the maximum value of the response map R t (1). For some trackers that do not output any response map (e.g., MDNet ), it is also not difficult to obtain this confidence score based on the classification probability or margin.(1)  indicates that the confidence score is not stable during the tracking process (see 89-and 261-th frames). In this work, we also exploit a convolutional neural network (CNN) to thoroughly mine the information within the response map, and obtain a response vectorwhere f R (.; .) denotes the CNN model with the parameter W R . The output vector v R t implicitly encodes the reliability information of the tracker in the current frame, and is further processed by the subsequent model. Appearance Cue. The self-evaluation of the trackers' reliability with its outputs has inevitable risks, since online updating with noisy samples often makes the response not sensitive to appearance variations. Thus, we resort to a template matching method as a vital supplement, and define an appearance score aswhere f A ., W A is the embedding function to embed the target and candidates into a discriminative Euclidean space, W A stands for its offline trained network parameters. As presented in , the network f A ., W A can be effectively trained with the combination of triplet and classification loss functions. The score s A t measures the distance between the tracked result I t and target template I 0 . This template matching scheme is not affected by noisy observations. Sequential Information. We integrate the aforementioned geometric, discriminative and appearance cues into a sequential matrix as Xt , s A t , and b t . d is the dimension of concentrated cues, and t s is a time step to balance the historical experience and current observation. This sequential information is further mined with the following cascaded LSTM scheme.Here, we briefly introduce the basic ideas and notions of LSTM  to make this paper self-contained. Its mathematical descriptions are presented as follows.where \u03c3 (.) denotes the element-wise sigmoid function, tanh (.) stands for the element-wise tangent operation, and is the element-wise multiplication. W, U, and b denote the weight matrices and bias vector requiring to be learned. The subscripts f , i, o, and c stand for the forget gate, input gate, output gate, and memory cell, respectively. Other variables are defined as follows.  t sequential information and focus on the recent frames. The input-output relations are presented in . The superscript i denotes the i-th stage LSTM. Finally, the output h 3 t is processed by two fully connected layers to generate a binary classification score, indicating whether the tracker should be updated or not.Sample Collection. We run the local tracker on different training video sequences  , and record the tracking results in all frames. Then, we divide these results into a series of time slices, denoted as Y =. v is the video index, V is the number of training sequences, and t v is the total frame length of the v-th video.where t s denotes the time step. Each time slice y v t includes the bounding box, response map, response score, and predicted target image in the t-th frame, along with the corresponding target template. See Section 3.2.1 for more detailed descriptions  .Then, we determine the label ofwhere IoU stands for the Intersection-over-Union criterion. The slices whose IoUs are between 0 and 0.5 have been not adopted in the training phases to guarantee the training convergence. b v t is the output bounding box in the t-th frame in video v, and g v t is the corresponding groundtruth  . Equation (4) means that the label of a given time slice is determined based on whether the target is successfully located or not in the current (i.e., t-th) frame.  visualizes some positive and negative samples for training our meta-updater.for k = 0; k < K; k + + do Run T , MU k (T ) , and record the tracking resultsCollect training samples Y k with their labels L k Train the meta-updater MU k+1 (T ) end for Model Training. In this study, the local tracker and its meta-updater are tightly-coupled. The tracker affects the sample collection process for training its meta-updater. The meta-updater will change the tracker's performance, and further affect sample collection indirectly. Thus, we propose an iterative training algorithm, listed in Algorithm 1. The symbol {T , MU (T )} is used to denote a local tracker equipped with its meta-updater MU (T ). MU k (T ) is the learned meta-updater after the k-th iteration (k = 0 means no meta-updater). K is set to 3 in this work.The aforementioned introduction is with respect to the online-updated tracker outputting a response map. For the trackers without the response map (e.g., MDNet , RT-MDNet ), we can simply remove the subnetwork f R , and train the meta-updater with the remaining information. For some trackers those are online updated with accumulated samples over time (such as ECO ), our metaupdater is able to purify the sample pool used for updating. For a given frame, if the output of the meta-updater is 0, then the current tracking results will not be added into the sample pool (i.e., not used for updating). If an ensemble of multiple online-updated trackers (such as our long-term trackers, ATOM* for local tracking and RTMDNet for verification), we can train only one meta-updater with the information from all trackers as the input, and then use it to guide all trackers' update. Section 4.3 shows our meta-updater's generalization ability for different trackers.All networks below are trained using the stochastic gradient decent optimizer, with the momentum of 0.9. The training samples are all from the LaSOT  training set. Matching Network f A .The matching network f A adopts the ResNet-50 architecture and takes 107 \u00d7 107 image patches as inputs. For each target, we randomly sample bounding boxes around the groundtruth in each frame. We choose the patches with IoU above 0.7 as the positive data, and use the boxes with high confidence scores from the SiamRPN-based network  but not belonging to the target as the negative data. The batch size of the network f A is 16 and we train it for 60000 iterations. The initial learning rate is 10 \u22124 and divided by 10 every 200000 iterations. The matching network is individually trained and fixed when training the remaining networks of our metaupdater. Subnetwork f R . The input response map is first resized to 50 \u00d7 50, processed by two convolutional layers, and then followed by a global average pooling layer. The output is a 1 \u00d7 1 \u00d7 8 vector. This subnetwork is jointly trained with the cascade LSTMs and the two fully connected layers.  . Illustration of positive and negative samples for meta-updater training. The first two rows illustrate two positive examples, whereas the last two rows display the negative ones. In fact, there is no interval among frames, the interval 5 is merely for clear visualization.and is trained by 100, 000 iterations with the learning rate of 10 \u22124 .We implement our tracker using Tensorflow on a PC machine with an Intel-i9 CPU (64G RAM) and a NVIDIA GTX2080Ti GPU (11G memory). The tracking speed is approximatively 13 fps. We evaluate our tracker on five benchmarks: VOT2018LT , VOT2019LT , OxU-vALT , TLP , and LaSOT . VOT2018LT. We first compare our tracker with other state-of-the-art algorithms on the VOT2018LT dataset , which contains 35 challenging sequences of diverse objects (e.g., persons, cars, motorcycles, bicycles and animals) with the total length of 146817 frames. Each sequence contains on average 12 long-term target disappearances, each lasting on average 40 frames. The accuracy evaluation of the VOT2018LT dataset  mainly includes tracking precision (Pr), tracking recall (Re) and tracking F-score. Different trackers are ranked according to the tracking F-score.The detailed definitions of Pr, Re and F-score can be found in the VOT2018 challenge official report .We compare our tracker with the VOT2018 official trackers and three recent methods (i.e., MBMD , SiamRPN++ , and SPLT ) and report the evaluation results in . The results show that the proposed tracker outperforms all other trackers by a very large margin.VOT2019LT. The VOT2019LT  dataset, containing 50 videos with 215294 frames in total, is the most recent longterm tracking dataset. Each sequence contains on average 10 long-term target disappearances, each lasting on average 52 frames. Compared with VOT2018LT , VOT2019LT poses more challenges since it introduces 15 more difficult videos and some uncommon targets (e.g., boat, bull, and parachute). Its evaluation protocol is the same as that in VOT2018LT.  shows that our trackers achieves the first place on the VOT2019LT challenge.OxUvALT. The OxUvA long-term (denoted as OxUvALT) dataset  contains 366 object tracks in 337 videos, which are selected from YTBB. Each video in this dataset lasts for average 2.4 minutes, which is much longer than other commonly used short-term datasets (such as OTB2015 ). The targets are sparsely labeled at a frequency of 1 Hz. The dataset was divided into two disjoint subsets, dev and test. In this work, we follow the open challenge in OxUvALT, which means that trackers can use any dataset except for the YTBB validation set for training and use the OxUvALT . Performance evaluation of our tracker and eight competing algorithms on the VOT2019LT dataset. The best three results are shown in red , blue and green colors, respectively. The trackers are ranked from top to bottom using the F-score measure. , which is used to rank different trackers. We compare our tracker with three recent algorithms (MBMD , SPLT  and GlobalTrack ) and ten algorithms reported in  (such as LCT , EBT , TLD , ECO-HC , BACF , Staple [1], MDNet , SINT , SiamFC , and SiamFC+R ).  shows that our tracker performs best in terms of MaxGM and TPR while maintaining a very competitive TNR value. LaSOT. The LaSOT dataset  is one of the most recent large-scale datasets with high-quality annotations. It contains 1400 challenging sequences (1120 for training and 280 for testing) with 70 tracking categories, with an average of 2500 frames per sequence. In this work, we follow the one-pass evaluation (success and precision) to evaluate different trackers on the test set of LaSOT.  illustrates both success and precision plots of our tracker and ten state-of-the-art algorithms, including Dimp50 , Dimp18 , GlobalTrack , SPLT , ATOM , SiamRPN++ , ECO(python) , Struct-Siam , DSiam , and MDNet .  shows that our tracker achieves the best results among all competing methods. TLP. The TLP dataset  contains 50 HD videos from real-world scenarios, with an average of 13500 frames per sequence. We follow the one-pass evaluation (success and precision) to evaluate different trackers on the TLP dataset. As shown in , our tracker achieves the best results among all competing methods. In this subsection, we conduct ablation analysis of our meta-updater using the LaSOT dataset . Different time steps of meta-updater. First, we investigate the effects of different time steps. An appropriate time step could achieve a good trade-off between historical information and current observations.  shows that the best performance is obtained when the time step is set to 20. Different inputs for our meta-updater. For our long-term trackers, the inputs of the meta-updater include bounding box (B), confidence score (C), response map (R), and appearance score (A). We verify their contributions by sepa-rately removing them from our meta-update. Detailed results are reported in , showing that each input contributes to our meta-updater (w/o means 'without'). Evaluation of iterative steps.  shows that the performance is gradually improved with the increase of k. Generalization ability and speed analysis. We note that our meta-updater is easy to be embedded into other trackers with online learning. To show this good generalization ability, we introduce our meta-updater into four tracking algorithms, including ATOM, ECO (the official python implementation), RTMDNet and our base tracker (using a threshold to control update).  shows the tracking performance of different trackers without and with metaupdater on the LaSOT dataset, and it demonstrates that the proposed meta-updater can consistently improve the tracking accuracy of different trackers.  reports the running speeds of those trackers without and with the proposed meta-updater, which demonstrates that the tracking speeds decrease slightly with an additional meta-updater scheme. Thus, we can conclude that our meta-updater has a good generalization ability, which can consistently improve the tracking accuracy almost without sacrificing the efficiency.  Location error threshold   Why our meta-updater works? We run a tracker without and with its meta-updater, and record the trackers' update state (u = 0, 1) paired with its ground truth in each frame (l = 0, 1). u = 1 means that the tracker has been updated; otherwise, has not been updated. l = 1 means that the tracker can be updated; otherwise, cannot be updated. The definition of ground truth l is the same as equation . We have the following concepts: (1) true positive (TP): l = 1, u = 1; (2) false positive (FP): l = 0, u = 1;(3) true negative (TN): l = 0, u = 0; and (4) false negative (FN): l = 1, u = 0. Then, we can obtain the update precision (Pr), and update recall (Re) as Pr = TP/(TP+FP), and Re = TP/(TP+FN), respectively. A higher precision means that the tracker has been updated with less wrong observations. A higher recall means that the tracker more likely accepts to be updated with correct observations. We also define a true negative rate (TNR) to pay much attention to wrong observations as TNR = TN/(TN+FP). A higher TNR value means that the tracker rejects to be updated with wrong observations more strongly.  shows the statistic results of different trackers with and without their metaupdater modules. The usage of meta-updater slightly sacrifices the update recall, which means that a portion of correct observations have not been used to update the tracker in comparison with that without meta-updater. This phenomenon affects little on the trackers' performance because correct observations are all for the same target and have a large amount of redundant information. In contrast, the usage of meta-updater significantly improves the Pr and TNR values, indicating that the tracker is much less polluted by wrong observations. Thus, the risk of online update will be significantly decreased. This work presents a novel long-term tracking framework with the proposed meta-updater. Combined with other top-ranked trackers, our framework exploits an onlineupdate-based tracker to conduct local tracking, which makes the long-term tracking performance benefit from the excellent short-term trackers with online update (such as ATOM). More importantly, a novel meta-updater is proposed by integrating geometric, discriminative, and appearance cues in a sequential manner to determine whether the tracker should be updated or not at the present moment. This method substantially reduces the risk of online update for long-term tracking, and effectively yet efficiently guides the tracker's update. Numerous experimental results on five recent long-term benchmarks demonstrate that our long-term tracker achieves significantly better performance than other state-of-the-art methods. The results also indicate that our meta-updater has good generalization ability.", "videoStruct": [{"timeStart": "00-00-02", "timeEnd": "00-00-09", "sentence": "we introduce a novel matchup data in fact the module that controls the an update of trackers with little extra competition birthday"}, {"timeStart": "00-00-12", "timeEnd": "00-00-17", "sentence": "the task of long-term single object tracking has more challenges than the shortterm counterpart"}, {"timeStart": "00-00-17", "timeEnd": "00-00-23", "sentence": "the last of frames is much longer the target disappears frequently and Morty formations and Potter background or"}, {"timeStart": "00-00-23", "timeEnd": "00-00-27", "sentence": "so the an update of bottom tracking is much more difficult"}, {"timeStart": "00-00-28", "timeEnd": "00-00-31", "sentence": "to control the AHL update of the tracker"}, {"timeStart": "00-00-31", "timeEnd": "00-00-33", "sentence": "we propose a novel met update her"}, {"timeStart": "00-00-33", "timeEnd": "00-00-37", "sentence": "it can accurately judge whether the current frame is super for updating the parameters"}, {"timeStart": "00-00-37", "timeEnd": "00-00-43", "sentence": "experiments observed several kinds of information that is useful for the control of an update"}, {"timeStart": "00-00-43", "timeEnd": "00-00-46", "sentence": "the first one is a geometric q of the targets"}, {"timeStart": "00-00-52", "timeEnd": "00-00-56", "sentence": "as shown in this figure indicates the position and shape of the target"}, {"timeStart": "00-01-01", "timeEnd": "00-01-04", "sentence": "the second one is the discriminative que"}, {"timeStart": "00-01-07", "timeEnd": "00-01-11", "sentence": "and refers to the confidence map or response my predicted by the tracker"}, {"timeStart": "00-01-15", "timeEnd": "00-01-19", "sentence": "I also observed that the appearance cube can be helpful as well"}, {"timeStart": "00-01-22", "timeEnd": "00-01-26", "sentence": "they made patch off the tracking result in each frame is used as the appearance"}, {"timeStart": "00-01-29", "timeEnd": "00-01-32", "sentence": "however the information in a single frame is not enough to judge"}, {"timeStart": "00-01-32", "timeEnd": "00-01-35", "sentence": "whether the current tracking results should be used"}, {"timeStart": "00-01-35", "timeEnd": "00-01-36", "sentence": "for updating parameters"}, {"timeStart": "00-01-36", "timeEnd": "00-01-41", "sentence": "for example this figure shows the confidence my ex I'll put it by item on video sequence"}, {"timeStart": "00-01-41", "timeEnd": "00-01-47", "sentence": "we can say that it's hard to make decisions by the value of school or the shape of the response map from a single frame"}, {"timeStart": "00-01-47", "timeEnd": "00-01-50", "sentence": "but we can observe some crews from the history curves"}, {"timeStart": "00-01-53", "timeEnd": "00-01-56", "sentence": "so we should embed this ques with your history information"}, {"timeStart": "00-01-59", "timeEnd": "00-02-02", "sentence": "it's obvious that if we utilize their sequential information"}, {"timeStart": "00-02-02", "timeEnd": "00-02-09", "sentence": "now we can get more messages such as speed trajectory appearance changes response map variations and so on"}, {"timeStart": "00-02-11", "timeEnd": "00-02-14", "sentence": "this is the framework of our map update"}, {"timeStart": "00-02-14", "timeEnd": "00-02-17", "sentence": "the imposter Doris here mark Hughes mentioned above"}, {"timeStart": "00-02-17", "timeEnd": "00-02-18", "sentence": "selected in each frame"}, {"timeStart": "00-02-18", "timeEnd": "00-02-21", "sentence": "and the output is a single feature vector of the last time step"}, {"timeStart": "00-02-21", "timeEnd": "00-02-24", "sentence": "which is further processed by fully connected ears"}, {"timeStart": "00-02-24", "timeEnd": "00-02-25", "sentence": "the final output is a single scar"}, {"timeStart": "00-02-25", "timeEnd": "00-02-28", "sentence": "that indicates whether the current tracking results should be used"}, {"timeStart": "00-02-28", "timeEnd": "00-02-30", "sentence": "for honor update"}, {"timeStart": "00-02-31", "timeEnd": "00-02-36", "sentence": "visualize the training samples of our math update her the top two rows are positive examples"}, {"timeStart": "00-02-36", "timeEnd": "00-02-38", "sentence": "and the bottom two rows are nineteen once"}, {"timeStart": "00-02-41", "timeEnd": "00-02-50", "sentence": "the matter did her it's trending that iterative way the tracker first runs on a training set of a a s O t and collects the training samples it uses for an update"}, {"timeStart": "00-02-50", "timeEnd": "00-02-53", "sentence": "along with the confidence course the bounding boxes and the response maps"}, {"timeStart": "00-02-53", "timeEnd": "00-02-56", "sentence": "then we used a sitter to train our matter data"}, {"timeStart": "00-02-56", "timeEnd": "00-02-59", "sentence": "and the embedded might have did her back into the tracker"}, {"timeStart": "00-02-59", "timeEnd": "00-03-01", "sentence": "and your tiger rides on the training set again"}, {"timeStart": "00-03-01", "timeEnd": "00-03-03", "sentence": "to collect madam daters new training data"}, {"timeStart": "00-03-03", "timeEnd": "00-03-05", "sentence": "and then she knew the math update are correspondingly"}, {"timeStart": "00-03-05", "timeEnd": "00-03-11", "sentence": "we do this procedure iterative Lee until the matter of data can work with the tracker perfectly"}, {"timeStart": "00-03-12", "timeEnd": "00-03-15", "sentence": "this figure shows our overall framework"}, {"timeStart": "00-03-15", "timeEnd": "00-03-19", "sentence": "the math update her controls the update of local choker and verify are"}, {"timeStart": "00-03-19", "timeEnd": "00-03-22", "sentence": "the other sections are similar to current long term trackers"}, {"timeStart": "00-03-26", "timeEnd": "00-03-30", "sentence": "these two tables show our performance on two popular long-term tracking data sets"}, {"timeStart": "00-03-39", "timeEnd": "00-03-42", "sentence": "and these are the results are another three popular benchmarks"}, {"timeStart": "00-03-51", "timeEnd": "00-03-58", "sentence": "we further embed our matchup data into three popular short-term checkers and their long-term performance is improved by a large margin"}, {"timeStart": "00-03-58", "timeEnd": "00-04-00", "sentence": "with little space sacrifice"}, {"timeStart": "00-04-06", "timeEnd": "00-04-09", "sentence": "the performance of the SNP can also be improved dramatically"}, {"timeStart": "00-04-18", "timeEnd": "00-04-25", "sentence": "we're also having banded our map data into other popular checkers and all the codes have been released in the pink of these killer cold"}]}, {"title": "SIGIR 2020 Talk - Bayesian Inferential Risk Evaluation on Multiple IR Systems ", "authors": "Rodger Benham", "abstract": "", "publicationOrg": "SIGIR", "year": "2020", "pdfUrl": "https://rodgerbenham.github.io/bccm20-sigir.pdf", "pdfPath": "/data/cache/1/PDFs/SIGIR2020TalkBayesianInferentialRiskEvaluationonMultipleIRSystems.pdf", "publicationUrl": "https://rodgerbenham.github.io/bccm20-sigir.pdf", "codeUrl": "https://github.com/rmit-ir/bayesian-risk", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/VwCSufd-ops", "videoPath": "/data/cache/1/videos/Bayesian Inferential Risk Evaluation on Multiple IR Systems - SIGIR 2020 Talk.mp4", "pdfText": "Determining how to properly compare information retrieval systems has been a central problem in the field of IR since its inception more than fifty years ago. Cleverdon et al.  established the foundations in which repeatable evaluation could be conducted, and despite the advent of online evaluation models for use in commercial settings, batch evaluation remains an important tool. For example, online evaluation cannot be used to evaluate alternative rankers, as click-logs may be biased to the original ranker . A range of offline evaluation metrics have been proposed , including risk-sensitive overlays , used to avoid situations where a subset of topics gives rise to severe performance degradation, even though effectiveness is improved on average. One simple technique is to regard score degradations as having a greater weight than do score improvements of the same magnitude.At the same time, statistical tests are widely used to rule out topic sampling error as a factor when measuring improvements , usually with the goal of finding out whether a candidate ranker, or challenger, is demonstrably more effective than the current system -the champion. Sakai  analyzed several mistakes commonly made in IR papers, including under-and over-powered significance tests. Another issue is failing to \"correct\" when making multiple comparisons , which may lead to false discoveries when comparing many challengers, or many champions.While applying traditional significance testing when comparing average effectiveness has proven to be beneficial in IR, it may be important to perform inference on other aspects of performance such as risk-sensitive overlays as well. Din\u00e7er et al.  explore risk-sensitive inferential testing when a single champion is used as a baseline, but do not explore the multiple experimental inference case. Benham et al.  compare current approaches, concluding that risk-adjusted score differences can create asymmetric score distributions; that the confidence intervals from different Bootstrap approaches disagree when risk thresholds are high; and advocating the use of a biased-corrected approach. Din\u00e7er et al.  note that single risk baseline evaluation biases towards the champion, and propose GeoRisk  as a many-to-many risk measure. GeoRisk is not an inferential measure, however, and can be difficult to interpret. In addition, publicly available collections often have artifact systems available, but these are rarely leveraged fully when performing system comparisons.Bayesian inference has been shown to provide additional insights in certain scenarios as well . Here we explore the advantages of a Bayesian inferential approach, using past systems (and risk) to improve the reliability of multi-system comparisons. One benefit is that random effects like systems and topics shrink towards their respective mean, which makes their inferential analysis more conservative and avoids the need for multiple comparison correction . Additionally, since Bayesian inference allows selection of which family the data belongs to when specifying a prior distribution, we are able to extend the frequentist approach of Benham et al.  to a Bayesian one capable of accounting for skewed risk-adjusted score differences.We present an empirical evaluation using a pool of challenger systems and a single champion, and discuss the implications for the alternative scenario of evaluating a single challenger relative to a pool of champions. More specifically, we consider these three research questions:\u2022 RQ1: How does using previous system artifacts affect  inferential results for IR test collections?\u2022 RQ2: How does the Bayesian prior affect inferential results using risk adjusted scores, for one-to-one comparisons and one-to-many?\u2022 RQ3: How do Bayesian and frequentist credible and confidence intervals differ when performing risk-adjusted evaluation?Risk Measures. Risk-adjusted evaluation overlays quantify the extent to which topic-specific score changes, positive and negative, might affect a comparison between a champion and a challenger system. Of interest are approaches that parameterize the weighting applied to effectiveness losses and compute a summary value. For example, URisk  sums differences in score while applying a weighting of r to the losses  :where n is the number of score pairs (topics), and wins and losses are relative to the champion's effectiveness scores. Values greater than zero indicate that the challenger is more rewarding than the champion, even after scaling the losses by r . URisk has been used in evaluation  and as a loss function in learning-to-rank . To be interpreted usefully, URisk values must be compared with risk scores for other systems, leading Din\u00e7er et al.  to an inferential version of URisk, denoted TRisk. It uses values from the Student-t distribution to calculate if, after the r -weighting of losses is taken into account, the challenger is statistically better (or worse) than the champion. However, Benham et al.  found that the distribution of risk-adjusted scores is asymmetrical, eroding the t-test assumption of normality. Instead, Benham et al. recommend approaches that form confidence intervals on risk-adjusted scores, such as the bias-corrected accelerated Bootstrap (BCa).Din\u00e7er et al.  questioned the validity of single-champion single-challenger risk evaluation in \"academic\" settings, as risksensitivity is a characteristic of the relative system comparisons, and not an absolute property of the challenger. Since runs that are similar to the original ranker are inherently the safest choice, that bias can be avoided by forming a synthetic run composed of the average per-topic scores over a group of champions. That line of inquiry is extended by Din\u00e7er et al. , with risk-sensitivity information collected for multiple runs, and used to compute ZRisk, which combines the shape and variance of scores across the set, and to compute GeoRisk, which further combines the ZRisk score with the effectiveness score. Both approaches are descriptive like URisk, and hence must be interpreted after being computed.Benham et al.  note that all of these previous approaches lead to high numeric values when risk is \"low\". To avoid ambiguity, we adopt their suggestion and multiply each score by \u22121 so that high values correspond to high risk, with the change indicated by a \"-\" appended to the method's name, to get URisk \u2212 and so on.Bayesian MCMC. Kruschke  explains the two key ideas in a Bayesian data analysis: \"(1) Bayesian inference is reallocation of credibility across possibilities\"; and \"(2) The possibilities, over which we allocate credibility, are parameter values in meaningful mathematical models\". These are captured in Bayes' rule:where \u03b8 is the set parameters (or hypothesis); d is the data (or evidence); p(\u03b8 | d) is the posterior distribution; p(d | \u03b8 ) is the likelihood; p(\u03b8 ) is the prior; and p(d) is, as Lambert  explains: \"the probability distribution for a future data sample given our choice of model\". The posterior distribution is composed of combinations of parameters that form credible generations of the underlying data. Carterette  explored an approach that models system relevance directly from the judgments themselves instead of effectiveness metrics. That approach was extended by Sakai  to include an effect size computation using Glass' \u2206.The family of methods that support modern Bayesian inference are known as Markov Chain Monte-Carlo (MCMC) simulations. Lambert  observes that it is usually impossible to independently sample from the global posterior distribution, but that it can be done locally.  gives a high-level overview of the generalized case when conducting a Bayesian MCMC experiment. The inputs are a data-set d; prior beliefs \u03b8 about the data; and a set of instructions for the sampling algorithm. How \u03b8 is organized depends on the trends in d, whether there are hierarchical attributes to model; and how informative or weakly-informative (the data has a heavyemphasis on the outcome) it should be. Similarly, the number of chains, iterations, warmup iterations, and the seed used all play a role in determining the outcome of the Bayesian posterior produced. In the warm-up period an initial set of iterations are trimmed from the final result, to discard iterations that had not yet converged upon the posterior distribution. Lambert  recommend 4-8 chains for simple models, and \"few tens of chains\" for complex models, to ensure the MCMC sampler does not settle in local minima. Many sampling algorithms exist, and the state-of-the-art approach is the No-U-Turn Sampler (NUTS) , implemented in the Stan language.Many heuristics exist to ensure that the chains have mixed, in-cludingR , where values closest to 1.0 are optimal. A full suite of diagnostic tools is provided in shinystan in R . Because sampling methods at different points of the simulation can fail to meet the condition to sample more of the posterior, Lambert  suggest that an effective sample size is used to describe how many dependent samples are required to interpret statistics describing the posterior, mapped to a value of how many independent samples there are. For estimates of the 95% high-density interval limits, Kruschke  recommends an effective sample size of at least 10,000. If there are no diagnostic issues, the posterior may then be used.  Factor Analysis. A range of recent results in regard to variance contribute to understanding how to reduce error when modeling scores. Ferro and Silvello  use a General Linear Mixed Model (GLMM) to approximate system performance as an amalgamation of system effect, topic effect, and system-topic interactions, establishing the relative impact on retrieval scores of stop lists, stemmers, and retrieval models. GLMMs and two-way ANOVA were also used to explore shard and topic-shard interaction effects in distributed retrieval, greatly reducing the regression error . Carterette  compares a Bayesian linear model against the t-test, noting that using a t-test means implicitly accepting a model too.In our work here we employ aspects of all of these to model Bayesian inferences with topic and system effects, but excluding their interaction (system-topic effects, in other words) and shard effects in order to make simulation speeds tractable. Those component effects might be added back in the future to improve the reliability of score point estimates and credible intervals.We employ a hierarchical model for system and topic effects:where y t s is the effectiveness score observed for system s and topic t; T 0t is the effect size of topic t; S 0s is the effect size of system s; and e t s is an error term. Since other IR tests assume normality, we use a simple Gaussian prior distribution in this example, selecting the mathematical objects that describe the distributions, and leaving their hyper-parameters \u03c4 2 00 , \u03c7 2 00 , and \u03c3 2 to be automatically determined during the simulation, based on the input data.In order to find the most appropriate challenger to replace the champion, Bayesian sampling is used in an MCMC simulation. One advantage of sampling from the posterior is the generation of credible intervals for almost any summary statistic, as well as predictive intervals on data observations.  shows the relationship between these various components.   : System-topic scores used to demonstrate the utility of the Bayesian hierarchical inference model.  lists system-topic scores for an example over five systems and five topics, with a champion system being compared against four challengers. As well, a sample of 79 artifact systems (not shown) over the same test data is used to generate a posterior distribution.  shows the result of modeling the system-topic scores using sampling. Each observation has an associated predictive interval generated out of the simulation, and each model parameter has a credible interval. With only five different scores associated with each system, it makes sense that no clear winner (nor loser) emerges. The context provided by the artifact systems makes it apparent that all of the observed scores are in line with what is expected.The Bayesian model is based on the hierarchical model of Gelman et al. . The key idea is to use a \"common population distribution\" as a hyper-prior with learned hyper-parameters as we did in Equation 3 -for example, N is the hyper-prior of T 0t , which is  : Granularity of information made available when performing Bayesian inference. At top, credible intervals for the system effect intercepts are plotted with a green circle indicating the random effect point; and below it topic effect intervals are shown in green, with point estimates represented by triangles. Forecasts for 95% intervals based on the score observations are shown in pink in the remaining part of the diagram, with squares corresponding to actual scores .composed of the hyper-parameters 0 and \u03c4 2 00 . When the generalized linear model is created, with results for different instantiations of a single system grouped together and ordered topic-wise, the resulting intervals have subtle variations as performance changes, and at the same time converge towards the average effect (a concept referred to as partial-pooling). If full-pooling is used, a single average represents that random effect. When no pooling is used in  : Experimental systems and their average precision (AP) scores over the three different collections.a non-hierarchical model, prediction over-fitting becomes likely, and hence the model will not generalize to new data in the form of unseen systems. Intuitively, the convergence makes Bayesian inference more conservative when trying to find statistically significant random effects (for example, the challenger systems in ), unless a highly credible effect exists.As an aside, these results point to a potentially interesting effect in regard to the evaluation bias of runs that contributed to relevance judgments for a collection. Since runs that did, and did not, contribute to the judgment set are used as artifact systems, bias can be somewhat alleviated through partial pooling.Section 3 illustrated the power of Bayesian hierarchical modeling via five systems and five topics. This section examines a more detailed -and more typical -experimental configuration, using three different document collections, and 50 topics for each.Runs. The champion and first three challengers are from the Terrier v5.2 search engine, taking configurations originally proposed in the SIGIR Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR) workshop  as representative retrieval models.  describes these runs, and their corresponding meanover-topics effectiveness computed on three collections 2 . Bo1 query expansion was included in the second challenger, with ten feedback documents used, and 25 terms selected.The fourth challenger was designed to be a clear winner, and was produced as a fusion run merging the best three participant runs submitted to the TREC rounds associated with the three document collections. These nine runs (three per collection) were omitted from the set of artifact systems used to compute the Bayesian prior. For Robust04, CombSUM  fusion was used; and for TREC17 and TREC18, Reciprocal Rank fusion (RRF)  was employed. Both of these are simple techniques known to generate effective outcomes. All of the five runs are interesting in a risk-sensitive retrieval context, as query expansion is known to be risky due to query drift, and rank fusion has had low risk in previous comparisons .Corpora. Three collections of news documents were employed: the classic Robust04 corpus, and the more recent New York Times and Washington Post collections, TREC17 and TREC18 respectively.\u2022 Robust04 : Every fifth topic was used 301, 306, and so on, to obtain 50 topics. The collection contains \u2248 528k documents, \u2248 664k unique terms, and \u2248 253M terms in total. Of the 110 runs submitted, m = 79 are available as artifacts, once the bottom 25% had been removed, and the top three removed to create Challenger 4.\u2022 TREC17 : The TREC CORE 2017 Track exercise, with \u2248 1.85M documents, \u2248 2.97M unique terms, and \u2248 1.28T terms in total. There were runs submitted, with m = after removing the bottom 25% of runs and the top three.\u2022 TREC18: The TREC CORE 2018 Track exercise, \u2248 595k documents, \u2248 1.47M unique terms, and \u2248 481B terms in total. Of 72 runs submitted, m = 51 were used as artifacts.Bayesian MCMC. All simulations were executed using 12 Markov chains for 12,000 iterations, after 6,000 iterations of warm-up. Posteriors with 72,000 draws were produced when the chains were mixed. These choices were informed by inspecting the effective sample size of the posteriors post-hoc, ensuring that they were all above 10,000 to support inference with a 95% credible interval. To aid in reproducibility, we report the random seed value of 12,345. For transparency, all MCMC involving only effectiveness measures was computed with rstanarm, as extra families are not needed. If an experiment involved computing system scores with risk-adjusted scores applied, we used brms for either Gaussian or skew-normal prior experimentation. Both packages interface with the Stan probabilistic programming language. Both Stan front-ends use the lmer syntax when specifying a model, which is score(. Only Gaussian hyper-priors have been applied in hierarchical modeling previously, as selecting other distributions could reduce the ability to converge to the grand mean of the random effects, resulting in over-fitted models  .Metric. Runs and risk overlays were scored using average precision (AP), the official metric for all three of the collections. The first three experiments use AP scores without risk-adjustment. Our choice of risk-parameter is r = 5 for tabulated analysis, a moderatehigh value from the set of conventional IR values: 1, 2, 5, 10. When showing the effect of the risk parameter in diagrams, we adhere to this norm .Inference over Multiple Systems. Having provided details of the experimental resources and settings, we now describe the sequence of results obtained using them, comparing the systems in .  shows the benefit of employing all of the elements shown in  (with their exclusion policies) for the scenario in which the champion is compared against the four challengers. For all three corpora the champion system can be completely differentiated from Challenger 4 (which was the expected outcome), but with the Bayesian approach being more conservative than its frequentist counterpart. The lower plots in  show the 95% credible intervals for topic scores, and demonstrate the ability of the Bayesian approach to differentiate between \"easy\" and \"hard\" topics, opening avenues for possible future work.The Effect Of Artifact Systems. Recall ) that a set of artifact systems is used as background information. To explore the way in which the number of artifacts affects the discriminative power of the inferences generated, the number of artifact systems included was varied, after first ordering them from highest to lowest score. This is motivated by the concerns raised by Armstrong et al. , where one of the main messages is that it is important to evaluate against strong baselines. The number of systems in the pool in all cases is m + 5, with m varying as the set of artifacts is extended. Note that in order for the total of m + 5 systems to be fairly compared, an accurate prior distribution is critical, and hence changing the number of challengers would also have an effect the outcome of the inferences. Nevertheless, when the number of artifacts is substantially greater than the number of competing systems, the outcomes can be expected to be relatively stable, and here the champion and four challengers are compared without varying the number of challengers.  shows the outcome for the Robust04 and TREC17 collections (TREC18 omitted, but with results very similar to those for TREC17), comparing the champion to Challenger 4. When m = 1, the champion system and Challenger 4 are barely separable via a 95% credible interval for Robust04. When m \u2208 5, 10, 20, the systems are more distinguishable from each other; but as m approaches 40 the gap between the credible intervals starts to decrease again. This suggests that we cannot expect statistical significance to monotonically increase with respect to m as weaker systems are introduced to the pool of artifacts, a somewhat surprising result. The TREC17 collection yields a somewhat different pattern -there is a greater gap between the credible intervals at m = 40 than at m = 5. We conjecture that this effect is evidence of the high quality of runs submitted to the Robust04 track. Since effective artifact runs are being referenced as our universe of valid systems and our runs are also effective, more variability in effectiveness scores must be shown to differentiate runs from others.One concern with this experiment is that we are knowingly including less effective systems, with the systems added in mosteffective to least-effective order. This might suggest that the ability to discriminate good and bad runs increases because the prior is being conditioned on lower quality runs. However, in many cases in which the properties of runs are being exhaustively explored, the bottom 25% of runs are discarded. Since we wish to compare this approach to that of frequentist approaches (which are free to include in their methodology system runs that may not be the most competitive in real-life), we use all available run information, along with the most sensible exclusions applied as described above. Simulation studies such as those of Urbano et al.  may provide additional insights to establish the power of these approaches, an area that we leave for future work. (Note that the goal of this paper was to use real systems for evaluation where possible, and to develop the complete Bayesian framework.)We are now in a position to answer RQ1: the number of system artifacts used in a Bayesian inferential evaluation scenario plays an important role in determining the quality of the generated models, and the more runs available, the more consistent the performance.Academic Evaluation. In a typical academic evaluation scenario, a researcher will propose a new retrieval model as a challenger, and needs to show improvement relative to a set of champions, rather than comparing against a single system. In the Bayesian framework we have described, since all champion and challenger runs are input into a single inferential framework, the run labels  The Bayesian approach is also flexible enough to support manyvs-many system comparisons too. Suppose in  that we had originally set out to show that Challenger 4 is more effective than the other systems when regarded as a set of champions. In , we would see for TREC17 and TREC18 that it is indeed the best system. On Robust04, however, the credible interval overlaps on both Challenger 1 and Challenger 2.A key piece of what makes Bayesian inference over multiple artifact systems attractive is its capacity to give robust inferences that are non-committally tied to the set of baselines being evaluated against. Since \u03c7 00 in Equation 3 corresponds to the grand variance over the m + n + 1 systems used in the system evaluation poolwhere m can be regarded as being sufficiently large -it matters little to the outcome of the evaluation if a specific baseline system was or was not evaluated against, provided that a baseline with similar mean effectiveness was included. That is, a specific comparison relative to one recent result or another is no longer an experimental requirement, as long as some of the systems in the comparison pool are high quality and state-of-the-art. So concerns about baselines not being a very specific recent result are no longer as important when evaluating in this framework. Evidence of this claim is visible in all of the system comparison graphs in this paper, with the credible intervals for all system effects nearly identical.Bayesian Risk (One-To-One). Other work has shown that, as risk overlays linearly scale losses, the distribution becomes one-sided and may affect statistical methods that assume normality . Returning to a single system versus single system framework,  shows the distribution of mean scores for all of the 72,000 draws available from the posterior when risk-adjusted score differences are modeled as a Gaussian distribution, compared to a skew-normal distribution. These results confirm that the distribution becomes one-sided as r (the risk parameter) increases: the difference at r = 1 and r = 2 is negligible; but for r = 5 and r = 10 the intervals shifts to the right. In the frequentist paradigm the confidence intervals simply become more extended, and provide lower confidence predictions. Here, the density of the location parameter for the skew-normal distribution is narrower than the Gaussian alternative, a consequence of the data being more skewed after the score differences have been adjusted to the r = 5 and r = 10 levels.The inferential effects are due to the risk overlay and not reliant on any particular run or corpus combination. We also expect that increasing the number of topics evaluated to 249 will left-shift the skew-normal distribution to agree more with the Gaussian one, due to the central limit theorem. Although the skew-normal prior has proved useful in a one-to-one comparison, as noted in Section 3, the hyper-prior must be Gaussian, and so the outcome of this part cannot be used for multiple system evaluations of risk, providing at least a partial answer to RQ2: the one-to-one prior can be safely set to a skew-normal distribution, where it has a tighter density plot with respect to the location parameter. However, due to current constraints with multi-level modeling, risk inferences in one-tomany evaluation contexts must use a Gaussian distribution, which may give less consistent results for high r values. Nevertheless, evaluating risk with multiple systems is still possible subject to certain caveats, as is considered shortly.Bayesian Risk (One-Versus-Many). Extending the one-versusmany Bayesian approach to include a risk overlay is relatively simple, provided that it is remembered that absolute system scores are being considered, rather than score differences. To accomplish that, the champion's score are retained unchanged, with no score adjustments applied; while all challenger and artifact systems have risk-adjusted score differences applied to their per-topic scores. For example, if the champion's score for some topic was 0.50, and a challenger system obtained 0.45, an r = 2 adjustment would see that second score modified to 0.40 before inclusion in the simulation.A natural mathematical distribution to represent this score distribution is the skew-normal . Many system scores must be parameterized by a Gaussian hyper-prior, however, and this approach will only be useful when forming bounds on observations. That distribution is parameterized by a shape parameter as well as location and scale, which can be simulated using the Stan-driven Bayesian MCMC brms package.  compares the boundaries formed on system score observations for the skew-normal and Gaussian distributions when modeling risk, computed by including the sets of artifact runs (with their exclusion policies) when comparing the system effectiveness of a champion against the 4 challengers, for a range of values of r . This figure shows only Robust04 results, but system orderings by risk are consistent with those of TREC17 and TREC18. When r = 1, no risk is applied and the x-axis corresponds to (a reflection of) the y-axis in . At r = 2, Challenger 4 remains more risk-sensitive than Challenger 3 when compared to the champion system. When r = 5, the credible intervals begin to overlap, and they remain that way for r = 10. When r = 10, the system effect size point estimate of the champion system catches up to Challenger 1 and Challenger 2, while the point estimate for Challenger 4 remains marginally superior. This sequence of data provides Bayesian support for the result of Benham and Culpepper , that fusion reduces risk.In Section 3 we commented on the utility of modeling risk-scores using a skew-normal distribution rather than a Gaussian one for topic-by-topic comparisons. As Challenger 3 was consistently the riskiest choice, exploring which topics failed against the expected values learned from the background distribution of system scores might be illuminating.  compares the observed score from the champion system when compared against the risk adjusted score for Challenger 3, using r = 5. Scores in the positive direction indicate risk; negative scores, reward.On the skew-normal graph, the score for the champion system gives the highest reward across all predictive forecasts most of the time, indicating that the model is fitting the data well, since its scores are not penalized. On the other hand, the Gaussian plot is Challenger 4 is the best system to use without any risk adjustments applied, but for r = 5 onward it becomes indeterminate against the champion and the other challengers.unable to resolve lower score values particularly well, conflating the original system scores with the risk-adjusted scores. Observed risk adjusted scores from the Challenger 3 run are consistently higher across topics, due to it being riskier on average than the champion run. Of particular interest are the two riskiest topics, which have risk values so high that they sit right on the boundary of the 95% skew-normal forecast of where the score should lie. While there are topics on which Challenger 3 performs marginally better, nothing can be decided statistically as to which topic is less risky than any of the champion run's scores when r = 5. We now answer RQ2, having presented a complete example of how the Bayesian prior impacts the results of Bayesian risk data analysis.Bayesian Risk In Context. We have discussed at length how Bayesian techniques can be used to model effectiveness and risk. We now place the Bayesian risk discussion into context with existing measures that provide a numeric output and sometimes an accompanying frequentist inference. The goal remains the same; to compare the risk of one champion system versus many challengers. For methods that compare one system against another, we explore the use of URisk \u2212 , TRisk \u2212 , and BCa \u2212 . TRisk \u2212 scores are not corrected for multiple comparisons, consistent with the original paper, and are useful as a point of reference to show the \"false\" power that is present when the correction is not done. BCa \u2212 risk intervals are corrected using Bonferroni, with four hypotheses. We introduce the only approach designed to support one-versusmany comparisons as BRisk \u2212 , or B-for short. Finally, ZRisk \u2212 and GeoRisk \u2212 are included as many-vs-many measures. We do not include the m artifact systems into the pool of systems for these many-to-many risk measures, as it appears that a large number of systems reduces the resolution between the scores, which become skewed due to the loss penalty. In our approach, we use a large pool of artifact systems which are risk-adjusted to improve the signal of the Bayesian approach, and to help distinguish boundaries on observed risk values.  shows the results of this all-in experiment, over the three document collections. The inferential risk overlays that compare one system against another -with or without Bonferroni correction -are more likely than BRisk \u2212 to declare that changing to an alternative system is either statistically rewarding or statistically risky. That happens because BRisk \u2212 is constrained in the inferences  : Comparison of four challenger systems against a champion system using existing risk measures, and compared against the proposed Bayesian risk measure BRisk \u2212 , using r = 5 throughout. The BCa \u2212 interval is Bonferroni corrected for multiple comparisons, while TRisk \u2212 is not, to be consistent with the original publication, and as a point of reference to give an intuition behind the loss of power when correction in applied. Positive values in blue indicate that a system has a statistically significantly higher risk compared to the champion; negative values in blue indicate significant reward when compared to the champion.it produces, where simulated effectiveness scores must be sane with respect to the pool of systems provided to the model, thereby answering RQ3. In comparing the outputs of the many-to-many risk measures, GeoRisk \u2212 always preferences the highest-scoring run. Using only ZRisk \u2212 as a guide, Challenger 2 is the system with the lowest risk over every collection. That suggestion may be biased to the runs supplied to ZRisk \u2212 however, as Challenger 1 and Challenger 3 both share components with Challenger 2 whereas the BM25 champion and the fusion run are penalized like the single baseline case discussed in Din\u00e7er et al. . The Bayesian approach is capable of leveraging additional systems as artifacts to produce better informed decisions than beforewhere confidence in an evaluation stems from known system effectiveness profiles. Also, exploratory and failure-based data analysis over topics and systems is fully supported by the new framework, as long as there is a sufficient signal.It should be noted that the above framework does not invalidate the results of other testing methodologies, as no complete gold standard list of certain inferences exists to refer back to, in determining the correctness of statistical tests for different sample statistics. Running simulation studies to compare the false positive rate of this approach to others in the standard IR way  would be very expensive indeed. As the definition of what a probability value should represent is subjective, an objective researcher will report both Bayesian and frequentist inferences, and base their decisions on multiple forms of evidence.We have applied an adaptation of Bayesian hierarchical modeling using MCMC to the task of comparing the effectiveness of multiple retrieval systems. Using that methodology, we explored inferential evaluation of topic and system data comparing a champion system to a set of challengers, both without and with the overlay of risk.In a study using run configurations from the RIGOR workshop, we found that including many diverse runs as artifacts enables the Bayesian inferential approach to be both discriminatory and generalizable. Given the nature of inferring information from past experience, the outcome of these experiments would naturally change as more evidence about the universe of known systems grows. We also explored the use of a skew-normal prior instead of the Gaussian when evaluating risk adjusted scores, and found that it was useful for one-to-one evaluation scenarios, or when forecasting risk score observations in an exploratory topic-wise data analysis.Finally, we found that for the systems used in this study, Bayesian and frequentist confidence and credible intervals differ, and that Bayesian Risk (BRisk \u2212 ) is conservative in its inferential statements, based on pools of scores that are produced by a set of legitimate artifact IR systems.Software. Code that reproduces the experiments reported in this work is available at https://github.com/rmit-ir/bayesian-risk.", "videoStruct": [{"timeStart": "00-00-00", "timeEnd": "00-00-04", "sentence": "hi everyone and welcome to the talk by asking him for Rachel risk evaluation on"}, {"timeStart": "00-00-04", "timeEnd": "00-00-06", "sentence": "multiple information retrieval systems"}, {"timeStart": "00-00-06", "timeEnd": "00-00-08", "sentence": "I'm your speaker Roger Bannon"}, {"timeStart": "00-00-08", "timeEnd": "00-00-12", "sentence": "and this paper is also coauthored with Ben cataract from spy"}, {"timeStart": "00-00-12", "timeEnd": "00-00-15", "sentence": "and my two supervisors shame and also"}, {"timeStart": "00-00-17", "timeEnd": "00-00-20", "sentence": "in statistical inference we think about"}, {"timeStart": "00-00-20", "timeEnd": "00-00-24", "sentence": "how can we make inferences based on what it is"}, {"timeStart": "00-00-24", "timeEnd": "00-00-30", "sentence": "observable over the entire universe of values based on what we can adequately sample"}, {"timeStart": "00-00-31", "timeEnd": "00-00-34", "sentence": "so left him we've got the sample"}, {"timeStart": "00-00-34", "timeEnd": "00-00-36", "sentence": "but simply from the population distribution"}, {"timeStart": "00-00-36", "timeEnd": "00-00-40", "sentence": "and we have this representative sample of what we know about"}, {"timeStart": "00-00-40", "timeEnd": "00-00-43", "sentence": "say for instance information retrieval schools"}, {"timeStart": "00-00-43", "timeEnd": "00-00-46", "sentence": "so any information were trade was school with got"}, {"timeStart": "00-00-46", "timeEnd": "00-00-52", "sentence": "combinations of SR and topic effects which forms and affect the school"}, {"timeStart": "00-00-53", "timeEnd": "00-00-59", "sentence": "and what we typically use is a student places in ionic moment"}, {"timeStart": "00-00-59", "timeEnd": "00-01-05", "sentence": "however there's been other tests that have been proposed like the rain penetration tests and bootstrap"}, {"timeStart": "00-01-05", "timeEnd": "00-01-09", "sentence": "and so under an umpire metric that don't have the amount of"}, {"timeStart": "00-01-09", "timeEnd": "00-01-11", "sentence": "students"}, {"timeStart": "00-01-11", "timeEnd": "00-01-14", "sentence": "however we know that information retrieval scores"}, {"timeStart": "00-01-14", "timeEnd": "00-01-18", "sentence": "typically they are up to these kinds of"}, {"timeStart": "00-01-18", "timeEnd": "00-01-21", "sentence": "violations of assumptions which is"}, {"timeStart": "00-01-25", "timeEnd": "00-01-31", "sentence": "however we know with with the scores that they're basing it on this hypothetical population distribution where"}, {"timeStart": "00-01-31", "timeEnd": "00-01-37", "sentence": "you could have a system that is really terrible or a system that doesn't even exist yet"}, {"timeStart": "00-01-37", "timeEnd": "00-01-40", "sentence": "I went with these hypothetical"}, {"timeStart": "00-01-40", "timeEnd": "00-01-43", "sentence": "re sampling methods and replicate school so"}, {"timeStart": "00-01-43", "timeEnd": "00-01-49", "sentence": "what if we were able to perform in France is based on what we actually know about information retrieval skills"}, {"timeStart": "00-01-50", "timeEnd": "00-01-56", "sentence": "these systems likely enough submitted to track and anti CIA which practitioners have"}, {"timeStart": "00-01-56", "timeEnd": "00-02-00", "sentence": "sorry we want to know what happens when we use those schools to form"}, {"timeStart": "00-02-01", "timeEnd": "00-02-03", "sentence": "so I research question one is"}, {"timeStart": "00-02-03", "timeEnd": "00-02-08", "sentence": "I was using previous system out of out so I can call bae systems out of eggs"}, {"timeStart": "00-02-08", "timeEnd": "00-02-12", "sentence": "as it affects the base Union for ritual results for IRS collections"}, {"timeStart": "00-02-12", "timeEnd": "00-02-17", "sentence": "the next part of this title is risk risk"}, {"timeStart": "00-02-17", "timeEnd": "00-02-20", "sentence": "as many different either legged connotations in iron butt"}, {"timeStart": "00-02-20", "timeEnd": "00-02-23", "sentence": "for for now let's talk about risk as in"}, {"timeStart": "00-02-23", "timeEnd": "00-02-28", "sentence": "you risk which was proposed by laying it out as a"}, {"timeStart": "00-02-28", "timeEnd": "00-02-29", "sentence": "when he the right function"}, {"timeStart": "00-02-29", "timeEnd": "00-02-31", "sentence": "what my objective function"}, {"timeStart": "00-02-31", "timeEnd": "00-02-33", "sentence": "we've got this"}, {"timeStart": "00-02-34", "timeEnd": "00-02-36", "sentence": "when you've got a"}, {"timeStart": "00-02-36", "timeEnd": "00-02-40", "sentence": "experimental system being compared against a baseline system"}, {"timeStart": "00-02-40", "timeEnd": "00-02-45", "sentence": "sorry you take the sum of the school differences per topic that were wins"}, {"timeStart": "00-02-45", "timeEnd": "00-02-46", "sentence": "and think of this life in a"}, {"timeStart": "00-02-46", "timeEnd": "00-02-48", "sentence": "aggregate function like them eight"}, {"timeStart": "00-02-48", "timeEnd": "00-02-53", "sentence": "and then you subtract the sum of the score differences that are losses hello"}, {"timeStart": "00-02-53", "timeEnd": "00-02-56", "sentence": "different to the Arafat ik man we have this ah"}, {"timeStart": "00-02-57", "timeEnd": "00-03-00", "sentence": "styles up the impact of those losses"}, {"timeStart": "00-03-00", "timeEnd": "00-03-04", "sentence": "that's meant to reflect a different searches risk aversion"}, {"timeStart": "00-03-04", "timeEnd": "00-03-07", "sentence": "and studies in economics no"}, {"timeStart": "00-03-08", "timeEnd": "00-03-11", "sentence": "humans are typically risky this right"}, {"timeStart": "00-03-11", "timeEnd": "00-03-14", "sentence": "a game of a hundred dollars"}, {"timeStart": "00-03-14", "timeEnd": "00-03-17", "sentence": "has twice as much if you lose that hundred dollars"}, {"timeStart": "00-03-19", "timeEnd": "00-03-22", "sentence": "why is this interesting from a statistical perspective so"}, {"timeStart": "00-03-22", "timeEnd": "00-03-28", "sentence": "previous work such as taking risk of confidence which we worked on last year at a d see s"}, {"timeStart": "00-03-31", "timeEnd": "00-03-33", "sentence": "different risk values"}, {"timeStart": "00-03-33", "timeEnd": "00-03-39", "sentence": "and this is the best submitted for caster for vs being twenty-five"}, {"timeStart": "00-03-39", "timeEnd": "00-03-45", "sentence": "and when the risk value is the same surely they all agree are of each other however"}, {"timeStart": "00-03-45", "timeEnd": "00-03-47", "sentence": "when you increase the risk value"}, {"timeStart": "00-03-47", "timeEnd": "00-03-55", "sentence": "the distributions get more as a metric and the assumptions that were made on the test no longer apply anymore"}, {"timeStart": "00-03-55", "timeEnd": "00-03-57", "sentence": "it needs to be"}, {"timeStart": "00-03-57", "timeEnd": "00-04-01", "sentence": "corrected for this by studio secretary to distribution"}, {"timeStart": "00-04-01", "timeEnd": "00-04-05", "sentence": "sorry how does that time devising your friends so"}, {"timeStart": "00-04-05", "timeEnd": "00-04-07", "sentence": "in Beijing influence we have"}, {"timeStart": "00-04-07", "timeEnd": "00-04-11", "sentence": "which is a mathematical objects that we ascribe"}, {"timeStart": "00-04-11", "timeEnd": "00-04-13", "sentence": "understanding what we think"}, {"timeStart": "00-04-13", "timeEnd": "00-04-15", "sentence": "previous stage said"}, {"timeStart": "00-04-15", "timeEnd": "00-04-19", "sentence": "what be the shape of the distribution should look like"}, {"timeStart": "00-04-19", "timeEnd": "00-04-22", "sentence": "when we combine that with the likelihood"}, {"timeStart": "00-04-22", "timeEnd": "00-04-24", "sentence": "the likelihood is"}, {"timeStart": "00-04-24", "timeEnd": "00-04-28", "sentence": "observations that we have observed so think of that all of"}, {"timeStart": "00-04-28", "timeEnd": "00-04-30", "sentence": "the frequent has test before"}, {"timeStart": "00-04-30", "timeEnd": "00-04-33", "sentence": "I'm sick that's your likelihood"}, {"timeStart": "00-04-33", "timeEnd": "00-04-39", "sentence": "and then we combine them together and we get this posterior distribution which is known as the completion friends"}, {"timeStart": "00-04-39", "timeEnd": "00-04-41", "sentence": "and that's what we form now"}, {"timeStart": "00-04-41", "timeEnd": "00-04-43", "sentence": "credible intervals on"}, {"timeStart": "00-04-44", "timeEnd": "00-04-49", "sentence": "a cry is the most controversial aspect of Beijing in France"}, {"timeStart": "00-04-49", "timeEnd": "00-04-54", "sentence": "so it can be formed from previous studies and that's known as a informative"}, {"timeStart": "00-04-55", "timeEnd": "00-05-01", "sentence": "but typically what's done is we use a weekly informative prior with proud of the survey"}, {"timeStart": "00-05-01", "timeEnd": "00-05-09", "sentence": "but we know you something about so we would say that ok this the school distribution looks like it might be gaussian so let's say it's Kelsey"}, {"timeStart": "00-05-09", "timeEnd": "00-05-13", "sentence": "and so the Martha came with the college simulation"}, {"timeStart": "00-05-15", "timeEnd": "00-05-18", "sentence": "updates the parameters in this week"}, {"timeStart": "00-05-18", "timeEnd": "00-05-20", "sentence": "weekly informative prior"}, {"timeStart": "00-05-20", "timeEnd": "00-05-25", "sentence": "to push it towards a more credible value for which we would then perform inference on"}, {"timeStart": "00-05-26", "timeEnd": "00-05-32", "sentence": "of interest is to compare this skewed normal distribution which takes three parameters x gonna main"}, {"timeStart": "00-05-32", "timeEnd": "00-05-37", "sentence": "it's got a variance and it's got this parameter"}, {"timeStart": "00-05-37", "timeEnd": "00-05-39", "sentence": "and we want to compare that with the gaussian"}, {"timeStart": "00-05-40", "timeEnd": "00-05-44", "sentence": "distribution on rejected score differences to see if we can get"}, {"timeStart": "00-05-44", "timeEnd": "00-05-46", "sentence": "intervals based on means"}, {"timeStart": "00-05-46", "timeEnd": "00-05-48", "sentence": "mean ultimately determines"}, {"timeStart": "00-05-48", "timeEnd": "00-05-51", "sentence": "which system was different on your test"}, {"timeStart": "00-05-52", "timeEnd": "00-05-55", "sentence": "so I research question to is how does the Beijing fire"}, {"timeStart": "00-05-55", "timeEnd": "00-06-00", "sentence": "inferential results using risk adjusted scores for one to one comparison and one to me"}, {"timeStart": "00-06-00", "timeEnd": "00-06-06", "sentence": "and then finally the third research question is using the other risk measures in the lid childhood home discussed later on"}, {"timeStart": "00-06-06", "timeEnd": "00-06-10", "sentence": "how does our perked compared against them for streaming"}, {"timeStart": "00-06-10", "timeEnd": "00-06-12", "sentence": "inferential"}, {"timeStart": "00-06-12", "timeEnd": "00-06-15", "sentence": "differences and just how different"}, {"timeStart": "00-06-15", "timeEnd": "00-06-17", "sentence": "how much different is it"}, {"timeStart": "00-06-18", "timeEnd": "00-06-24", "sentence": "had a Beijing and frequent as credible and confidence intervals different by performing risk adjusted evaluation"}, {"timeStart": "00-06-24", "timeEnd": "00-06-29", "sentence": "credible intervals of Beijing into the and a confidence into a comes from a frequent test test"}, {"timeStart": "00-06-31", "timeEnd": "00-06-37", "sentence": "sorry in the approach that we use in our paper let's talk about Beijing hierarchical modeling so"}, {"timeStart": "00-06-37", "timeEnd": "00-06-41", "sentence": "hierarchical modeling is a part of the Beijing so toolkit"}, {"timeStart": "00-06-47", "timeEnd": "00-06-51", "sentence": "assumes that everything comes from the same distribution sorry"}, {"timeStart": "00-06-51", "timeEnd": "00-06-58", "sentence": "something can be attributed to this is a global overall system affects but then"}, {"timeStart": "00-06-58", "timeEnd": "00-07-00", "sentence": "there are down the hierarchy"}, {"timeStart": "00-07-01", "timeEnd": "00-07-05", "sentence": "her system affects the and near the system effects that we want to be deciding"}, {"timeStart": "00-07-05", "timeEnd": "00-07-08", "sentence": "this system has been over the other system in vice pres"}, {"timeStart": "00-07-09", "timeEnd": "00-07-11", "sentence": "sorry"}, {"timeStart": "00-07-11", "timeEnd": "00-07-14", "sentence": "a prominent statistician Andrew Jones"}, {"timeStart": "00-07-14", "timeEnd": "00-07-17", "sentence": "as suggested we don't usually have to compare for these multiple"}, {"timeStart": "00-07-17", "timeEnd": "00-07-21", "sentence": "we don't have to correct for these multiple comparisons because of two reasons one"}, {"timeStart": "00-07-21", "timeEnd": "00-07-28", "sentence": "we are looking at early at school observations that we don't care about and we don't comparing against the total population"}, {"timeStart": "00-07-28", "timeEnd": "00-07-31", "sentence": "and secondly this powerful pulling apart"}, {"timeStart": "00-07-31", "timeEnd": "00-07-35", "sentence": "takes away some of the risk of the false positive rate"}, {"timeStart": "00-07-35", "timeEnd": "00-07-40", "sentence": "I'm basing this explanation of bass you in hierarchical modeling on this map"}, {"timeStart": "00-07-41", "timeEnd": "00-07-43", "sentence": "check out that link"}, {"timeStart": "00-07-43", "timeEnd": "00-07-44", "sentence": "if you would like to know more"}, {"timeStart": "00-07-57", "timeEnd": "00-08-03", "sentence": "system scores are governed by some kind of physics for water system scholar should be right so"}, {"timeStart": "00-08-03", "timeEnd": "00-08-08", "sentence": "it should be based on record should be based on precision on and"}, {"timeStart": "00-08-08", "timeEnd": "00-08-12", "sentence": "and sometimes some topics are just hard to get scores on him someone"}, {"timeStart": "00-08-13", "timeEnd": "00-08-17", "sentence": "ah if it altogether and ascribed based on like this"}, {"timeStart": "00-08-17", "timeEnd": "00-08-21", "sentence": "one giant system distribution and schools are being pulled from that"}, {"timeStart": "00-08-22", "timeEnd": "00-08-28", "sentence": "okay cool but we're not going to be able to ascertain which system is better using this approach because we need to be"}, {"timeStart": "00-08-29", "timeEnd": "00-08-33", "sentence": "deciding which system is better based on the model parameters"}, {"timeStart": "00-08-33", "timeEnd": "00-08-36", "sentence": "not score observations in the Beijing life"}, {"timeStart": "00-08-36", "timeEnd": "00-08-39", "sentence": "sorry what we do is we can combine"}, {"timeStart": "00-08-40", "timeEnd": "00-08-45", "sentence": "this global system main with the system score interactions"}, {"timeStart": "00-08-45", "timeEnd": "00-08-47", "sentence": "and this is cold coffee boy"}, {"timeStart": "00-08-47", "timeEnd": "00-08-53", "sentence": "and we know that system in system topic interactions forming effectiveness metrics"}, {"timeStart": "00-08-54", "timeEnd": "00-08-56", "sentence": "strictly governed by a system"}, {"timeStart": "00-08-57", "timeEnd": "00-09-00", "sentence": "typically dominated by the topic effect"}, {"timeStart": "00-09-00", "timeEnd": "00-09-06", "sentence": "but some topics are easier to get good scores on an odd or another's so if we combine that in our motto"}, {"timeStart": "00-09-06", "timeEnd": "00-09-09", "sentence": "and we probably gay Tara up into this carrot own where we"}, {"timeStart": "00-09-10", "timeEnd": "00-09-15", "sentence": "we excluded colpus effects and system topic interactions"}, {"timeStart": "00-09-15", "timeEnd": "00-09-19", "sentence": "we can conform this into the phone"}, {"timeStart": "00-09-19", "timeEnd": "00-09-23", "sentence": "and then we can include these artifacts systems into this system"}, {"timeStart": "00-09-23", "timeEnd": "00-09-29", "sentence": "and then get even cleaning systems inference is based on what is known about the system"}, {"timeStart": "00-09-30", "timeEnd": "00-09-34", "sentence": "other risk measures arm got you risk which is a descriptive measure"}, {"timeStart": "00-09-34", "timeEnd": "00-09-36", "sentence": "to risk my teacher out"}, {"timeStart": "00-09-47", "timeEnd": "00-09-51", "sentence": "is this ethical receipt or not"}, {"timeStart": "00-09-52", "timeEnd": "00-09-54", "sentence": "the issue that danger out"}, {"timeStart": "00-09-54", "timeEnd": "00-09-59", "sentence": "looked at was that when you've got this one too many different systems scenario"}, {"timeStart": "00-09-59", "timeEnd": "00-10-02", "sentence": "there's this vibe towards the original rancor"}, {"timeStart": "00-10-02", "timeEnd": "00-10-06", "sentence": "so to correct for that babe from this"}, {"timeStart": "00-10-06", "timeEnd": "00-10-10", "sentence": "school online ation of parts with a matrix of system and topic of purchase"}, {"timeStart": "00-10-10", "timeEnd": "00-10-16", "sentence": "and then they those system scores get transformed into a z score which is then applied"}, {"timeStart": "00-10-16", "timeEnd": "00-10-18", "sentence": "I risk very"}, {"timeStart": "00-10-18", "timeEnd": "00-10-21", "sentence": "and then the geometric mean"}, {"timeStart": "00-10-21", "timeEnd": "00-10-26", "sentence": "of the system effectiveness and the zero scores combined for jurist"}, {"timeStart": "00-10-27", "timeEnd": "00-10-29", "sentence": "approach doesn't"}, {"timeStart": "00-10-29", "timeEnd": "00-10-32", "sentence": "allow for influential cousins are so average"}, {"timeStart": "00-10-32", "timeEnd": "00-10-37", "sentence": "allows for one to many for it right for comparison and it also combines"}, {"timeStart": "00-10-37", "timeEnd": "00-10-40", "sentence": "the nice properties of bathing in France as well"}, {"timeStart": "00-10-41", "timeEnd": "00-10-43", "sentence": "sorry to wake happen research questions"}, {"timeStart": "00-10-43", "timeEnd": "00-10-48", "sentence": "how does using previous dissed amount of facts affect Beijing in Frankel results for my AR test collections"}, {"timeStart": "00-10-48", "timeEnd": "00-10-52", "sentence": "how does the Beijing prior affecting French will results using risk adjusted schools"}, {"timeStart": "00-10-52", "timeEnd": "00-10-55", "sentence": "one to one comparison and went to many"}, {"timeStart": "00-10-55", "timeEnd": "00-11-02", "sentence": "and finally how does the Beijing frequent dis credible and confidence intervals different by performing risk adjusted valuation"}, {"timeStart": "00-11-02", "timeEnd": "00-11-04", "sentence": "so here's our approach we have"}, {"timeStart": "00-11-04", "timeEnd": "00-11-05", "sentence": "five different systems"}, {"timeStart": "00-11-05", "timeEnd": "00-11-09", "sentence": "Richard challenges one to three were selected from the"}, {"timeStart": "00-11-09", "timeEnd": "00-11-11", "sentence": "reproduce ability truck"}, {"timeStart": "00-11-11", "timeEnd": "00-11-14", "sentence": "and then we have a challenger for which is this manufactured from"}, {"timeStart": "00-11-14", "timeEnd": "00-11-20", "sentence": "based on the top three systems that was fitted to each respective track track of robust are for"}, {"timeStart": "00-11-20", "timeEnd": "00-11-24", "sentence": "track or two thousand and seventeen and trick called you the name"}, {"timeStart": "00-11-24", "timeEnd": "00-11-28", "sentence": "and we use reciprocal right fusion all concerned to coming on the track"}, {"timeStart": "00-11-28", "timeEnd": "00-11-30", "sentence": "and we've got this really"}, {"timeStart": "00-11-30", "timeEnd": "00-11-32", "sentence": "highly effected from which we want to be out"}, {"timeStart": "00-11-33", "timeEnd": "00-11-35", "sentence": "that Robert is better than"}, {"timeStart": "00-11-35", "timeEnd": "00-11-41", "sentence": "being twenty five on why's our purchasing very good at determining which one has been on"}, {"timeStart": "00-12-05", "timeEnd": "00-12-09", "sentence": "and then on the estimate on the website"}, {"timeStart": "00-12-09", "timeEnd": "00-12-16", "sentence": "a value of zero is the main system affects our challenge for is better than average from all the other systems thankfully"}, {"timeStart": "00-12-29", "timeEnd": "00-12-36", "sentence": "so as we include more systems in this phone we get more germination however that's crimination is not always"}, {"timeStart": "00-12-36", "timeEnd": "00-12-40", "sentence": "monatomic increasing which was somewhat surprising but a testament to"}, {"timeStart": "00-12-40", "timeEnd": "00-12-44", "sentence": "the quality of systems that were submitted to roster for"}, {"timeStart": "00-12-44", "timeEnd": "00-12-49", "sentence": "in Czech set obtain and check out a new get a purchase of schools similar to this"}, {"timeStart": "00-12-49", "timeEnd": "00-12-52", "sentence": "so that's our first research question a dress"}, {"timeStart": "00-12-52", "timeEnd": "00-12-57", "sentence": "and the second research question when looking at daisy and prize right sorry"}, {"timeStart": "00-12-57", "timeEnd": "00-13-00", "sentence": "in the month of mine comparison by comparing"}, {"timeStart": "00-13-00", "timeEnd": "00-13-03", "sentence": "it's submitted to the treasury"}, {"timeStart": "00-13-03", "timeEnd": "00-13-05", "sentence": "to record a full"}, {"timeStart": "00-13-05", "timeEnd": "00-13-07", "sentence": "and then preparing against being twenty-five"}, {"timeStart": "00-13-07", "timeEnd": "00-13-11", "sentence": "and then I compare a gaussian prime of his funeral"}, {"timeStart": "00-13-11", "timeEnd": "00-13-15", "sentence": "as we expect when the risk value doesn't change they both agree with each other"}, {"timeStart": "00-13-15", "timeEnd": "00-13-18", "sentence": "as we increase this risk value"}, {"timeStart": "00-13-18", "timeEnd": "00-13-26", "sentence": "they tend to disagree with each other and their shifting to the right which was in line with what we saw in the previous confidence intervals"}, {"timeStart": "00-13-26", "timeEnd": "00-13-29", "sentence": "with sea by scripture the purchase"}, {"timeStart": "00-13-29", "timeEnd": "00-13-30", "sentence": "but also we get a title"}, {"timeStart": "00-13-30", "timeEnd": "00-13-35", "sentence": "credible interval around the density of the location parameter for kino"}, {"timeStart": "00-13-35", "timeEnd": "00-13-40", "sentence": "so this is this is not right this is what we want because then we can make more discriminative"}, {"timeStart": "00-13-40", "timeEnd": "00-13-42", "sentence": "choice is between"}, {"timeStart": "00-13-42", "timeEnd": "00-13-44", "sentence": "bless for student system being summer"}, {"timeStart": "00-13-45", "timeEnd": "00-13-50", "sentence": "the Wayne really doesn't come in this one too many bathing risk approach because"}, {"timeStart": "00-13-50", "timeEnd": "00-13-54", "sentence": "although these different systems in that hierarchy to perform"}, {"timeStart": "00-13-54", "timeEnd": "00-14-02", "sentence": "need to be formed using gaussian shape distributions otherwise this is my way to for the powerful pulling approach to work"}, {"timeStart": "00-14-02", "timeEnd": "00-14-08", "sentence": "sorry where the winners actually comes into these observations on this topic level"}, {"timeStart": "00-14-08", "timeEnd": "00-14-10", "sentence": "where you can stay at the top right of this graph"}, {"timeStart": "00-14-11", "timeEnd": "00-14-17", "sentence": "predictive into bullets well away from where the daughter points for actually suggesting that it should be"}, {"timeStart": "00-14-17", "timeEnd": "00-14-23", "sentence": "sorry skin on the prize better able to fit the data and more tightly"}, {"timeStart": "00-14-23", "timeEnd": "00-14-26", "sentence": "and so that all the answers research question to"}, {"timeStart": "00-14-28", "timeEnd": "00-14-30", "sentence": "surgery set question"}, {"timeStart": "00-14-30", "timeEnd": "00-14-35", "sentence": "we're not able to share all of the diver and the slag but effectively put"}, {"timeStart": "00-14-35", "timeEnd": "00-14-42", "sentence": "Beijing wrestler perches how it's different to the other frame this purchase is that its more conservative in"}, {"timeStart": "00-14-42", "timeEnd": "00-14-44", "sentence": "the inference is that it provides"}, {"timeStart": "00-14-44", "timeEnd": "00-14-50", "sentence": "purely based on the fact that it can only base these inferences on what is known about system"}, {"timeStart": "00-14-52", "timeEnd": "00-14-55", "sentence": "rather than the entire hypothetical population"}, {"timeStart": "00-14-55", "timeEnd": "00-14-57", "sentence": "different schools"}, {"timeStart": "00-14-57", "timeEnd": "00-15-01", "sentence": "in any case there's no harm in combining these"}, {"timeStart": "00-15-01", "timeEnd": "00-15-05", "sentence": "frequent post in france's with Maisie differences as well"}, {"timeStart": "00-15-05", "timeEnd": "00-15-08", "sentence": "in fact they are they are complimentary right"}, {"timeStart": "00-15-08", "timeEnd": "00-15-14", "sentence": "so the key take away from his station is by using Beijing hierarchical models of system effects"}, {"timeStart": "00-15-14", "timeEnd": "00-15-17", "sentence": "you get inferences based on a set of new reference systems"}, {"timeStart": "00-15-17", "timeEnd": "00-15-23", "sentence": "which also corrects the multiple comparisons as part of the Beijing hierarchical pulling process"}, {"timeStart": "00-15-23", "timeEnd": "00-15-26", "sentence": "because is available for"}, {"timeStart": "00-15-26", "timeEnd": "00-15-34", "sentence": "for doing some of this stuff in your own research if you'd like I could hug and thank you so much for watching"}]}, {"title": "[CVPR 2020 Award Nominee] Deep Geometric Functional Maps: Robust Feature Learning for Shape Correspondence", "authors": "ComputerVisionFoundation Videos", "abstract": "", "publicationOrg": "CVPR", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2003.14286.pdf", "pdfPath": "/data/cache/2/PDFs/CVPR2020AwardNomineeDeepGeometricFunctionalMapsRobustFeatureLearningforShapeCorrespondence.pdf", "publicationUrl": "https://arxiv.org/pdf/2003.14286.pdf", "codeUrl": "https://github.com/LIX-shape-analysis/GeomFmaps.", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/_K15Gg7MNTY", "videoPath": "/data/cache/2/videos/Deep Geometric Functional Maps- Robust Feature Learning for Shape Correspondence.mp4", "pdfText": "Shape correspondence is a key problem in computer vision, computer graphics and related fields with a broad range of applications, including texture or deformation transfer and statistical shape analysis , among many others. While classical correspondence methods have been based on handcrafted features or deformation models , more recent approaches have focused on learning an optimal model from the data either in supervised  or even unsupervised settings .Despite significant progress in recent years, however, learning-based approaches for shape correspondence typically require large amounts of training data in order to learn a model that generalizes well to diverse shape classes . Several existing methods address this challenge by learning a derived representation, through a nonlinear transformation of pre-computed feature descriptors , rather than on the geometry of the shapes themselves. Unfortunately, as we demonstrate below, this . Given a pair of shapes, our approach builds consistent descriptors directly from the underlying point clouds (left), and automatically computes an accurate pointwise correspondence (right).reliance on a priori hand-crafted descriptors makes the resulting learned models both less robust and less accurate leading to a significant drop in generalization power to new shape classes or instances.In this work, we propose an approach that combines the power of learning directly from the 3D shapes with strong regularization based on a novel spectral correspondence extraction layer. Our method is inspired by recent learning techniques employing the functional map representation ; however, we extend them to learn the features from 3D geometry rather than from some pre-computed de-scriptors. Furthermore, we introduce a regularizer into the functional map computation layer that greatly improves the speed and robustness of training. Finally, we demonstrate how the spectral loss based on the functional map representation in the reduced basis significantly reduces overfitting, while still leading to accurate correspondences coupled with recent post-processing techniques. As a result, our overall pipeline is both more robust and has greater generalization power than existing methods, while still being able to learn from limited training data.Computing point-to-point maps between two 3D discrete surfaces is a very well-studied area of computer vision. Below, we review those closest to our method, or with the best known results to serve as baselines, and refer to recent surveys  for an in-depth discussion.Our method is built upon the functional map representation, which was originally introduced in  as a tool for non-rigid shape matching, and then extended in follow-up works . The key property of this representation is being able to express maps as small matrices, encoded in a reduced basis, which greatly simplifies the associated optimization problems.The original work used only a basic set of constraints on functional maps, which have been extended significantly in, e.g.,  among many other works. These approaches both extend the generality and improve the robustness of the functional map estimation pipeline, by using regularizers, robust penalties and powerful post-processing of the computed maps.A key challenge in all of functional map estimation techniques, however, is the strong reliance on given input descriptors used for computing the maps. Several approaches have suggested to use robust norms , improved pointwise map recovery  or more principled regularizers  which can help alleviate noise in the input descriptors to a certain extent but do not resolve strong inconsistencies in challenging cases.More recent techniques have advocated learning optimal descriptors for functional map estimation directly from the data . These methods compute a transformation of given input descriptors so that the estimated functional maps are close to ground truth maps given during training. This idea was very recently extended to the unsupervised setting  where the supervised loss was replaced with structural penalties on the computed maps.Despite significant progress, however, in all of these cases, the descriptors are optimized through a transformation of hand-crafted input features, such as SHOT , Heat  or Wave kernel signatures . This has two severe consequences: first, any information not present in the input features will be absent from the optimized descriptors, and second, such approaches generalize poorly across datasets as the input features can change significantly. This is particularly true of the commonly-used SHOT descriptors , which are sensitive to the triangle mesh structure and, as we show below, can vary drastically across different datasets.A number of other techniques have also been proposed for shape correspondence learning without using the functional map representation. These include approaches that exploit novel convolutional layers on triangle meshes  and more general methods that use learning from depth-maps  or in some feature space  among many others. Remarkably, relatively few methods aim to learn directly from the raw 3D shape geometry for shape correspondence, with the notable exceptions of . In large part this is due to the complexity of the correspondence problem, where unlike, e.g., shape segmentation, the number of labels can be unbounded. As a result, existing techniques address this either by learning from precomputed features, or relying on template-based matching and large training sets , that might even require manual curation. Although PointNet  and its variants  achieve impressive results from raw point clouds for classification tasks, they are not yet competitive for shape correspondence task.In this paper we show that feature learning for shape matching can be done directly from the raw 3D geometry even in the presence of relatively little training data, and without relying on a template or an a priori parametric (e.g., human body) model. Our main contribution is a end-toend learnable pipeline that computes features from the 3D shapes and uses them for accurate dense point-to-point correspondence. We achieve this by introducing a novel map extraction layer using the functional map representation in a reduced basis, which provides a very strong regularization. Finally, we demonstrate that recent refinement techniques adapted to small functional maps , combined with our efficient learning pipeline jointly result in accurate dense maps at the fraction of the cost of existing methods.One of the building blocks in our pipeline work is based on the functional map framework and representation. For completeness, we briefly review the basic notions for estimating functional maps, and refer the interested reader to a recent course  for a more in-depth discussion.Basic Pipeline Given a pair of 3D shapes, M, N represented in a discrete setting as triangle meshes, and containing respectively m and n vertices, this pipeline aims at computing a map between them.It consists in four main steps. First, the first few eigen-functions of the discrete Laplace-Beltrami operator are computed on each shape, namely k M and k N functions respectively. Second, a set of descriptor functions on each shape that are expected to be approximately preserved by the unknown map. For instance, a descriptor function can correspond to a particular dimension of the Heat or Wave Kernel Signatures  computed at each point. Their coefficients are stored in the respective basis as columns in matrices A, B.Third, the optimal functional map C is then computed by solving the following optimization problem:where the first term aims at preserving the descriptors:whereas the second term regularizes the map by promoting the correctness of its overall structural properties. It is common to use Frbenius norm to compute the distance between these matrices. This Eq. leads to a simple least-squares problem with k M \u00d7 k N unknowns, independent of the number of points on the shapes. As a last step, the estimated functional map C, which maps across the spectral domains and converted to a pointto-point map. As a post processing step, called refinement, a number of advanced techniques are available . Most of them iteratively take the map from spectral to spatial domain, until it reaches a local optimum.Despite its simplicity and efficiency, being a sequential framework, the functional map estimation pipeline described above is fundamentally error prone, due to the initial choice of descriptor functions. To alleviate this dependence, several approaches have been proposed to learn an optimal transformation of initial descriptors from data . These works aim at transforming a given set of descriptors so that the optimal computed map satisfies some desired criteria during training. This transformation can be learned with a supervised loss, as in , as well as with an unsupervised loss as in the more recent works of .More specifically, the FMNet approach proposed in  assumes to have as input, a set of shape pairs for which ground truth point-wise maps are known, and aims to solve the following problem:Here, adopting the notation from  T is a non-linear transformation, in the form of a neural network, to be applied to some input descriptor functions D, Train is the set of training pairs for which ground truth correspondence GT (S1,S2) is known, l F is the soft error loss, which penalizes the deviation of the computed functional map C opt , after converting it to a soft map Sof t(C opt ) from the ground truth correspondence, and A T (D1) denotes the transformed descriptors D 1 written in the basis of shape 1. Thus, the FMNet framework  learns a transformation T of descriptors T (D 1 ), T (D 2 ) based on a supervised loss that minimizes the discrepancy between the resulting soft map and the known ground truth correspondence.A related recent approach, SURFMNet  follows a similar strategy but replaces l F with an unsupervised loss that enforces the desired structural properties on the resulting map, such as its bijectivity, orthonormality and commutativity with the Laplacian.3D-CODED In contrast to the the methods described above that primarily operate in the spectral domain, there are also some approaches that never leave the spatial domain. With the recent works on point clouds neural networks, pioneered by PointNet , and significantly extended by , to name a few, it is now possible to learn 3D features directly from point clouds. 3D-CODED  is based on this approach, as it is a method built on a variational auto-encoder with a PointNet architecture for the encoder. Their method relies on a template that is supposed to be deformable in a non-rigid but isometric way to any of the shape of the datasets. It is a supervised method, and requires the knowledge of all ground-truth correspondences between any shape of the dataset and the deformable template. 3D-CODED is trained on 230K shapes, introduced in SURREAL , and generated with SMPL .Motivation The two main classes of existing approaches have their associated benefits and drawbacks. On the one hand, spectral methods are able to use small matrices instead of all the points of the shape, and operate on intrinsic properties of the 3D surfaces, making them resilient to a change in pose, and allowing them to train on really small datasets. However, due to their use of input descriptors (typically SHOT ), they tend to overfit to the connectivity of the training set, which can lead to catastrophic results even in apparently simple cases. On the other hand, 3D-CODED shows extreme efficiency when trained on enough data, regardless of the connectivity, but with a small dataset, it is prone to overfitting and fails to generalize the training poses to predict the different poses of the test set.Our method is a mix of the two approaches, and, as we show below, can obtain accurate results with little training data leading to state-of-the-art accuracy on a challenging recent benchmark of human shapes in different poses and with different connectivity .In this paper, we introduce a novel approach to learning descriptors on shapes in order to get correspondences through the functional map framework. Our method is composed of two main parts, labeled as Feat and FMReg in  2. The first aims at optimizing point cloud convolutional filters  to extract features from the raw geometry of the shapes. These filters are learned using a Siamese network on the source and a target shapes by using shared learnable parameters \u0398, in a similar way as in . However, unlike that approach and follow-up works  we learn the features directly from the geometry of the shapes rather than computing a transformation of some pre-defined existing descriptors. These learned descriptors are projected in the spectral bases of the shapes and fed to the second block of the method, which uses them in a novel regularized functional map estimation layer. Finally, we use a spectral loss, based on the difference between the computed and the ground truth functional maps. This makes our approach very efficient as it operates purely in the spectral domain, avoiding expensive geodesic distance matrix computations as in  and moreover allows us to handle functional or soft ground truth input maps without requiring the training shapes to have the same number of points or fixed mesh connectivity.We stress again that the two components: learning features directly from the shapes and using the functional map representation both play a crucial role in our setup. The former allows us to learn robust and informative features independently from the mesh structure, while the latter allows us to strongly regularize correspondence learning, resulting in a method that generalizes even in the presence of a relatively small training set.The novelty of our architecture lies in its hybrid character. The first part, which we will refer to as the feature extractor in the following, aims at computing point-wise features on the input shapes. It corresponds to the Feat block in , and takes as input only the point clouds making it robust towards changes in connectivity.The purpose of the second part is to recover robust functional maps using these learned features. This block is built according to the pipeline of , first taking the features to the spectral domain over the two shapes (which corresponds to the dot products blocks after the Feat blocks in ), and then computing the map by minimizing an energy. However, since our method is based on a neural network, this operation should be differentiable with respect to the features over the shapes for the back-propagation algorithm to work. We extend the previously proposed functional map  . Overview of our approach: given a pair of shapes, we optimize for a point cloud convolutional model to get point-wise features for each shape, that we convert to a functional map using our FMReg block. The loss that we put forward penalizes maps according to their distance to the ground-truth map between the two shapes.layers  to also incorporate a differentiable regularizer, which results in the very robust map extraction, represented as FMReg in .The goal of this block is to learn functional characterizations of point clouds that will later be used to compute spectral descriptors and then functional maps. To this end, this network must be applied with the same weights to the source and target shapes, as represented in , and must result in informative descriptors, extracted from the point clouds of the two shapes.For this part, we chose the state of the art point cloud learning method KPConv , by extending the segmentation network proposed in that work. Our feature extractor is thus a Siamese version of the segmentation network described in KPConv, which we review for completeness in the supplementary materials.This block provides a novel fully differentiable way to compute a robust functional map from potentially low dimensional spectral descriptors.The main goal is, as in Section 3, to recover the groundtruth bijection between M and N , on which we have the computed raw-data features F and G.For this we first express the computed feature functions in the respective spectral basis, which we denote by \u03a6 M and \u03a6 N . This leads to the spectral descriptors A = (\u03a6 M ) \u2020 F and B = (\u03a6 M ) \u2020 G, with \u03a6 \u2020 the Moore pseudoinverse of \u03a6. We stress again that this step is where we shift focus from the spatial to the spectral domain, and corresponds to the dot product blocks in .In the pipeline first introduced in  and then widely used in the follow-up works , the standard strategy is to compute the functional map C that optimizes the following energy:where \u03bb is a scalar regularization parameter.Remark that the optimization problem in Eq. (4.4 ) is quadratic in terms of C and can be solved e.g. via standard convex optimization techniques. However, in the learning context, we need to differentiate the solution with respect to the spectral features A, B, which is challenging when C is computed via an iterative solver. Alternatively, the problem in Eq. can be written directly in terms of a large least squares system, by vectorizing the matrix C as was suggested in . However, for a k \u00d7 k functional map, this leads to a system of size k 2 \u00d7 k 2 which becomes prohibitive even for moderate values of k. To avoid these issues, previous learning-based approaches based on functional maps  have only optimized for C using the first part of the energy in Eq. (4.4 ): CA \u2212 B2 . This results in a simple linear system for which the derivatives can be computed in closed form. This has two major limitations, however: first the linear system is only invertible if there are at least k linearly independent feature functions. This condition can easily be violated in practice, especially in the early stages of learning, potentially resulting in a fatal error. Furthermore, the lack of regularization makes the solved-for functional map very sensitive to inconsistencies in the computed descriptors, which leads to an overall loss of robustness.In our work we address this problem by using the full energy in Eq. (4.4 ) in a fully differentiable way. In particular, we use the fact that the operators \u2206 M and \u2206 N are diagonal when expressed in their own eigen-basis.Indeed we remark that the gradient of the energy in Eq. (4.4 ) vanishes whenever CAA T +\u03bb\u2206\u2022C = BA T , where the operation \u2022 represents the element-wise multiplication, and, where \u00b5 M l and \u00b5 N l respectively correspond to the eigenvalues of \u2206 M and \u2206 N . It is then easy to see that this amounts to a separate linear system for every row c i of C :where b i stands for i th row of B. In total, if k is the number of eigenvectors used for representing the functional map, this operation amounts to inverting k different k \u00d7 k matrices. Since inverting a linear system is a differentiable operation, which is already implemented e.g. in TensorFlow, this allows us to estimate the functional map in a robust way, while preserving differentiability.Our method also uses a loss with respect to the ground truth functional map in the spectral domain. This is similar to the energy used in , but is different from the loss of the original FMNet work , which converted a functional map to a soft correspondence matrix and imposed a loss with respect to the ground truth point-wise map, relying on expensive geodesic distance matrix computation.Specifically, calling C the functional map obtained by the FMap block, and C gt the ground truth spectral map, our loss is defined as:As mentioned above, we use a Frbenius norm to compute the distance between matrices.It is important to note that whenever a pointwise ground truth map is given it is trivial to convert it to the functional map representation. Conversely, the ground truth spectral map is more general than the point-wise ground truth correspondence. Indeed, with just a few precise landmarks one can recover a functional map accurate enough to make this loss efficient, for instance through the original pipeline of , but also with more recent follow-up works, such as  or , which we will further describe as baselines to our method in Section 5. This is useful, e.g., in the case of re-meshed datasets. Indeed, complete ground truth correspondences between two shapes of these datasets are not fully known. One can only have access to the (often partial and not bijective) ground truth pointwise map from a template mesh T to each remeshed shape S i . Ecah such map can be converted to a functional map C i and a very good approximation of the spectral ground truth C gt i\u2192j between S i and S j can be expressed as C \u2020 j C i .Once our model is trained, we can then test it on a pair of shapes and get a functional map between these shapes. This map can either directly be converted to a point to point map, or refined further. We use a very recent and efficient refining algorithm, called ZoomOut  based on navigating between spatial and spectral domains while progressively inceasing the number of spectral basis functions. This efficient postprocessing technique allows us to get state-of-theart results, as described in Section 5.We implemented our method in TensorFlow  by adapting the open-source implementation of SURFMNet  and KPConv .Our feature extraction network is based on a residual convolutional architecture of , consisting of 4 convolutional blocks with leaky linear units, with successive poolings and dimension augmentation from 128 to 2048, followed by a 4 up-sampling blocks with shortcuts from corresponding pooling layers, and dimension reduction from 2048 back to 128. Please see the Supplementary materials, part A, in  for more details. Following the pipeline of KPConv, we start with a sub-sampled version of our point clouds with a grid subsampling of step 0.03. The pooling layers are therefore obtained with grid samplings of parameters 0.06, 0.12, 0.24 and 0.48.Similarly to FMNet  and SURFMNet , our network is applied in a Siamese way on the two shapes, using the same learned weights for the feature extractor.In the case of fully automatic spectral methods such as BCICP  and ZoomOut , or the deep learning based FMNet  (supervised or unsupervised) and SURFMnet , all results are invariant by any rigid transformation of the input shapes. However, in the case of methods using the 3D coordinates of the points to generate information about the input shape, this does not remain true. Consequently, both 3D-CODED  and our method avoid this dependency through data augmentation to be as close as possible to the generality of fully spectral methods. To that end, assuming the shapes are all aligned on one axis (e.g. on human the natural up axis), both 3D-CODED and our method perform data augmentation by randomly rotating the input shapes along that axis.In addition to the architecture above, our method has two key hyper-parameters: the size of the functional basis and the regularizer \u03bb in Equation 5. For the size of the basis, we discovered if this number is too high, for instance, with 120 eigenvectors as in FMNet and SURFMNet, it can easily lead to overfitting. However, by reducing this number to 30, the results of SURFMNet on FAUST re-meshed (here reported in ) go from 0.15 to 4.5. As a consequence, we choose the number of eigenvectors to be 30 in all of our experiments on our method. Regarding the weight \u03bb in Equation , we observed that setting it to \u03bb = 10 \u22123 helps getting good results while drastically reducing the number of training steps, as pointed out in the ablation study. We use this value throughout all experiments.We train our network with a batch size of 4 shape pairs for a number of epochs depending on the number of shapes in the dataset. We use a learning rate of .001 and gradually decreasing it to 0.0001 with ADAM optimizer .We test our method on a wide spectrum of human datasets: first, the re-meshed versions of FAUST dataset  containing 100 human shapes in 1-1 correspondence, and of SCAPE , made publicly available by Ren et al. . These re-meshed datasets offer significantly more variability in terms of shape structures and connectivity, including for instance point sampling density, making them harder to match for existing algorithms. We also highlight that the SCAPE dataset is slightly more challenging since the shapes are less regular, and two shapes never share the same pose. This is not true for FAUST, wherein all the poses present in the test set also exist in the training set, with the variation coming from body type only, making the pose recovery easier at test time. We also use the re-meshed version of the more recent SHREC'19 dataset , which, in theory, is the most challenging of the test sets, because of stronger distortions in the poses, the presence of an incomplete shape, and the number of test pairs (430 in total, so two times the number of test pairs of FAUST or SCAPE). At last, we also use the generic training dataset of 3D-CODED , originally consisting in 230K synthetic shapes generated using Surreal , with the parametric model SMPL introduced in . We use it only for training purposes in our second experiment, to show that our method can generalize well to changes in connectivity, being able to train on a synthetic, very smooth, identical triangulation for the whole training set, and still produce results of excellent quality on re-meshed datasets.Our method is built with a number of building blocks, all of which we consider essential to achieve optimal performance. To illustrate this, in the supplementary materials we provide an extensive ablation study of all the key components of our algorithm.We compare our method to several state of the art methods: the first category includes fully automatic methods without any learning component . These methods are simply evaluated on the test sets without any training. The second category includes FMNet  and its unsupervised versions, referred to as Unsup FMNet  and SURFMNet , with and without post-processing (PMF  . Comparison with 3D-CODED while varying training size of SURREAL dataset and simultaneously testing on other datasets.  for FMNet, and standard functional map refinement , referred to as ICP, for SURFMNet). All these variants of FMNet give similar results, but SURFMNet is the only one to train within a few hours, without requiring too much space. This is due to the fact SURFMNet only operates in the spectral domain, in contrast to other methods. Lastly, we compare to the supervised 3D-CODED , described earlier in more details in Section 3. For conciseness, we refer to our method as Ours in the following text. We show our results with and without ZoomOut  refinement, referred to as ZO, in order to prove that our method stands out even without post processing. We compare these different methods in two main settings named Experiment 1 and Experiment 2 below. Experiment 1 consists of evaluating the different methods in the following setting: we split FAUST re-meshed and SCAPE re-meshed into training and test sets containing 80 and 20 shapes for FAUST, and 51 and 20 shapes for SCAPE. We obtain results for training and testing on the same dataset, but also by testing on a different dataset. For instance, by training on SCAPE re-meshed train set and testing on FAUST re-meshed test set. This experiment aims at testing the generalization power of all methods to small re-meshed datasets, as well as its ability to adapt to a different dataset at test time.Experiment 2 consists of sampling 100, 500, 2000, and 5000 shapes from the SURREAL dataset to be used for training. We then test the trained models on the test sets of FAUST re-meshed, SCAPE re-meshed, and SHREC19 remeshed. This experiment aims at testing the robustness and generalization power of the different methods in the presence of varying amounts of training data, as well as adaptability to train on a perfect synthetic triangulations and still get results on challenging re-meshed shapes.To evaluate the results, we use the protocol introduced in , where the per-point-average geodesic distance between the ground truth map and the computed map is re-ported. All results are multiplied by 100 for the sake of readability.As we can see in , our method performs the best overall on Experiment 1. Fully automatic methods do not provide competitive results compared to the learning methods (except on crossed settings because they did not train on anything and are thus not influenced by the training shapes). As reported in the Section 3.1 , this highlights that handcrafted features can easily fail. It is noticeable that spectral methods (FMNet variations, and Ours as a hybrid method) get reasonable, or even good results in our case, with these small datasets. In comparison, 3D-CODED seems to fail in almost all cases. It is remarkable that it can learn on such a small dataset as the training set of FAUST re-meshed. One explanation for that is that FAUST contains the same set of poses in the test set as in the train set.Contrary to other baselines, our method gives good results on all settings, even without refinement, showing good resilience to a really low number of shapes, even with remeshed geometry. We would like to stress that no other method is able to achieve such a generalization with this low number of shapes.For a fair comparison with 3D-CODED, we complete our study with a second experiment, in which the training set is now made of the same shapes 3D-CODED uses for training in their paper, namely SURREAL dataset. The aim of this experiment is to further showcase the generalization power of our method when compared to 3D-CODED. First, by training on a very smooth synthetic dataset, on which previous fully spectral methods tend to easily overfit due to the obvious mismatch in triangulation in training and test set. Our second goal is to observe the dependence of different methods on size of the training set.We report the results (multiplied by 100) of 3D-CODED and Our method in , as they are the only two competitive algorithms in Experiment 2. These results once again demonstrate that our method can achieve impressive results even with a low number of training shapes. On SHREC re-meshed, we achieve state of the art results with an average error of 0.048 with only 500 training shapes. We  It can be observed in  that our results are consistent and unaltered even with the drop in number of training shapes. 3D-CODED, on the other hand, always suffers from a reduced training set.In  we show the results of our method (with and without ZoomOut refinement ), 3D-CODED , FM-Net  (with and without PMF refinement ), trained on respectively 2000 and 100 shapes, as presented in Experiment 2, via texture transfer.With 2000 training shapes, both our method and 3D-CODED lead to good or even excellent texture transfers, while fully spectral methods fail due to the change of connectivity from training to test set. However, with only 100 training shapes, 3D-CODED fails to get a good reconstruction in many cases, leading to bad texture transfer as in . This highlights the fact that our method performs better than any other existing method when only a few training shapes are provided.We presented a method for improving the robustness and reducing overfitting in learning shape correspondences. Key to our approach is a hybrid network structure, made of a raw-data feature extractor that learns descriptors on a pair of shapes, and a novel robust functional map layer. Our network can thus operate in both the spectral and the spatial domain, thus taking advantages of both representations.Our approach has several limitations: first, as a supervised method it requires at least partial correspondences (as discussed in Section 4.5 ) between the training shapes. Also, it requires data augmentation to be able to predict non-aligned shapes, which can be costly and unstable.In the future, we plan to work towards an unsupervised spectral loss, similar in spirit to SURFMNet , while avoiding the symmetry ambiguity problem. We also plan to try other, invariant feature extractors such as , or  to avoid data augmentation.", "videoStruct": [{"timeStart": "00-00-00", "timeEnd": "00-00-06", "sentence": "hey everyone and welcome to this talk on how to perform robust feature learning for shaped respondent"}, {"timeStart": "00-00-06", "timeEnd": "00-00-10", "sentence": "this is joint work with max a journey girlfriend abishek Sharma from equal pretty technique in France"}, {"timeStart": "00-00-10", "timeEnd": "00-00-12", "sentence": "let's state algol"}, {"timeStart": "00-00-12", "timeEnd": "00-00-18", "sentence": "even a pair of three d shapes shaped matching ends at finding mother points on the first shape good on the second shape"}, {"timeStart": "00-00-18", "timeEnd": "00-00-23", "sentence": "we want to compute a natural map also called correspondence between these two shapes"}, {"timeStart": "00-00-23", "timeEnd": "00-00-25", "sentence": "this problem is fundamental"}, {"timeStart": "00-00-25", "timeEnd": "00-00-31", "sentence": "as high quality maps can be used to transfer information such as texture animation"}, {"timeStart": "00-00-31", "timeEnd": "00-00-33", "sentence": "which has many practical applications"}, {"timeStart": "00-00-33", "timeEnd": "00-00-36", "sentence": "matching points directly is a really hard problem"}, {"timeStart": "00-00-36", "timeEnd": "00-00-39", "sentence": "they can be tackled using the function map framework"}, {"timeStart": "00-00-39", "timeEnd": "00-00-42", "sentence": "the key idea is to much functions although the shapes"}, {"timeStart": "00-00-42", "timeEnd": "00-00-44", "sentence": "rather than individual points"}, {"timeStart": "00-00-44", "timeEnd": "00-00-46", "sentence": "this encodes the mapping"}, {"timeStart": "00-00-46", "timeEnd": "00-00-48", "sentence": "with a linear operator t f"}, {"timeStart": "00-00-48", "timeEnd": "00-00-50", "sentence": "what is great about linear operators"}, {"timeStart": "00-00-50", "timeEnd": "00-00-53", "sentence": "is that they can be written in a basis"}, {"timeStart": "00-00-53", "timeEnd": "00-00-59", "sentence": "we use the classical applause beltrami eigen basis in which we can write any function on their shape as shown here"}, {"timeStart": "00-00-59", "timeEnd": "00-01-02", "sentence": "typically to estimate a functional map in practice"}, {"timeStart": "00-01-02", "timeEnd": "00-01-07", "sentence": "weaver's computer ridge is set on this basis functions in which our operator becomes a small matrix"}, {"timeStart": "00-01-07", "timeEnd": "00-01-09", "sentence": "now the whole pipeline"}, {"timeStart": "00-01-09", "timeEnd": "00-01-17", "sentence": "even a set of functions known to be preserved by the map we express them in the corresponding Laplace basis and formulating optimization problem based"}, {"timeStart": "00-01-17", "timeEnd": "00-01-20", "sentence": "and the preservation of these bases of this function sorry"}, {"timeStart": "00-01-20", "timeEnd": "00-01-24", "sentence": "we then convert the computed function map back to appoint wise map"}, {"timeStart": "00-01-25", "timeEnd": "00-01-27", "sentence": "resume to no functions"}, {"timeStart": "00-01-27", "timeEnd": "00-01-29", "sentence": "naturally preserved by our map"}, {"timeStart": "00-01-29", "timeEnd": "00-01-31", "sentence": "this functions are called descriptive"}, {"timeStart": "00-01-32", "timeEnd": "00-01-34", "sentence": "but the fundamental problem is"}, {"timeStart": "00-01-34", "timeEnd": "00-01-36", "sentence": "how do we get to this good Des scriptures"}, {"timeStart": "00-01-37", "timeEnd": "00-01-38", "sentence": "recent approaches"}, {"timeStart": "00-01-38", "timeEnd": "00-01-44", "sentence": "pounded by FM nets and network proposed by little it's all in two thousand and seventeen"}, {"timeStart": "00-01-44", "timeEnd": "00-01-47", "sentence": "tackled his problem by learning to Des scriptures"}, {"timeStart": "00-01-47", "timeEnd": "00-01-50", "sentence": "which are the news with the pipeline described earlier to computer map"}, {"timeStart": "00-01-50", "timeEnd": "00-01-52", "sentence": "on which a loss is defined"}, {"timeStart": "00-01-52", "timeEnd": "00-02-00", "sentence": "some follow up works extend this approach but are always highly dependent on some input descriptive such as shots"}, {"timeStart": "00-02-00", "timeEnd": "00-02-05", "sentence": "this constraint these methods to have similar connectivity a train and test time"}, {"timeStart": "00-02-05", "timeEnd": "00-02-07", "sentence": "another different idea"}, {"timeStart": "00-02-18", "timeEnd": "00-02-20", "sentence": "this usually requires a lot of data"}, {"timeStart": "00-02-21", "timeEnd": "00-02-24", "sentence": "I'll method stands between these two ideas"}, {"timeStart": "00-02-24", "timeEnd": "00-02-29", "sentence": "we used the functional maps but we compute descriptive directly from the point clouds"}, {"timeStart": "00-02-29", "timeEnd": "00-02-32", "sentence": "with the same basic structure as if Annette as shown here"}, {"timeStart": "00-02-32", "timeEnd": "00-02-35", "sentence": "the features are learned jointly in the two shapes"}, {"timeStart": "00-02-35", "timeEnd": "00-02-37", "sentence": "they are handed out to the function by pipeline"}, {"timeStart": "00-02-37", "timeEnd": "00-02-42", "sentence": "within computer loss on the resulting map to help the network compute the optimal descriptive"}, {"timeStart": "00-02-43", "timeEnd": "00-02-45", "sentence": "offers contribution is"}, {"timeStart": "00-02-45", "timeEnd": "00-02-47", "sentence": "do you use at a punked out feature extractor"}, {"timeStart": "00-02-47", "timeEnd": "00-02-51", "sentence": "which was the state of the art Byrne cloud based dip norm at work"}, {"timeStart": "00-02-51", "timeEnd": "00-02-52", "sentence": "and its segmentation version"}, {"timeStart": "00-02-52", "timeEnd": "00-02-55", "sentence": "and extract the features of the last layer"}, {"timeStart": "00-02-56", "timeEnd": "00-03-00", "sentence": "he's raw features are then projected into the respective virtual basis"}, {"timeStart": "00-03-00", "timeEnd": "00-03-02", "sentence": "making them ready for my computation"}, {"timeStart": "00-03-03", "timeEnd": "00-03-04", "sentence": "in FM net"}, {"timeStart": "00-03-04", "timeEnd": "00-03-09", "sentence": "the map is computed by minimizing the difference between maths target and saw the scriptures"}, {"timeStart": "00-03-09", "timeEnd": "00-03-15", "sentence": "as a contribution is that it can be made more robust using a wellknown additional constraint"}, {"timeStart": "00-03-15", "timeEnd": "00-03-17", "sentence": "the LA blash in regular riser"}, {"timeStart": "00-03-17", "timeEnd": "00-03-20", "sentence": "thus rather than simply using the descriptive"}, {"timeStart": "00-03-20", "timeEnd": "00-03-21", "sentence": "two computer function map"}, {"timeStart": "00-03-21", "timeEnd": "00-03-26", "sentence": "we introduced a regularized f map layer that is both differential and efficient"}, {"timeStart": "00-03-26", "timeEnd": "00-03-31", "sentence": "our third contribution is the Los were simply compare the map obtained by the network"}, {"timeStart": "00-03-31", "timeEnd": "00-03-32", "sentence": "underground truth map"}, {"timeStart": "00-03-32", "timeEnd": "00-03-34", "sentence": "a method is deh supervised"}, {"timeStart": "00-03-34", "timeEnd": "00-03-38", "sentence": "but it's easy to train and synthetic shapes on which ground truth maps on own"}, {"timeStart": "00-03-39", "timeEnd": "00-03-44", "sentence": "within performing last small post processing step by using the recent zoom algorithm"}, {"timeStart": "00-03-44", "timeEnd": "00-03-48", "sentence": "to summarize we use a hybrid approach"}, {"timeStart": "00-03-48", "timeEnd": "00-03-52", "sentence": "it has both the advantages of spectral and PRN cloud based methods"}, {"timeStart": "00-03-53", "timeEnd": "00-03-58", "sentence": "can generalize very well with very little data without depending on connectivity"}, {"timeStart": "00-03-59", "timeEnd": "00-04-08", "sentence": "I'm sorry here we exhibit some Des scriptures produced by our method as you can see these functions show strong resilience to an rigid transformation"}, {"timeStart": "00-04-08", "timeEnd": "00-04-11", "sentence": "now some good results with texture transfer"}, {"timeStart": "00-04-11", "timeEnd": "00-04-14", "sentence": "in a setting with different connectivity from trane to test"}, {"timeStart": "00-04-14", "timeEnd": "00-04-17", "sentence": "as expected only our method and three decoded"}, {"timeStart": "00-04-17", "timeEnd": "00-04-19", "sentence": "give good results"}, {"timeStart": "00-04-19", "timeEnd": "00-04-26", "sentence": "we also highlight though that with a low number of training shapes three decoded can give false predictions"}, {"timeStart": "00-04-26", "timeEnd": "00-04-28", "sentence": "here you can see that map is flipped"}, {"timeStart": "00-04-28", "timeEnd": "00-04-30", "sentence": "when looking at the numbers"}, {"timeStart": "00-04-30", "timeEnd": "00-04-36", "sentence": "therefore we see that for three d coded the error actually explodes with a low number of training shapes"}, {"timeStart": "00-04-37", "timeEnd": "00-04-39", "sentence": "which is not true for our method"}, {"timeStart": "00-04-39", "timeEnd": "00-04-42", "sentence": "which is better and more robust in this case"}, {"timeStart": "00-04-44", "timeEnd": "00-04-48", "sentence": "a shape matching method that learns descript his directly from point clouds"}, {"timeStart": "00-04-48", "timeEnd": "00-04-50", "sentence": "with a regularized function that player"}, {"timeStart": "00-04-50", "timeEnd": "00-04-53", "sentence": "allows our approach to generalize very well"}, {"timeStart": "00-04-53", "timeEnd": "00-04-55", "sentence": "with very little training data"}, {"timeStart": "00-04-55", "timeEnd": "00-04-58", "sentence": "source code is available on this drink thank you very much for us"}]}, {"title": "Transform and Tell: Entity-Aware News Image Captioning", "authors": "Alasdair Tran", "abstract": "", "publicationOrg": "CVPR", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2004.08070.pdf", "pdfPath": "/data/cache/2/PDFs/TransformandTellEntityAwareNewsImageCaptioning.pdf", "publicationUrl": "https://arxiv.org/pdf/2004.08070.pdf", "codeUrl": "https://github.com/alasdairtran/transform-and-tell", "datasetUrl": "", "videoUrl": "https://stream.crossminds.ai/5fa9ea8819ee2d16769c8460-1604971157228/hls-5fa9ea8819ee2d16769c8460.m3u8", "videoPath": "", "pdfText": "The Internet is home to a large number of images, many of which lack useful captions. While a growing body of work has developed the capacity to narrate the contents of generic images , these techniques still have two important weaknesses. The first weakness is in world knowledge. Most captioning systems are aware of generic object categories but unaware of names and places. Also generated captions are often inconsistent with commonsense knowledge. The second weakness is in linguistic expressiveness. The community has observed that generated captions tend to be shorter and less diverse than human-written captions . Most captioning systems rely on a fixed vocabulary and cannot correctly place or spell new or rare words.News image captioning is an interesting case study for tackling these two challenges. Not only do news captionsThe United States' Alex Morgan, center, scored the first goal in the match against Thailand. describe specific people, organizations and places, but the associated news articles also provide rich contextual information. The language used in news is evolving, with both the vocabulary and style changing over time. Thus news captioning approaches need to adapt to new words and concepts that emerge over a longer period of time (e.g. walkman in the 1990s or mp3 player in the 2000s). Existing approaches  rely on text extraction or template filling, which prevents the results from being linguistically richer than the template generator and are error-prone due to the difficulty in ranking entities for gap filling. Successful strategies for news image captioning can be generalized to images from domains with other types of rich context, such as web pages, social media posts, and user comments.We propose an end-to-end model for news image captioning with a novel combination of sequence-to-sequence neural networks, language representation learning, and vision subsystems. In particular, we address the knowledge gap by computing multi-head attention on the words in the article, along with faces and objects that are extracted from the image. We address the linguistic gap with a flexible byte-pair-encoding that can generate unseen words. We use dynamic convolutions and mix different linguistic representation layers to make the neural network representation richer. We also propose a new dataset, NYTimes800k, that is 70% larger than GoodNews  and has higherquality articles along with additional image location information. We observe a performance gain of 6.8\u00d7 in BLEU-4 (0.89 \u2192 6.05) and 4.1\u00d7 in CIDEr (13.1 \u2192 53.8) compared to previous work . On both datasets we observe consistent gains for each new component in our language, vision, and knowledge-aware system. We also find that our model generates names not seen during training, resulting in linguistically richer captions, which are closer in length (mean 15 words) to the ground truth (mean 18 words) than the previous state of the art (mean 10 words).Our main contributions include:1. A new captioning model that incorporates transformers, an attention-centric language model, byte-pair encoding, and attention over four different modalities (text, images, faces, and objects).2. Significant performance gains over all metrics, with associated ablation studies quantifying the contributions of our main modeling components using BLEU-4, CIDEr, precision & recall of named entities and rare proper nouns, and linguistic quality metrics.3. NYTimes800k, the largest news image captioning dataset to date, containing 445K articles and 793K images with captions from The New York Times spanning 14 years. NYTimes800k builds and improves upon the recently proposed GoodNews dataset . It has 70% more articles and includes image locations within the article text. The dataset, code, and pretrained models are available on GitHub 1 .A popular design choice for image captioning systems involves using a convolutional neural network (CNN) as the image encoder and a recurrent neural network (RNN) with a closed vocabulary as a decoder . Attention over image patches using a multilayer perception was introduced in \"Show, Attend and Tell\" . Further extensions include having the option to not attend to any image region  using a bottom-up approach to propose a region to attend to , and attending specifically to object regions  and visual concepts  identified in the image.News image captioning includes the article text as input and focuses on the types of images used in news articles. A key challenge here is to generate correct entity names, especially rare ones. Existing approaches include extractive methods that use n-gram models to combine existing phrases  or simply retrieving the most representative  https://github.com/alasdairtran/transform-and-tell sentence  in the article. Ramisa et al.  built an endto-end LSTM decoder that takes both the article and image as inputs, but the model was still unable to produce names that were not seen during training.To overcome the limitation of a fixed-size vocabulary, template-based methods have been proposed. An LSTM first generates a template sentence with placeholders for named entities, e.g. \"PERSON speaks at BUILDING in DATE.\" . Afterwards the best candidate for each placeholder is chosen via a knowledge graph of entity combinations , or via sentence similarity . One key difference between our proposed model and previous approaches  is that our model can generate a caption with named entities directly without using an intermediate template.One tool that has seen recent successes in many natural language processing tasks are transformer networks. Transformers have been shown to consistently outperform RNNs in language modeling , story generation , summarization , and machine translation . In particular, transformer-based models such as BERT , XLM , XLNet , RoBERTa , and ALBERT  are able to produce high level text representations suitable for transfer learning. Furthermore, using byte-pair encoding (BPE)  to represent uncommon words as a sequence of subword units enables transformers to function in an open vocabulary setting. To date the only image captioning work that uses BPE is , but they did not use it for rare named entities as these were removed during pre-processing. In contrast we explicitly examine BPE for generating rare names and compare it to template-based methods.Transformers have been shown to yield competitive results in generating generic MS COCO captions . Zhao et al.  have gone further and trained transformers to produce some named entities in the Conceptual Captions dataset . However, the authors used web-entity labels, extracted using Google Cloud Vision API, as inputs to the model. In our work, we do not explicitly give the model a list of entities to appear in the caption. Instead our model automatically identifies relevant entities from the provided news article.Our model consists of a set of pretrained encoders and a decoder, as illustrated in . The encoders (Section 3.1) generate high-level vector representations of the images, faces, objects, and article text. The decoder (Section 3.2) attends over these representations to generate a caption at the sub-word level.  ageNet. We use the output of the final block before the pooling layer as the image representation. This is a set of 49 different vectors x I i \u2208 R 2048 where each vector corresponds to a separate image patch after the image is divided into equally-sized 7 by 7 patches. This gives us the set, where D I = 2048 and M I = 49 for ResNet-152. Using this representation allows the decoder to attend to different regions of the image, which is known to improve performance in other image captioning tasks  and has been widely adopted. Face Encoder: We use MTCNN  to detect face bounding boxes in the image. We then select up to four faces since the majority of the captions contain at most four people's names (see Section 4). A vector representation of each face is obtained by passing the bounding boxes to FaceNet , which was pre-trained on the VGGFace2 dataset . The resulting set of face vectors for each image is, where D F = 512 for FaceNet and M F is the number of faces. If there are no faces in the image, X F is an empty set.Even though the faces are extracted from the image, it is useful to consider them as a separate input domain. This is because a specialized face embedding model is tuned for identifying people and thus can help the decoder to generate more accurate named entities. Object Encoder: We use YOLOv3  to detect object bounding boxes in the image. We filter out objects with a confidence less than 0.3 and select up to 64 objects with the highest confidence scores to feed through a ResNet-152 pretrained on ImageNet. In contrast to the image encoder, we take the output after the pooling layer as the representation for each object. This gives us a set of object vectors, where D O = 2048 for ResNet-152 and M O is the number of objects. Article Encoder: To encode the article text we use RoBERTa , a recent improvement over the popular BERT  model. RoBERTa is a pretrained language representation model providing contextual embeddings for text. It consists of 24 layers of bidirectional transformer blocks.Unlike GloVe  and word2vec  embeddings, where each word has exactly one representation, the bidirectionality and the attention mechanism in the transformer allow a word to have different vector representations depending on the surrounding context. The largest GloVe model has a vocabulary size of 1.2 million. Although this is large, many rare names will still get mapped to the unknown token. In contrast, RoBERTa uses BPE  which can encode any word made from Unicode characters. In BPE, each word is first broken down into a sequence of bytes. Common byte sequences are then merged using a greedy algorithm. Following , our vocabulary consists of 50K most common byte sequences.Inspired by Tenney et al.  who showed that different layers in BERT represent different steps in the tradi-tional NLP pipeline, we mix the RoBERTa layers to obtain a richer representation. Given an input of length M T , the pretrained RoBERTa encoder will return 25 sequences of embeddings, G = {g i \u2208 R 2048 : \u2208 {0, 1, ..., 24}, i \u2208 {1, 2, ..., M T }}. This includes the initial uncontextualized embeddings and the output of each of the 24 transformer layers. We take a weighted sum across all layers to obtain the article embedding x A i :where \u03b1 are learnable weights. Thus our RoBERTa encoder produces the set of token embeddings, where D T = 1024 in RoBERTa.The decoder is a function that generates caption tokens sequentially. At time step t, it takes as input: the embedding of the token generated in the previous step, z 0t \u2208 R D E where D E is the hidden size; embeddings of all other previously generated tokens Z 0<t = {z 00 , z 01 , ..., z 0t\u22121 }; and the context embeddings X I , X A , X F , and X O from the encoders. These inputs are then fed through L transformer blocks:where z t is the output of the th transformer block at time step t. The final block's output z Lt is used to estimate p(y t ), the probability of generating the tth token in the vocabulary via adaptive softmax :By dividing the vocabulary into three clusters based on frequency-5K, 15K, and 30K-adaptive softmax makes training more efficient since most of the time, the decoder only needs to compute the softmax over the first cluster containing the 5,000 most common tokens. In the following two subsections, we will describe the transformer block in detail. In each block, the conditioning on past tokens is achieved using dynamic convolutions, and the conditioning on the contexts is achieved using multihead attention. Dynamic Convolutions: Introduced by Wu et al. , the goal of dynamic convolution is to provide a more efficient alternative to self-attention  when attending to past tokens. At block + 1 and time step t, we have the input. Given kernel size K and H attention heads, for each head h \u2208 {1, 2, ..., H}, we first project the current and last K \u2212 1 steps using a feedforward layer to obtain z hj \u2208 R D E /H :for j \u2208 {t \u2212 K + 1, t \u2212 K + 2, ..., t}. Here GLU is the gated linear unit activation function . The output of each head's dynamic convolution is the weighted sum of these projected values:zwhere the weight \u03b3 hj is a linear projection of the input (hence the term \"dynamic\"), followed by a softmax over the kernel window:The overall output is the concatenation of all the head outputs, followed by a feedforward with a residual connection and layer normalization , which does a z-score normalization across the feature dimension (instead of the batch dimension as in batch normalization ):The output d t can now be used to attend over the context embeddings.Multi-Head Attention: The multi-head attention mechanism  has been the standard method to attend over encoder outputs in transformers. In our setting, we need to attend over four context domains-images, text, faces, and objects. As an example, we will go over the image attention module, which consists of H heads. Each head h first does a linear projection of d t and the image embeddings X I into a query q I ht \u2208 R D E /H , a set of keys, and the corresponding valuesThen the attended image for each head is the weighted sum of the values, where the weights are obtained from the dot product between the query and key:xThe attention from each head is then concatenated intoand the overall image attentionx I t \u2208 R D E is obtained after adding a residual connection and layer normalization:We use the same multi-head attention mechanism (with different weight matrices) to obtain the attended articlex A t , facesx F t , and objectsx O t . These four are finally concatenated and fed through a feedforward layer:The final output z +1 t \u2208 R D E is used as the input to the next transformer block.We describe two datasets that contain news articles, images, and captions. The first dataset, GoodNews, was recently proposed in Biten et al. , while the second dataset, NYTimes800k, is our contribution. GoodNews: The GoodNews dataset was previously the largest dataset for news image captioning . Each example in the dataset is a triplet containing an article, an image, and a caption. Since only the article text, captions, and image URLs are publicly released, the images need to be downloaded from the original source. Out of the 466K image URLs provided by , we were able to download 463K images, or 99.2% of the original dataset-the remaining are broken links.We use this 99.2% sample of GoodNews and the trainvalidation-test split provided by . There are 421K training, 18K validation, and 23K test captions. Note that this split was performed at the level of captions, so it is possible for a training and test caption to share the same article text (since articles have multiple images).We observe several issues with GoodNews that may limit a system's ability to generate high-quality captions. Many of the articles in GoodNews are partially extracted because the generic article extraction library failed to recognize some of the HTML tags specific to The New York Times. Importantly, the missing text often included the first few paragraphs which frequently contain important information for captioning images. In addition GoodNews contains some non-English articles and captioned images from the recommendation sidebar which are not related to the main article.  We split the training, validation, and test sets according to time, as shown in . Compared to the random split used in GoodNews, splitting by time allows us to study the model performance on novel news events and new names, which might be important in a deployment scenario. Out of the 100K proper nouns in our test captions, 4% never appear in any training captions.This section describes settings for neural network learning, baselines and evaluation metrics, followed by a discussion of key results.Following Wu et al. , we set the hidden size D E to 1024; the number of heads H to 16; and the number of transformer blocks L to four with kernel sizes 3, 7, 15, and 31, respectively. For parameter optimization we use the adaptive gradient algorithm Adam  with the following parameter: \u03b2 1 = 0.9, \u03b2 2 = 0.98, = 10 \u22126 . We warm up the learning rate in the first 5% of the training steps to 10 \u22124 , and decay it linearly afterwards. We apply L 2 regularization to all network weights with a weight decay of 10 \u22125 and using the fix  that decouples the learning rate from the regularization parameter. We clip the gradient norm at 0.1. We use a maximum batch size of 16 and training is stopped after the model has seen 6.6 million examples. This is equivalent to 16 epochs on GoodNews and 9 epochs on NYTimes800k.The training pipeline is written in PyTorch  using the AllenNLP framework . The RoBERTa model and dynamic convolution code are adapted from fairseq . Training is done with mixed precision to reduce the memory footprint and allow our full model to be trained on a single GPU. The full model takes 5 days to train on one Titan V GPU and has 200 million trainable parameters-see the supplementary material for the size of each model variant. We use BLEU-4  and CIDEr  scores as they are standard for evaluating image captions. These are obtained using the COCO caption evaluation toolkit  . The supplementary material additionally reports BLEU-1, BLEU-2, BLEU-3, ROUGE , and METEOR . Note that CIDEr is particularly suited for evaluating news captioning models as it puts more weight than other metrics on uncommon words. In addition, we evaluate the precision and recall on named entities, people's names, and rare proper names. Named entities are identified in both the groundtruth captions and the generated captions using SpaCy. We then count exact string matches between the ground truths and generated entities. For people's names we restrict the set of named entities to those marked as PERSON by the SpaCy parser. Rare proper nouns are nouns that appear in a test caption but not in any training caption.We show two previous state-of-the-art models: Biten (Avg + CtxIns) and Biten (TBB + AttIns) . To provide a fair comparison we used the full caption results released by Biten et al.  and re-evaluated with our evaluation pipeline on a slightly smaller test set (a few test images are no longer available due to broken URLs). The final metrics are the same as originally reported if rounded to the nearest whole number.We evaluate a few key modeling choices: the decoder type (LSTM vs Transformer), the text encoder type (GloVe vs RoBERTa vs weighted RoBERTa), and the additional context domains (location-aware, face attention, and object attention). The location-aware models select the 512 tokens surrounding the image instead of the first 512 tokens of the article. Note that all our models use BPE in the decoder with adaptive softmax. We ensure that the total number of trainable parameters for each model is within 7% of one another (148 million to 159 million), with the exception of face attention (171 million) and object attention (200 million) since the latter two have extra multi-head attention modules. The results reported over GoodNews are based on a model trained solely on GoodNews, using the original random split of  for easier comparison to previous work. : Results on GoodNews (rows 1-10) and NYTimes800k (rows . We report BLEU-4, ROUGE, CIDEr, and precision (P) & recall (R) of named entities, people's names, and rare proper nouns. Precision and recall are expressed as percentages. Rows 1-2 contain previous state-of-the-art results . Rows 3-5 and 11-13 are ablation studies where we swap the Transformer with an LSTM and/or RoBERTa with GloVe. These models only have the image attention (IA).     summarizes evaluation metrics on GoodNews and NYTimes800k, while  compares generated captions from different model variants. Our full model (row 10) performs substantially better than the existing state of the art  across all evaluation metrics. On GoodNews, the full model yields a CIDEr score of 53.8, whereas the previous state of the art  achieved a CIDEr score of only 13.1.Our most basic LSTM model (row 3) differs from Biten et al.  in that we use BPE in the caption decoder instead of template generation and filling. The slight improvement in CIDEr (from 13.1 to 13.9) shows that BPE offers a competitive end-to-end alternative to the template filling method. This justifies the use of BPE in the remaining experiments.Models that encode articles using GloVe embeddings (rows 3-4 and 11-12) are unable to generate rare proper nouns, giving a precision and recall of 0. This is because the encoder skips words that are not part of the fixed GloVe vocabulary. This motivates the switch from GloVe to RoBERTa, which has an unbounded vocabulary. This switch shows a clear advantage in rare proper noun generation. On NYTimes800k, even the worst performing model that uses RoBERTa (row 13) achieves a precision of 7.47%, a recall of 9.50%, and a CIDEr gap of 12.8 points over the model without RoBERTa (row 11).Another important modeling choice is the functional form of the caption decoder. We find that the Transformer architecture provides a substantial improvement over the LSTM with respect to all evaluation metrics. For example, when we swap the LSTM with a Transformer (from row 13 to 15), the CIDEr score on NYTimes800k jumps from 24.9 to 40.3.Adding attention over faces improves both the recall and precision of people's names. It has no significant effect on other entity types (see the supplementary material forA nursery school teacher showing a bug to his class.Ms. Takato, who was born in Japan, was forced out of the day care program because she was pregnant.Ms. Takato with her son, Kishiko, and their children, from left, Kaiti, 3, and Kaitama, 3, at a day care center in Tokyo.Ms. Takato, with her son, Shiro, and son, at home in Tokyo. Ms. Takato, who was pregnant, said she was \"so frustrated and lost hope of being able to work.\" + location-aware A day care center in Tokyo.A child care center in Tokyo. The government is eager to bring more women into the work force, and is trying to come up with enough child care for mothers.A day care worker in Tokyo. The government is trying to bring more women into the work force, and the government is trying to come up with enough child care for mothers to go back to work.Japan Desperately Needs More Day Care Workers. New Mothers Need Not Apply.TOKYO -Ever since she was a young girl, all Erica Takato wanted to do was work with small children. A few weeks into her term, she requested time off for bed rest ordered by her doctor. .\u2026Union officials and former teachers cite a major obstacle to the aspirations: \u2026. The model with no access to the image makes a sensible but incorrect guess that the image is about Ms. Takato. Since the image appears in the middle of the article, only the location-aware models correctly state that the focus of the image is on a day care center.a detailed breakdown). Importantly, people's names are the most common entity type in news captions and so we also see an improvement in CIDEr. Attention over objects also improves performance on most metrics, especially on NYTimes800k. More broadly, this result suggests that introducing specialized vision models tuned to the common types of objects such as organizations (via logos or landmarks) is a promising future direction to improve the performance on news image captioning.The location-aware models (rows 17-19) focus the article context using the image location in the article, information which is only available in our NYTimes800k dataset. This simple focusing of context offers a big improvement to CIDEr, from 45.1 (row 16) to . This suggests a strong correspondence between an image and the closest text that can be easily exploited to generate better captions.The supplementary material additionally reports three caption quality metrics: caption length, type-token ratio (TTR) , and Flesch reading ease (FRE) . TTR is the ratio of the number of unique words to the total number of words in a caption. The FRE takes into account the number of words and syllables and produces a score between 0 and 100, where higher means being easier to read. As measured by FRE, captions generated by our model exhibit a level of language complexity that is closer to the ground truths. Additionally, captions generated by our model are 15 words long on average, which is closer to the groundtruths (18 words) than those generated by the previous state of the art (10 words) .In this paper, we have shown that by using a carefully selected novel combination of the latest techniques drawn from multiple sub-fields within machine learning, we are able to set a new SOTA for news image captioning. Our model can incorporate real-world knowledge about entities across different modalities and generate text with better linguistic diversity. The key modeling components are bytepair encoding that can output any word, contextualized embeddings for article text, specialized face & object encoding, and transformer-based caption generation. This result provides a promising step for other image description tasks with contextual knowledge, such as web pages, social media feeds, or medical documents. Promising future directions include specialized visual models for a broader set of entities like countries and organizations, extending the image context from the current article to recent or linked articles, or designing similar techniques for other image and text domains.", "videoStruct": []}, {"title": "Multimodal Future Localization and Emergence Prediction with a Reachability Prior - Short Intro", "authors": "makansio", "abstract": "", "publicationOrg": "CVPR", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2006.04700.pdf", "pdfPath": "/data/cache/2/PDFs/MultimodalFutureLocalizationandEmergencePredictionwithaReachabilityPriorShortIntro.pdf", "publicationUrl": "https://arxiv.org/pdf/2006.04700.pdf", "codeUrl": "https://github.com/lmb-freiburg/FLN-E...", "datasetUrl": "", "videoUrl": "https://stream.crossminds.ai/5fab37ce223291467a9aee6d-1605056477901/hls-5fab37ce223291467a9aee6d.m3u8", "videoPath": "", "pdfText": "shows the view of a driver approaching pedestrians who are crossing the street. To safely control the car, the driver must anticipate where these pedestrians will be in the next few seconds. Will the last pedestrian (in blue) have completely crossed the street when I arrive or must I slow down more? Will the pedestrian on the sidewalk (in orange) continue on the sidewalk or will it also cross the street?This important task comes with many challenges. First of all, the future is not fully predictable. There are typically multiple possible outcomes, some of them being more likely than others. The controller of a car must be aware of these multiple possibilities and their likelihoods. If a car crashes into a pedestrian who predictably crosses the street, this will be considered a severe failure, whereas extremely unlikely behaviour, such as the pedestrian in purple turning around and crossing the street in the opposite direction, : An example from the nuScenes dataset . Given the past observations of pedestrians (colored bounding boxes (top)) and the egomotion of the car (red arrow), our framework predicts multiple modes of their future visualized by a set of bounding boxes and their distribution as an overlaid heatmap. Prediction covers possible options for (2nd row) turning left/right, (3rd row) slowing down/accelerating, (4th row) being on the sidewalk. must be ignored to enable efficient control. The approach we propose predicts two likely modes for this pedestrian: continuing left or right on the sidewalk.Ideally this task can be accomplished directly in the sensor data without the requirement of privileged information such as a third person view, or a street map that marks all lanes, sidewalks, crossings, etc.. Independence of such information helps the approach generalize to situations not covered by maps or extra sensors, e.g., due to changes not yet captured in the map or GPS failures. However, mak-ing predictions in egocentric views suffers from partial visibility: we only see the context of the environment in the present view -other relevant parts of the environment are occluded and only become visible as the car moves.  shows that the effect of the egomotion is substantial even in this example with relatively slow motion.In this paper, we approach these two challenges in combination: multimodality of the future and egocentric vision. For the multimodality, we build upon the recent work by Makansi et al. , who proposed a technique to overcome mode collapse and stability issues of mixture density networks. However, the work of Makansi et al. assumes a static bird's-eye view of the scene. In order to carry the technical concept over to the egocentric view, we introduce an intermediate prediction which improves the quality of the multimodal distribution: a reachability prior. The reachability prior is learned from a large set of egocentric views and tells where objects of a certain class are likely to be in the image based on the image's semantic segmentation; see  top. This prior focuses the attention of the prediction based on the environment. Even more important, we can propagate this prior much more easily into the futureusing the egomotion of the vehicle -than a whole image or a semantic map. The reachability prior is a condensation of the environment, which contains the semantic context most relevant to the task.The proposed framework of estimating and propagating a multimodal reachability prior is not only beneficial for future localization of a particular object  left), but it also enables the task of emergence prediction  right). For safe operation, it is not sufficient to reason about the future location of the observed objects, but also potentially emerging objects in the scene must be anticipated, if their emergence exceeds a certain probability. For example, passing by a school requires extra care since the probability that a child can jump on the street is higher. Autonomous systems should behave differently near a school exit than on a highway. Predicting emergence of new objects did not yet draw much attention in literature.The three tasks in  differ via their input conditions: the reachability prior is only conditioned by the semantic segmentation of the environment and the class of interest. It is independent of a particular object. Future localization includes the additional focus on an object of interest and its past trajectory. These conditions narrow down the space of solutions and make the output distribution much more peaked. Emergence prediction is a reduced case of the reachability prior, where new objects can only emerge from unobserved areas of the scene.In this paper (1) we propose a future localization framework in egocentric view by transferring the work by Makansi et al.  from bird's-eye view to egocentric observations, where multimodality is even more difficult to : Top: The reachability prior (white rectangles) answers the general question of where a pedestrian could be in a scene. Left: Future localization (green rectangles) of a particular pedestrian crossing the street narrows down the solution from the reachability prior by conditioning the solution on past and current observations. The true future is shown as purple box. Right: The emergence prediction (green rectangles) shows where a pedestrian could suddenly appear and narrows down the solution from the reachability prior by conditioning the solution on the current observation of the scene. capture. Thus, (2) we propose to compute a reachability prior as intermediate result, which serves as attention to prevent forgetting rare modes, and which can be used to efficiently propagate scene priors into the future taking into account the egomotion. For the first time, (3) we formulate the problem of object emergence prediction for egocentric view with multimodality. (4) We evaluate our approach and the existing methods on the recently largest public nuScenes dataset  where the proposed approach shows clear improvements over the state of the art. In contrast to most previous works, the proposed approach is not restricted to a single object category.  We include heterogeneous classes like pedestrians, cars, buses, trucks and tricycles. (6) The prediction horizon was tripled from 1 second to 3 seconds into the future compared to existing methods. Moreover,  we show that the approach allows zero-shot transfer to unseen and noisy datasets (Waymo  and FIT).Bird's-Eye View Future Localization. Predicting the future locations or trajectories of objects is a well studied problem. It includes techniques like the Kalman filter , linear regression , and Gaussian processes . These techniques are limited to low-dimensional data, which excludes taking into account the semantic context provided by an image. Convolutional networks allow processing such inputs and using them for future localization.LSTMs have been very popular due to time series processing. Initial works exploited LSTMs for trajectories to model the interaction between objects , for scenes to exploit the semantics , and LSTMs with attention to focus on the relevant semantics .Another line of works tackle the multimodal nature of the future by sampling through cVAEs , GANs , and latent decision distributions . Choi et al.  model future locations as nonparametric distribution, which can potentially result in multimodality but often collapses to a single mode. Given the instabilities of Mixture Density Networks (MDNs) in unrestricted environments, some works restrict the solution space to a set of predefined maneuvers or semantic areas . Makansi et al.  proposed a method to learn mixture densities in unrestricted environments. Their approach first predicts diverse samples and then fits a mixture model on these samples. All these methods have been applied on static scenes recorded from a bird's-eye view, i.e., with full local observability and no egomotion. We build on the technique from Makansi et al.  to estimate multimodal distributions in egocentric views.Egocentric Future Localization. The egocentric camera view is the typical way of observing the scene in autonomous driving. It introduces new challenges due to the egomotion and the narrow field of view. Multiple works have addressed these challenges by projecting the view into bird's-eye view using 3D sensors . This is a viable approach, but it suffers from nondense measurements or erroneous measurements in case of LIDAR and stereo sensors, respectively.Alternative approaches try to work directly in the egocentric view. Yagi et al.  utilized the pose, locations, scales and past egomotion for predicting the future trajectory of a person. TraPHic  exploits the interaction between nearby heterogeneous objects. DTP  and STED  use encoder-decoder schemes using optical flow and past locations and scales of the objects. Yao et al.  added the planned egomotion to further improve the prediction. For autonomous driving, knowing the planned motion is a reasonable assumption , and we also make use of this assumption. All these models work with a deterministic model and fail to account for the multimodality and uncertainty of the future. The effect of this is demonstrated by our experiments.The most related work to our approach, in the sense that it works on egocentric views and predicts multiple modes, is the Bayesian framework by Bhattacharyya et al. . It uses Bayesian RNNs to sample multiple futures with uncertainties. Additionally, they learn the planned egomotion and fuse it to the main future prediction framework. NEMO  extends this approach by learning a multimodal distribution for the planned egomotion leading to better accuracy. Both methods need multiple runs to sample different futures and suffer from mode collapse, i.e., tend to predict only the most dominant mode, as demonstrated by our experiments.Egocentric Emergence Prediction. To reinforce safety in autonomous driving, it is important to not only predict the future of the observed objects but also predict where new objects can emerge. Predicting the whereabouts of an emerging object inherits predicting the future environment itself. Predicting the future environment was addressed by predicting future frames  and future semantic segmentation . These methods can only hallucinate new objects in the scene in a photorealistic way, but none of them explicitly predicts the structure where new objects can actually emerge. Vondrick et al.  consider a higher-level task and predict the probability of a new object to appear in an egocentric view. However, they only predict \"what\" object to appear but not \"where\". Fan et al.  suggested transferring current object detection features to the future. This way they anticipate both observed and new objects.Reachability Prior Prediction. The environment poses constraints for objects during navigation. While some recent works use an LSTM to learn environment constraints from images , others  choose a more explicit approach by dividing the environment into meaningful grids to learn the grid-grid, object-object and object-grid interactions. Also soft attention mechanisms are commonly used to focus on relevant features of the environments . While these methods reason about static environment constraints within the model proposed, we propose to separate this task and learn a scene prior before the future localization in dynamic scenes. Lee et al  proposed a similar module, where a GAN per object class generates multiple locations to place an object photorealistically.  shows the pipeline of our framework for the future localization task consisting of three main modules: (1) reachability prior network (RPN), which learns a prior of where members of an object class could be located in semantic map, (2) reachability transfer network (RTN), which transfers the reachability prior from the current to a future time step taking into account the planned egomotion, and (3) future localization network (FLN), which is conditioned on the past and current observations of an object and learns to predict a multimodal distribution of its future location based on the general solution from the RTN.Emergence prediction shares the same first two modules and differs only in the third network where we drop the condition on the past object trajectory. We refer to it as emergence prediction network (EPN). The aim of EPN is to learn a multimodal distribution of where objects of a class emerge in the future. The ground truth for training this network is obtained in a self-supervised manner by running RPN on the future static semantic map. (c) The future localization network (FLN) yields a multimodal distribution of the future bounding boxes of the object of interest through a sampling network (to generate multiple bounding boxes (samples)) and then a fitting network to fit the samples to a Gaussian mixture model (shown as heatmap overlayed on the future image with the means of the mixture components shown as green bounding boxes). The emergence prediction network (EPN) is identical to the FLN, except that it lacks the object-of-interest masks in the input.Given an observed scene from an egocentric view, the reachability prior network predicts where an object of a certain class can be at the same time step in the form of bounding box hypotheses. Let b rpn i,t = [x, y, w, h] for i \u2208 [1, N ] be the set of bounding box hypotheses predicted by our RPN at time step t, where (x, y) represents the center coordinates and (w, h) the width and height.Since the reachability prior network should learn the relation between a class of objects (e.g, vehicle) and the scene semantics (e.g, road, sidewalk, and so on), we remove all dynamic objects from the training samples. This is achieved by inpainting . Because inpainting on the semantic map causes fewer artifacts, in contrast to inpainting in the raw RGB image , the reachability prior is based on the semantic map. On one hand, the semantic map does not show some of the useful details visible in the raw image (e.g. the type of traffic sign or building textures). On the other hand, it is important that the inpainting does not introduce strong artifacts. These would be picked up during training and would bias the result (similar to keeping the original objects in the image).For each image I t at time t, we compute its semantic segmentation S t using deeplabV3plus  and derive its static semantic segmentation S \u2020 t after inpainting all dynamic objects. This yields the training data for the reachability prior network: the static semantic segmentation is the input to the network, and the removed objects of class c are ground-truth samples for the reachability. The network yields multiple hypotheses b rpn i,t as output and is trained using the EWTA scheme  with the loss:b t denotes a ground-truth bounding box of one instance from class c (e.g, vehicle or pedestrian) in image I t and l(.) denotes the L 2 norm. EWTA applies this loss to the hypotheses in a hierarchical way. It penalizes all hypotheses (i.e, i \u2208  where N = 20). After convergence, it halves the hypotheses (N = 10) and penalizes only the best 10 hypotheses. This halving is repeated until only the best hypothesis is penalized; see Makansi et al.  for details. A sample output of the reachability prior network for a car is shown in  (top).When running RPN on the semantic segmentation at time t, we obtain a solution for the same time step t. However, at test time, we require this prior in the unobserved future. Thus, we train a network to transfer the reachability at time t to time t + \u2206t, where \u2206t is the fixed prediction horizon and e t\u21d2t+\u2206t is the relative pairwise transformation between the pose at time t and t + \u2206t (referred to as planned egomotion) which is represented as a transformation vector (3d translation vector [t x , t y , t z ] and rotation quaternions [q w , q x , q y , q z ]). This transfer network can be learned with a self-supervised loss from a time series . For clarity, we draw the hypotheses on both image and semantic domains. Note that none of our networks has access to the future image or its semantic map (at time t + \u2206t).whereis the output of the RTN network. I t is the image and S \u2020 t is the static semantic segmentation at time t.  (middle) shows the reachability prior (top) transferred to the future. Given the ego motion as moving forward (red arrow) and the visual cues for upcoming traffic light and a right turn, the RTN anticipates that some more cars can be on the street emerging and transforms some of the RPN hypotheses to cover these new locations.Given an object which is observed for a set of frames from t \u2212 \u03b4t to t, where \u03b4t denotes the observation period, FLN predicts the distribution of bounding boxes in the future frame t + \u2206t.  shows the input to this network: the past images (I t\u2212\u03b4t , ..., I t ), the past semantic maps (S t\u2212\u03b4t , ..., S t ), the past masks of the object of interest (M t\u2212\u03b4t , ..., M t ), the planned egomotion (e t\u21d2t+\u2206t ), and the reachability prior in the future frame (b rtn i,t+\u2206t ). The object masks M s are provided as images, where pixels inside the object bounding box are object class c and 0 elsewhere.We use the sampling-fitting framework from Makansi et al.  to predict a Gaussian mixture for the future bounding box of the object of interest. The sampling network generates multiple hypotheses and is trained with EWTA, just like the RPN. The additional fitting network estimates the parameters (\u03c0 k , \u00b5 k , \u03c3 k ) of a Gaussian mixture model with K = 4 from these hypotheses, similar to the expectationmaximization algorithm but via a network; see Makansi et al.  for details. An example of the FLN prediction is shown in . The fitting network is trained with the negative log-likelihood (NLL) lossRather than predicting the future of a seen object, the emergence prediction network predicts where an unseen object can emerge in the scene. The EPN is very similar to the FLN shown in figure 3c. The only difference is that the object masks are missing in the input, since the task is not conditioned on a particular object but predicts the general distribution of objects emerging.The network is trained on scenes where an object is visible in a later image I t+\u2206t (ground truth), but not in the current image I t . Like for the future localization network, we train the sampling network with EWTA and the fitting network with NLL.Mapillary Vistas . We used the Mapillary Vistas dataset for training the inpainting method from  on semantic segmentation and for training our reachability prior network. This dataset contains around 25K images recorded in different cities across 6 continents, from different viewpoints, and in different weather conditions. For each image, pixelwise semantic and instance segmentation are provided. The images of this dataset are not temporally ordered, which prevents its usage for training the RTN, FLN, or EPN.nuScenes . nuScenes is very large autonomous driving dataset consisting of 1000 scenes with 20 seconds each. We used it for training and evaluating the proposed framework. We did not re-train the reachability prior network on this dataset, as to test generalization of the reachability prior network across different datasets. The nuScenes dataset provides accurate bounding box tracking for different types of traffic objects and the egomotion of the observer vehicle. We used the standard training/validation split (700/150 scenes) of the dataset for training/evaluating all experiments.Waymo Open Dataset . Waymo is the most recent autonomous driving dataset and contains 1000 scenes with 20 seconds each. To show zero-shot transfer of our framework (i.e, without re-training the model), we used the standard 202 testing scenes.FIT Dataset. We collected 18 scenes from different locations in Europe and relied on MaskRCNN  and deepsort  to detect and track objects, and DSO  to esti-mate the egomotion. This dataset allows testing the robustness to noisy inputs (without human annotation). We will make these sequences and the annotations publicly available.FDE. For evaluating both future localization and emergence prediction, we report the common Final Displacement Error (FDE), which estimates the L 2 distance of the centers of two bounding boxes in pixels.IOU. We report the Intersection Over Union (IOU) metric to evaluate how well two bounding boxes overlap.The above metrics are designed for single outputs, not distributions. In case of multiple hypotheses, we applied the above metrics between the ground truth and the closest mode to the ground truth (known as Oracle ).NLL. To evaluate the accuracy of the multimodal distribution, we compute the negative log-likelihood of the ground-truth samples according to the estimated distribution.We used ResNet-50  as sampling network in all parts of this work. The fitting network consisted of two fully connected layers (each with 500 nodes) with a dropout layer (rate = 0.2) in between. In the FLN, we observed \u03b4t = 1 second and predicted \u2206t = 3 seconds into the future. For the EPN, we observed only one frame and predicted \u2206t = 1 second into the future. We used N = 20 for all sampling networks, and K = 4 and K = 8 as the number of mixture components for the FLN and the EPN, respectively. The emergence prediction task requires more modes compared to the future localization task since the distribution has typically more modes in this task.As there is only one other work so far on egocentric multimodal future prediction , we compare also to unimodal baselines, which are already more established.Kalman Filter . This linear filter is commonly used for estimating the future state of a dynamic process through a set of (low-dimensional) observations. It is not expected to be competitive, since it considers only the past trajectory and ignores all other information.DTP . DTP is a dynamic trajectory predictor for pedestrians based on motion features obtained from optical flow. We used their best performing framework, which predicts the difference to the constant velocity solution.STED . STED is a spatial-temporal encoderdecoder that models visual features by optical flow and temporal features by the past bounding boxes through GRU encoders. It later fuses the encoders into another GRU decoder to obtain the future bounding boxes. . RNN-ED-XOE is an RNN-based encoder-decoder framework which models both temporal and visual features similar to STED. RNN-ED-XOE additionally encodes the future egomotion before fusing all information into a GRU decoder for future bounding boxes.FLN-Bayesian using . The work by Bhattacharyya et al.  is the only multimodal future prediction work for the egocentric scenario in the literature. It uses Bayesian optimization to estimate multiple future hypotheses and their uncertainty. Since they use a different network architecture and data modalities, rather than direct method comparison we port their Bayesian optimization into our framework for fair comparison. We re-trained our FLN with their objective to create samples by dropout during training and testing time as replacement for the EWTA hypotheses. We used the same number of samples, N = 20, as in our standard approach.All these baselines predict the future trajectory of either pedestrians  or vehicles . Thus, we re-trained them on nuScenes  to handle both pedestrian and vehicle classes. Moreover, some baselines utilize the future egomotion obtained from ORB-SLAM2  or predicted by their framework, as in . For a fair comparison, we used the egomotion from nuScenes dataset when re-training and testing their models, thus eliminating the effect of different egomotion estimation methods.FLN w/o reachability. To measure the effect of the reachability prior, we ran this version of our framework without RPN and RTN.FLN + reachability. Our full framework including all 3 networks: RPN, RTN, FLN.Due to the lack of comparable work addressing the emergence prediction task, so far, we conduct an ablation study on the emergence prediction to analyze the effect of the proposed reachability prior on the accuracy of the prediction.  shows a quantitative evaluation of our proposed framework against all the baselines listed above. To distinguish test cases that can be solved with simple extrapolation from more difficult cases, we use the performance of the Kalman filter ; see also . A test sample, where the Kalman filter  has a displacement error larger than average is counted as challenging. An error more than twice the average is marked very challenging. In , we show the error only for the whole test set (all) and the very challenging subset (hard). More detailed results are in the supplemental material.As expected, deep learning methods outperform the extrapolation by a Kalman filter on all metrics. Both variants of our framework show a significant improvement over all baselines for the FDE and IOU metrics. When we use FDE or IOU, we use the oracle selection of the hypotheses (i.e, nuScenes  (all 11k / hard 1.4k)Waymo    : Result for future localization on the nuScenes , the Waymo , and our FIT datasets. The bottom three methods predict a multimodal distribution. The other methods are not probabilistic and do not allow evaluation of the NLL. For each cell, we report the average over (all testing scenarios/the very challenging scenarios). The number of all/very challenging scenarios for each dataset is shown in parentheses (top).the closest bounding box to the ground truth). Hence, a multimodal method is favored over a unimodal one. Still, such significant improvement indicates the need for multimodality. To evaluate without the bias introduced by the oracle selection, we also report the negative log-likelihood (NLL). Both variants of the proposed framework outperform the Bayesian framework on all metrics including the NLL. In fact, the Bayesian baseline is very close to the best unimodal baseline. This indicates its tendency for mode collapse, which we also see qualitatively. The use of the reachability prior is advantageous on all metrics and for all difficulties.As the networks (ours and all baselines) were trained on nuScenes, the results on Waymo and FIT include a zeroshot transfer to unseen datasets. We obtain the same ranking for unseen datasets as for the test set of nuScenes. This indicates that overfitting to a dataset is not an issue for this task. We recommend having cross-dataset experiments (as we show) also in future works to ensure that this stays true and future improvements in numbers are really due to better models and not just overfitting.  shows some qualitative example in four challenging scenarios, where there are multiple options for the future location. (1) A pedestrian starts crossing the street and his future is not deterministic due to different speed estimates. (2) A pedestrian enters the scene from the left and will either continue walking to cross the street or will stop at the traffic light. (3) A tricycle driving from a parking area will continue driving to cross the road or will stop to give way to our vehicle. (4) A car entering the scene from the left will either slow down to yield or drive faster to overpass.For all scenarios, we observe that the reachability prior (shown as set of colored bounding boxes) defines the general relation between the object of interest and the static elements of the scene. Similar to the observation from our quantitative evaluation, the Bayesian baseline predicts a single future with some uncertainty (unimodal distribution). Our framework without exploiting the reachability prior (FLN w/o RPN) tends to predict more diverse futures but still lacks predicting many of the modes. The reachability prior helps the approach to cover more of the possible  : Quantitative results for the emergence prediction task on the nuScenes dataset .future locations. We highly recommend watching the supplementary video at https://youtu.be/jLqymg0VQu4, which gives a much more detailed qualitative impression of the results, as it allows the observer to get a much better feeling for the situation than the static pictures in the paper.  shows the ablation study on the importance of using the reachability prior for the task of predicting object emergence in a scene. Similar to future localization, exploiting the reachability prior yields a higher accuracy and captures more of the modes. Two qualitative examples for this task are shown in . Examples include scenarios (1) where a vehicle could emerge in the scene from the left street, could pass by or could be oncoming; (2) where a car could emerge from the left, from the right, it could pass by, or could be oncoming. EPN learns not only the location in the image, but also meaningful scales. For instance, the anticipation of passing-by cars has a larger scale compared to expected oncoming cars. The distributions for the two examples are different since more modes for emerging vehicles are expected in the second example (e.g, emerging from the right side). Notably, the reachability prior solution is different from the emergence solution, where closeby cars in front of the egocar are part of the reachability prior solution but are ruled out, since a car cannot suddenly appear there. More results are provided in the supplemental material.In this work, we introduced a method for predicting future locations of traffic objects in egocentric views without predefined assumptions on the scene and by taking into ac-   : Sample results for emergence prediction on nuScenes . For each row (scenario), we show (a) the observed image and the planned egomotion to the future (red circle indicates no egomotion), (b) the reachability prior from the RTN in the future frame, (c-d) both variants of the emergence prediction framework.count the multimodality of the future. We showed that a reachability prior and multi-hypotheses learning help overcome mode collapse. We also introduced a new task relevant for autonomous driving: predicting locations of suddenly emerging objects. Overall, we obtained quite good results even in difficult scenarios, but careful qualitative inspection of many results still shows a lot of potential for improvement on future prediction.", "videoStruct": []}, {"title": "MICCAI-PRIME 2020 - Uniformizing Techniques to Process CT scans with 3D CNNs for TB Prediction", "authors": "Hasib Zunair", "abstract": "", "publicationOrg": "MICCAI", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2007.13224.pdf", "pdfPath": "/data/cache/2/PDFs/MICCAIPRIME2020UniformizingTechniquestoProcessCTscanswith3DCNNsforTBPrediction.pdf", "publicationUrl": "https://arxiv.org/pdf/2007.13224.pdf", "codeUrl": "https://github.com/hasibzunair/uniformizing-3D", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/IP6poudyny4", "videoPath": "/data/cache/2/videos/MICCAI-PRIME 2020 - Uniformizing Techniques to Process CT scans with 3D CNNs for TB Prediction.mp4", "pdfText": "To learn the geometric properties of volumetric data, there are challenges imposed by the data itself . One major challenge is fitting the data in GPU memory during optimization. Furthermore, complicacy also arises when dealing with the variable depth size in the data. Hence, the data preparation scheme plays a vital role to build robust systems comprising volumetric image data. In the context of medical imaging, deep learning  has been widely used in a variety of tasks and domains . While many of these medical image modalities are two dimensional (2D), computed tomography (CT) volumes are three dimensional (3D) and require more computational expense which can be an insurmountable obstacle in the case of limited GPU memory.This necessitates the use of 2D CNN architectures  where the 3D data is treated as a set of independent slices. However, there is evidence that better results are achievable when using the full volumetric data . An observation is that 2D approaches discard information along the depth dimension/z-axis and prevents to preserve 3D context  leading to non-optimal performance. On the other hand, memory challenges are also experienced due to the nature of 3D data. It is also noteworthy to mention that 3D datasets exist which are annotated at the slice level, where it is justified to use 2D CNN approaches  but it is not the case when the data is annotated at the volume level .In this work, we evaluate a set of volume uniformizing methods in the 3D image domain. First, we explore sampling a subset of the slices using a spacing factor to evenly sample from the sequence of slices to construct the desired volumetric output. However, deliberately losing information likely prevents learning robust representations from the data with the risk of adding artifacts. We explore interpolating over the z-axis to capture information from multiple slices which turns out to be a very reasonable solution and provides good performance and at the same time satisfy GPU memory requirements. We put our technique to the test using 3D medical images originating from the Computed Tomography (CT) domain with annotations at the CT/volume level. This is evaluated on the ImageCLEF Tuberculosis Severity Assessment 2019 test set which outperforms all methods leveraging only image information and achieves 5-th position overall. We summarize our contributions as follows:1. We evaluate Even Slice Selection (ESS) and Spline Interpolated Zoom (SIZ) which exploit the full geometry of the 3D CT data based on improvements upon SSS . 2. We develop a 17-layer 3D convolutional neural network inspired by  with major modifications. 3. We perform controlled ablation studies and show superior performance and reliability for SIZ, both qualitatively and quantitatively. 4. We evaluate our best approach on the ImageCLEF Tuberculosis Severity Assessment 2019 benchmark which outperforms all methods leveraging only image information achieving 5-th position overall.2D Approaches. To mimic the 3-channel image representation (i.e., RGB), prior works follow multi-slice representation of 3D images as 2D inputs . UUIP BioMed  proposes a CNN using 2D projections of 3D CT scans which provide a probability score. HHU  demonstrates a multi-stage approach where they first assess the CT-findings for another task and then apply linear regression to obtain the final predictions. A hybrid 2D CNN was trained by creating 2D derived images by concatenating sagittal and coronal CT slices by Com-pElecEngCU . Ensembling of 2D CNNs was also demonstrated by SD VA HCS/UCSD  for tuberculosis (TB) prediction. MedGIFT  used a graphbased approach by dividing the lung fields into several subregions (different for each patient) and considered these subregions as nodes of a graph. These graphs were transformed into lung descriptor vectors and then classified. UniversityAlicante  proposed to use each CT volume as a time series and used optical flow on the 3 directions. MostaganemFSEI  first selected relevant axial CT slices manually. Features are extracted using a 2D CNN which is followed by a long short term memory (LSTM) to obtain the final predictions. SSN CoE  manually selected a subset of slices for each patient and then used a 2D CNN for classification. FIIAugt  performed a random sampling of pixels of the CT volumes and used a combination of decision trees and weak classifiers.3D Approaches. Instead of regarding the 3D spatial information as the input channel in 2D based methods, studies based on 3D convolutions for 3D medical image analysis have been demonstrated . These methods are capable of capturing the 3D context in any axis and mitigates the limited 3D context along a certain axis (depth/z-axis) in 2D approaches. Hence, the 3D methods are generally better when the 3D context is required (e.g, locating nodules). A related study is UUIP , where they use 3D CNN as an autoencoder followed by a random forest classifier. Before training the autoencoder, the downsampling was performed at the volume level to preserve the 3D context. UoAP  used a 3D CNN (VoxNet) with either 16 or 32 slices that were selected from the top, middle, and bottom of the volume.In this section, we describe the main components and the algorithmic steps of the methods employed for the task of TB prediction. Our goal is to learn a discriminative function f (X) \u2208 {0, 1}, where 1 indicates high TB severity and 0 otherwise. X represents a CT scan volume of size W \u00d7 H \u00d7 D, where W, H, and D represent the width, height, and depth of the volume respectively.We discuss the techniques in detail which we use to prepare the data before learning f (.), the discriminative function. We talk about the algorithmic step of each technique and show qualitative results. It is important to mention that readers should not be confused when we refer to the term slices, it means that the sampling is done at the slice level to acquire the desired output volume.Slice Selection Depth variability of the 3D CT scans motivate the concept of sampling from slice level to construct the desired volume and balance between model performance and GPU memory constraints . Subset Slice Selection (SSS): In this technique originally proposed in , slices are sampled from the first, middle and last position of the entire volume. The middle slices are sampled by indexing from half of the input volume depth to ensure consistency due to the depth variability. A depthwise stack is then performed over the subsets to attain the desired input volume. An illustration is shown in .Even Slice Selection (ESS): A major drawback of SSS is that it prevents us from using the remaining subset of the data which causes the processed volume not to be representative of the original volume. We show qualitative evidence in  which is indicated by red circles. ESS can be considered as an improved version of SSS which provides good performance compared to SSS. In ESS, a target depth N and a scan depth of size D is computed. A spacing factor is then determined by the equation F = D N . Sampling is done at the slice level by maintaining the spacing factor F between the sequence of slices in the volumetric data. This gives a better representation compared to the SSS technique as we show experimentally in later sections. The algorithmic steps are shown in Algorithm 1. Spline Interpolated Zoom (SIZ) Even though ESS preserves the representation to an extent, the desired volume is still acquired from a subset of the data. Therefore, to discard the concept of sampling from independent slices, an alternative solution is Spline Interpolated Zoom (SIZ) which enables even better representation of the volumetric data. Similar techniques have been used in the other studies . In this technique, instead of manually selecting a subset of slices, a constant target depth size of N is pre-determined. We then take each volume, calculate its depth D, and zoom it along the z-axis by a factor of 1 D/N using spline interpolation , where the interpolant is an order of three. Here, the input volume is zoomed or squeezed by replicating the nearest pixel along the depth/z-axis. A visual representation of this can be seen in  along with the algorithm summarized in Algorithm 2. As it uses spline interpolation to squeeze or expand the z-axis to the desired depth, it retains a substantial level of information from original 3D volume as opposed to the aforementioned techniques, SSS and ESS, which discarded a part of the volumetric data. Inspired from , we design a 17 layer 3D CNN which comprises four 3D convolutional (CONV) layers with two layers consisting of 64 filters followed by 128 and 256 filters all with a kernel size of 3 \u00d7 3 \u00d7 3. Each CONV layer is followed by a max-pooling (MAXPOOL) layer with a stride of 2 and ReLU activation which ends with batch normalization (BN) layer . Essentially, our feature extraction block consists of four CONV-MAXPOOL-BN modules. The final output from the feature extraction block is flattened and passed to a fully connected layer with 512 neurons. We use an effective dropout rate of 60% similar to . Due to a coding error, we implement this using two dropout layers . The output is then carried to a dense layer of 2 neurons with softmax activation for the binary classification problem. The network architecture is shown in . We consider keeping the network relatively simple to avoid overparameterization  problems with only 10,658,498 learnable parameters. This is also motivated by the fewer number of training samples and the memory challenges associated with it.The dataset is provided by ImageCLEF Tuberculosis 2019  [17], intended for the task of severity scoring (SVR). It consists of a total 335 chest 3D CT scans with annotation of high and low provided by a medical doctor and also lung segmentation masks, in addition to clinically relevant metadata was also available which includes the following binary measures: disability, relapse, symptoms of TB, comorbidity, bacillary, drug resistance, higher education, ex-prisoner, alcoholic, smoking. From the dataset, 218 individual chest CT scans are provided for training, and the remaining 117 were held out for the final evaluation in the ImageCLEF evaluation platform. The CT images each have a dimension of 512\u00d7512 pixels and the depth size varying from about 50 to 400 which store raw voxel intensity in Hounsfield units (HU).  shows an instance of this.We consider SSS  as the baseline. Each configuration embodies a different type of processing discussed in Section 3.1. All configurations are based on the network described in Section 3.2 and trained on a machine with NVIDIA 1050Ti with 4GB memory. We used Stochastic Gradient Descent(SGD) optimizer with a learning rate of 10 \u22126 and a momentum of 0.99, parameters found from multiple trials. Weight is initialized using the Glorot initialization method  and minimize the Mean average error  during training. During training, the network accepts input of size 128 \u00d7 128 \u00d7 64 with a batch size of 2. We tried increasing the depth size to more than 64 but resulted in GPU memory error. For our experiments with ESS, we found four CT scans which had a depth of less than 64 with a minimum being 47. In these cases, we first apply ESS and then calculate the difference with the target depth, 64, and repeatedly add the last slice until the target depth is reached. We resize to 128 \u00d7 128 on the slice level and then use techniques discussed in Section 3.1 to get the desired volume. To ensure a fair comparison between the uniformizing methods, we keep the desired input size of 128 \u00d7 128 \u00d7 64 for all our experiments. We provide code and model to reproduce our experiments at https://github.com/hasibzunair/uniformizing-3D.As per challenge rules, the task is evaluated as a binary classification problem. The evaluation metrics are Area Under the ROC Curve (AUC) and accuracy (ACC), prioritizing the former by the challenge organizers. We refrain from using other evaluation metrics since it would limit our comparison with the approaches proposed in the challenge.In this section, extensive experiments are conducted to evaluate the performance of the uniformizing methods. First, we compare our methods with the baseline on the ImageCLEF Tuberculosis Severity Assessment 2019 benchmark. Since the dataset is small, we also perform cross-validation tests to estimate the general effectiveness and reliability of the methods, ensuring a balance between bias and variance. Finally, we show ablations of orthogonal preprocessing and how our method performs related to other methods on the ImageCLEF Tuberculosis Severity Assessment 2019 benchmark.Comparison with baseline. We believe SIZ better represents the 3D CT when downsampled compared to SSS and ESS. This is depicted in , which shows that SIZ yields better performance in both metrics by a margin of 9% and 8% compared to SSS and is of significance. We further validate this by showing the qualitative comparison in  in which we show visual evidence that slice selection methods do not leverage information from full 3D CT scans which SIZ does. It is also observed that ESS yields slightly better results than SSS. This is because even though ESS samples from half of the volume, the sampling is carried out in a sequential process. This approach results in a better representation of the 3D CT scan compared to SSS where a subset of slices is sampled from predefined points. Thus, selecting specific slices does not preserve the semantic meaning of volumetric data as it is not the proper representation of the 3D CT scan which is also intuitive. Even though ESS is downsampling the volume from a subset, this still results in better performance as the sampling is done throughout the entire volume. In particular, ESS increases the probability of sampling the TB affected slices compared to SSS. Since TB infection can affect any part of the lung, it is also not possible to determine which slices are to be discarded without looking at the scans individually. As the annotations are provided at the volume level and not at the slice level, it is crucial to retrieve information from the entire volume.Cross validation. We also report the cross-validation results as shown in . It can be seen that SIZ not only has a higher mean accuracy than the baselines but also has a lower standard deviation owing to more reliability. ROC curve that is closer to the upper right indicates better performance (TPR is higher than FPR). Even though during the early and last stages, the ROC curve of SIZ seems to highly fluctuate at certain points, the overall performance is much higher than the baselines, as indicated by the AUC value. This better performance demonstrates that 3D context plays a crucial role and enables the model to learn effective representations.  Ablation study.  illustrates the ablations on orthogonal preprocessing. For all configurations SSS, ESS, and SIZ we observe performance improvements on both AUC and ACC after pixel normalization and zero-centering. Since the 3D CT scans have raw voxel intensities in Hounsfield units (HU), we normalize the values between [0,1]. We then perform zero-centering by subtracting the total mean value from each pixel, making the mean of the total dataset zero. For SIZ, the increase in performance compared to baseline is the larget with an increase of 11% and 14% margin in AUC and ACC respectively.ImageCLEF Tuberculosis Severity Assessment 2019 Benchmark. We summarize the results in  which report the performance evaluation on the final test set. It is observed that our best method, SIZ, achieves comparable performance with the top-ranking methods. It is noteworthy to mention that UUIP Biomed , UUIP , HHU  and CompElecEngCU  leverage the clinically relevant metadata in order to significantly improve performance and also develop multi-stage approaches which adds complexity . We increased the input volume to the 128 \u00d7 128 \u00d7 128, the same as UUIP  which results in a model almost three times larger than ours with 29,532,866 learnable parameters and led to a memory error. Even with using only image information, our method performs better than SD VA HCS/UCSD  where they used an ensemble of 2D CNNs and the relevant meta-data. It also performed better than the 3D CNN method by UoAP . Our best method also outperforms several 2D approaches such as MedGIFT , SSN CoE  and FIIAugt . From  it is also seen that among the top-ranking results which only use image information (no meta-data), our method achieves the best results. Even though MedGIFT  did not use any meta-data, they were the only team that used the lung segmentation masks.We address the problem of variable volume size and heavy computation requirements during optimization when dealing with 3D image data. In particular, we evaluate a set of volume uniformizing methods applied to 3D medical images in the CT domain for the task of TB prediction. We hypothesize that analyzing 3D images in a per slice (2D) basis is a sub-optimal approach that can be improved by 3D context if computational challenges can be overcomed. We systematically evaluate different ways of uniformizing CT volumes so that they fit into memory and determine interpolating over the z-axis to be the best. We further validate this approach on the ImageCLEF benchmark obtaining 5th place and beat all methods which operate on the CT image alone without patient metadata.", "videoStruct": [{"timeStart": "00-00-06", "timeEnd": "00-00-13", "sentence": "a common approach to medical image analysis on volume metric data users deep to the convolution neural networks"}, {"timeStart": "00-00-15", "timeEnd": "00-00-20", "sentence": "this is largely attributed to the challenges imposed by the nature of the three d data"}, {"timeStart": "00-00-20", "timeEnd": "00-00-24", "sentence": "while many of these medical image modalities are two-dimensional"}, {"timeStart": "00-00-24", "timeEnd": "00-00-26", "sentence": "computer tomography"}, {"timeStart": "00-00-26", "timeEnd": "00-00-28", "sentence": "or CD scans are three dimensional"}, {"timeStart": "00-00-28", "timeEnd": "00-00-31", "sentence": "and require more computational expense"}, {"timeStart": "00-00-31", "timeEnd": "00-00-34", "sentence": "in order to make predictions from the entire volume"}, {"timeStart": "00-00-34", "timeEnd": "00-00-39", "sentence": "this can be an insurmountable obstacle in the case of limited dpe memory"}, {"timeStart": "00-00-39", "timeEnd": "00-00-41", "sentence": "complicate also rises"}, {"timeStart": "00-00-41", "timeEnd": "00-00-44", "sentence": "when dealing with the variable depth size in the data"}, {"timeStart": "00-00-44", "timeEnd": "00-00-48", "sentence": "and those the data preparation scheme plays a vital role"}, {"timeStart": "00-00-48", "timeEnd": "00-00-50", "sentence": "to build robust prediction models"}, {"timeStart": "00-00-50", "timeEnd": "00-00-52", "sentence": "comprising of volume metric image data"}, {"timeStart": "00-00-54", "timeEnd": "00-01-00", "sentence": "recent works address this problem where the three d data is treated as a set of independent fly says"}, {"timeStart": "00-01-01", "timeEnd": "00-01-05", "sentence": "these two d slices are then used to train to the cnn's"}, {"timeStart": "00-01-05", "timeEnd": "00-01-07", "sentence": "to perform the desired prediction tasks"}, {"timeStart": "00-01-09", "timeEnd": "00-01-15", "sentence": "an advantage of this method is that it enables practitioners to easily downscale the slices"}, {"timeStart": "00-01-15", "timeEnd": "00-01-18", "sentence": "do the desired shape using standard research operations"}, {"timeStart": "00-01-19", "timeEnd": "00-01-25", "sentence": "and at the same time being able to feed batches of images n g p memory during optimization"}, {"timeStart": "00-01-26", "timeEnd": "00-01-34", "sentence": "and observation is that these two d approaches discard information along the depth dimension or z axis"}, {"timeStart": "00-01-34", "timeEnd": "00-01-37", "sentence": "and prevents to preserve3Dcontext"}, {"timeStart": "00-01-37", "timeEnd": "00-01-39", "sentence": "which leads to non optimal performance"}, {"timeStart": "00-01-41", "timeEnd": "00-01-45", "sentence": "we evaluate a set of volume uniform sizing method"}, {"timeStart": "00-01-45", "timeEnd": "00-01-50", "sentence": "in the three d image domain for the task of your glasses prediction"}, {"timeStart": "00-01-51", "timeEnd": "00-01-54", "sentence": "in the first method called f f f f"}, {"timeStart": "00-01-54", "timeEnd": "00-01-56", "sentence": "the slices are sampled from the first"}, {"timeStart": "00-01-56", "timeEnd": "00-01-58", "sentence": "middle and last position"}, {"timeStart": "00-01-58", "timeEnd": "00-02-00", "sentence": "of the entire input volume"}, {"timeStart": "00-02-00", "timeEnd": "00-02-04", "sentence": "it dept why stack is then performed over the subset"}, {"timeStart": "00-02-04", "timeEnd": "00-02-06", "sentence": "to attain the desired input volume"}, {"timeStart": "00-02-08", "timeEnd": "00-02-14", "sentence": "to be more representative of the original volume the second method e s s"}, {"timeStart": "00-02-14", "timeEnd": "00-02-18", "sentence": "samples by maintaining a spacing factor between the sequence of slices"}, {"timeStart": "00-02-18", "timeEnd": "00-02-20", "sentence": "in the input volume"}, {"timeStart": "00-02-22", "timeEnd": "00-02-29", "sentence": "to completely discard the concept of sampling from independent slices and alternative solution is flying interpolated room"}, {"timeStart": "00-02-29", "timeEnd": "00-02-34", "sentence": "which performed the sampling by leveraging the entire volume"}, {"timeStart": "00-02-34", "timeEnd": "00-02-36", "sentence": "using spine interpolation"}, {"timeStart": "00-02-36", "timeEnd": "00-02-40", "sentence": "this method retains a substantial level of information"}, {"timeStart": "00-02-40", "timeEnd": "00-02-45", "sentence": "from the original three d volume as opposed to sss and SS"}, {"timeStart": "00-02-45", "timeEnd": "00-02-48", "sentence": "which discards a part of the volume metric data"}, {"timeStart": "00-02-50", "timeEnd": "00-02-55", "sentence": "after processing the volumes using the previous uniform raising methods"}, {"timeStart": "00-02-55", "timeEnd": "00-02-58", "sentence": "we train a3DCNN classifier"}, {"timeStart": "00-02-58", "timeEnd": "00-03-01", "sentence": "with two outputs for the binary classification problem"}, {"timeStart": "00-03-03", "timeEnd": "00-03-10", "sentence": "we test each uniform as in method on the image left were glosses severity assessment benchmark"}, {"timeStart": "00-03-11", "timeEnd": "00-03-13", "sentence": "sees yields better"}, {"timeStart": "00-03-14", "timeEnd": "00-03-20", "sentence": "in both matrix by a margin of nine and eight percent compared to s s s"}, {"timeStart": "00-03-21", "timeEnd": "00-03-24", "sentence": "since they were glosses infection can affect"}, {"timeStart": "00-03-24", "timeEnd": "00-03-28", "sentence": "any part of the lung it is not possible to determine"}, {"timeStart": "00-03-28", "timeEnd": "00-03-30", "sentence": "width slices are to be discarded"}, {"timeStart": "00-03-30", "timeEnd": "00-03-32", "sentence": "without looking at the scans individually"}, {"timeStart": "00-03-33", "timeEnd": "00-03-38", "sentence": "as the annotations are provided at the volume level and not at the slice level"}, {"timeStart": "00-03-38", "timeEnd": "00-03-42", "sentence": "it is crucial to retrieve information from the entire volume"}, {"timeStart": "00-03-44", "timeEnd": "00-03-46", "sentence": "cross validation tests"}, {"timeStart": "00-03-46", "timeEnd": "00-03-47", "sentence": "also showed that"}, {"timeStart": "00-03-47", "timeEnd": "00-03-51", "sentence": "spine into a polluted zoom not only has a higher mean accuracy"}, {"timeStart": "00-03-51", "timeEnd": "00-03-54", "sentence": "then sss and SS"}, {"timeStart": "00-03-54", "timeEnd": "00-03-57", "sentence": "but also has a lower standard deviation"}, {"timeStart": "00-03-57", "timeEnd": "00-03-59", "sentence": "owing to more reliability"}, {"timeStart": "00-03-59", "timeEnd": "00-04-03", "sentence": "a similar pattern is observed from the rsr curves"}, {"timeStart": "00-04-04", "timeEnd": "00-04-06", "sentence": "this method demonstrates"}, {"timeStart": "00-04-06", "timeEnd": "00-04-09", "sentence": "that3Dcontext plays a crucial role"}, {"timeStart": "00-04-09", "timeEnd": "00-04-12", "sentence": "and enables the model to learn effective representation"}, {"timeStart": "00-04-15", "timeEnd": "00-04-17", "sentence": "for all configuration"}, {"timeStart": "00-04-17", "timeEnd": "00-04-23", "sentence": "performance improvements were seen both after pigs along la's ation and zero centering"}, {"timeStart": "00-04-25", "timeEnd": "00-04-28", "sentence": "it says that having the largest"}, {"timeStart": "00-04-31", "timeEnd": "00-04-37", "sentence": "we also compared with other methods on the image cleft tuberculosis prediction desk"}, {"timeStart": "00-04-37", "timeEnd": "00-04-40", "sentence": "flying interpolating zoom if I said"}, {"timeStart": "00-04-40", "timeEnd": "00-04-42", "sentence": "with treaty CNN"}, {"timeStart": "00-04-43", "timeEnd": "00-04-47", "sentence": "achieves performance on par with the top ranking methods"}, {"timeStart": "00-04-47", "timeEnd": "00-04-53", "sentence": "it is noteworthy to mention that these methods leveraged the clinically relevant metadata"}, {"timeStart": "00-04-53", "timeEnd": "00-04-56", "sentence": "in order to significantly improve performance"}, {"timeStart": "00-04-56", "timeEnd": "00-04-59", "sentence": "and also developed multi stage approaches"}, {"timeStart": "00-05-01", "timeEnd": "00-05-05", "sentence": "we further tried increasing the input depth to want twenty eight"}, {"timeStart": "00-05-05", "timeEnd": "00-05-07", "sentence": "same as you you I p"}, {"timeStart": "00-05-07", "timeEnd": "00-05-10", "sentence": "but could not fit on a four g be memory"}, {"timeStart": "00-05-10", "timeEnd": "00-05-12", "sentence": "even with the bad sides of one"}, {"timeStart": "00-05-17", "timeEnd": "00-05-23", "sentence": "our method performed better than the3DCNN method by the group you are a p"}, {"timeStart": "00-05-23", "timeEnd": "00-05-25", "sentence": "and several of the two d approach is"}, {"timeStart": "00-05-26", "timeEnd": "00-05-32", "sentence": "among the teams which only used image information our method achieves the best results"}, {"timeStart": "00-05-33", "timeEnd": "00-05-36", "sentence": "even though med gift did not use any meta data"}, {"timeStart": "00-05-36", "timeEnd": "00-05-40", "sentence": "they were the only team that used the lung segmentation masks"}, {"timeStart": "00-05-42", "timeEnd": "00-05-48", "sentence": "we recommend practitioners that analyzing3Dimages in a power slide basis"}, {"timeStart": "00-05-49", "timeEnd": "00-05-54", "sentence": "is a soft optimal approach that can be improved by3Dcontext"}, {"timeStart": "00-05-54", "timeEnd": "00-05-56", "sentence": "if computational challenges"}, {"timeStart": "00-05-56", "timeEnd": "00-05-57", "sentence": "can be overcome"}, {"timeStart": "00-05-57", "timeEnd": "00-06-03", "sentence": "we systematically evaluate different ways of uniform mising city volumes"}, {"timeStart": "00-06-03", "timeEnd": "00-06-08", "sentence": "so that the feet into memory and determined interpolating over the z axis to be the best"}, {"timeStart": "00-06-09", "timeEnd": "00-06-15", "sentence": "we further validate this approach on the image left benchmark obtaining fifth position"}, {"timeStart": "00-06-15", "timeEnd": "00-06-17", "sentence": "and beat all methods"}, {"timeStart": "00-06-17", "timeEnd": "00-06-21", "sentence": "which operate on the city image alone without patient metadata"}]}, {"title": "Multi-Drone Delivery using Transit (ICRA 2020 Best Paper Finalist in Multi-Robot Systems)", "authors": "kmenda", "abstract": "", "publicationOrg": "ICRA", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/1909.11840.pdf", "pdfPath": "/data/cache/2/PDFs/MultiDroneDeliveryusingTransitICRA2020BestPaperFinalistinMultiRobotSystems.pdf", "publicationUrl": "https://arxiv.org/pdf/1909.11840.pdf", "codeUrl": "https://github.com/sisl/MultiAgentAllocationTransit.jl", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/2U8jI-n9Ulk", "videoPath": "/data/cache/2/videos/Multi-Drone Delivery using Transit (ICRA 2020 Best Paper Finalist in Multi-Robot Systems).mp4", "pdfText": "Rapidly growing e-commerce demands have greatly strained dense urban communities by increasing delivery truck traffic and slowing operations and impacting travel times for public and private vehicles . Further congestion is being induced by newer services relying on ride-sharing vehicles. There is a clear need to redesign the current method of package distribution in cities . The agility and aerial reach of drones, the flexibility and ease of establishing drone networks, and recent advances in drone capabilities make them highly promising for logistics networks . However, drones have limited travel range and carrying capacity . On the other hand, ground-based transit networks have less flexibility but greater coverage and throughput. By combining the strengths of both, we can achieve significant commercial benefits and social impact (e.g., reducing ground congestion and delivering essentials).We address the problem of operating a large number of drones to deliver multiple packages simultaneously in an area. The drones can use one or more vehicles in a publictransit network as modes of transportation, thereby saving their limited battery energy stored onboard and increasing their effective travel range. We are required to decide which deliveries each drone should make and in what order, which modes of transit to use, and for what duration ).Our approach must contend with the multiple significant challenges of our problem. It must plan over large timedependent transit networks, while accounting for energy constraints that limit the drones' flight ranges. It must avoid inter-drone conflicts, such as where more than one drone attempts to board the same vehicle at the same time, or when the maximum carrying capacity of a vehicle is exceeded.The authors are with Stanford University, CA, USA. Our multi-drone delivery framework plans for drones to piggyback on public transit vehicles while delivering packages from depots to the requested locations. Our framework is scalable and efficient, and minimizes the amount of time for any individual delivery.We seek not just feasible multi-agent plans but high-quality solutions in terms of a cumulative objective over all drones, the makespan, i.e., the maximum individual delivery time for any drone. Additionally, our approach must also solve the task allocation problem of determining which drones deliver which packages, and from which distribution centers.Some individual aspects of our problem have already been studied. Choudhury et al.  investigated the single-agent setting of controlling a drone to use multiple modes of transit en route to its destination. Recent work has considered pairing a drone with a delivery truck, which does not exploit public transit . The multi-agent issues of task allocation and inter-agent conflicts were not addressed either. Our problem is closely related to routing a fleet of autonomous vehicles providing mobility-on-demand services . Specifically, the task is to compute routes for the vehicles (both customer-carrying and empty) so that travel demand is fulfilled and operational cost is minimized. In particular, recent works study the combination of such service with public transit, where passengers can use several modes of transportation in the same trip . However, such works abstract away inter-agent constraints or dynamics and are not suited for autonomous pathfinding. The taskallocation setting we consider in our problem can be viewed as an instance of the vehicle routing problem , variants of which are typically solved by mixed integer linear programming (MILP) formulations that scale poorly, or by heuristics without optimality guarantees.We must contend with the challenges of planning for multiple agents. Accordingly, the second layer of our approach is a multi-agent path finding (MAPF) problem . Since the drones are on the same team, we have a centralized or cooperative pathfinding setting . The MAPF problem is NP-hard to solve optimally . A number arXiv:1909.11840v5 [cs.RO] 5 Jan 2021 of efficient solvers have been developed that work well in practice . The MAPF formulation and algorithms have been extended to several relevant scenarios such as lifelong pickup-and-delivery [33] and joint task assignment and pathfinding , though for different task settings and constraints than ours. Also, a MAPF formulation was applied for UAV traffic management in cities . However, none of the approaches considered pathfinding over large time-dependent transit networks. We use models, algorithms and techniques from transportation planning .We present a comprehensive algorithmic framework for large-scale multi-drone delivery in synergy with a ground transit network. Our approach strives to minimize the maximum time to complete any delivery. We decompose the highly challenging problem and solve it stage-wise with a two-layer approach. First, the upper layer assigns drones to package-delivery sequences with a task allocation algorithm. Then, the lower layer executes the allocation by periodically routing the fleet over the transit network.Algorithmically, we develop a new delivery sequence allocation method for the upper layer that obtains a near-optimal solution in polynomial runtime. For the lower layer, we extend techniques for multi-agent path finding that account for time-dependent transit networks and agent energy constraints to perform multi-drone routing. Experimentally, we present results supporting the efficiency of our approach on settings with up to 200 drones, 5000 packages, and transit networks of up to 8000 stops in San Francisco and the Washington DC area. Our framework can compute solutions within a few seconds (up to 2 minutes for the largest settings) on commodity hardware, and in our problem scenarios, drones can travel up to 450% of their flight range using transit.The following is the paper structure. We present an overall description of the two-layer approach in Section II, and then elaborate on each layer in Sections III and IV. We present experimental results on simulations in Section V, and conclude the paper with Section VI.We provide a high-level description of our formulation and approach to illustrate the various interacting components.We are operating a centralized homogeneous fleet of m drones within a city-scale domain. There are product depots with known geographic locations, denoted by V D := {d 1 , . . . , d } \u2282 R 2 . The depots are both product dispatch centers and drone-charging stations. At the start of a large time interval (e.g., a day), a batch of delivery request locations for k different packages, denoted V P := {p 1 , . . . , p k } \u2282 R 2 , is received (we assume that k m). We assume that any package can be dispatched from any depot; our framework exploits this property to optimize the solution quality in terms of makespan, i.e., the maximum execution time for any delivery. In Section III, we mention how our approach can accommodate dispatch constraints.The drones carry packages from depots to delivery locations. They can extend their effective travel range by using public transit vehicles in the area, which remain unaffected by the drones' actions. Our problem is to route drones to deliver all packages while minimizing makespan. A drone route consists of its current location and the sequence of depot and package locations to visit with a combination of flying and riding on transit. We characterize the drones' limited energy as a maximum flight distance constraint. A feasible solution must satisfy inter-drone constraints such as collision avoidance and transit vehicle capacity limits.Finally, we make some assumptions for our setting: a drone carries one package at a time, which is reasonable given state-of-the-art drone payloads ; drones are recharged upon visiting a depot in negligible time (e.g., a battery replacement); depots have unlimited drone capacity; the transit network is deterministic with respect to locations and vehicle travel times (we mention uncertainty in Section VI). We do account for the time-varying nature of the transit.In principle, we could frame the entire problem as a mixed integer linear program (MILP). However, for real-world problems (hundreds of drones; thousands of packages; large transit networks), even state-of-the-art MILP approaches are unlikely to scale. Moreover, even a simpler problem that ignores the interaction constraints is an instance of the notoriously challenging multi-depot vehicle routing problem . Thus, we decouple the problem into two distinct subproblems that we solve stage-wise in layers.The upper layer performs task allocation to determine which packages are delivered by which drone and in what order. It takes as input the known depot and package locations, and an estimate of the drone travel time between every pair of locations. It then solves a threefold allocation to minimize delivery makespan and assigns to each package (i) the dispatch depot and (ii) the delivery drone, and to each drone (iii) the order of package deliveries. To this end, we develop an efficient polynomial-time task-allocation algorithm that achieves a near-optimal makespan.The lower layer performs route planning for the drone fleet to execute the allocated delivery tasks. It generates detailed routes of drone locations in time and space and the transit vehicles used, while accounting for the timevarying transit network. It also ensures that (i) simultaneous transit boarding by multiple drones is avoided, (ii) no transit vehicle exceeds its drone-carrying capacity, and (iii) drone (battery) energy constraints are respected. We efficiently handle individual and inter-drone constraints by framing the routing problem as an extension of multi-agent path finding (MAPF) to transit networks. We adapt a scalable, bounded sub-optimal variant of a highly effective MAPF solver called Conflict-Based Search (CBS) [41] to solve the one-deliveryper-drone problem. Finally, we obtain routes for the sequence of deliveries in a receding-horizon fashion by replanning for the next task once a drone completes its current one.Decomposition-based stage-wise optimization approaches typically have an approximation gap compared to the optimal solution of the full problem. For us, this gap manifests in the surrogate cost estimate we use for the drone's travel time in the task-allocation layer (instead of jointly solving for allocation and multi-agent routing over transit networks, which is not feasible at scale). The better the surrogate, the more coupled the layers are, i.e., the better is the solution of the first stage for the second one. Such surrogates have a tradeoff between efficiency and approximation quality. An easy-to-compute travel time surrogate, for instance, is the drone's direct flight time between two locations (ignoring transit). However, that can be poor-quality when the drone requires transit for an out-of-range target. We use a surrogate that actually accounts for the transit network, at the expense of some modest preprocessing. We defer details to Appendix III, but the idea is to precompute the pairwise shortest travel times between locations spread around the city, over a representative snapshot of the transit network.We leverage our problem's structure to design a new algorithm called MERGESPLITTOURS for the task-allocation layer, which guarantees a near-optimal solution in polynomial time. The goal of this layer is to (i) distribute the set of packages V P among m agents, (ii) assign each package destination p \u2208 V P to a depot d \u2208 V D , and (iii) assign drones to a sequence of depot pickups and package deliveries. The objective is to minimize the maximum travel time among all agents over all three of the above components.Our problem can be cast as a special version of the m traveling salesman problem , which we call the m minimal visiting paths problem (m-MVP). We seek a set of m paths such that the makespan, i.e., the maximum travel time for any path, is minimized. We only need paths that start and end at (the same or different) depots, not tours. Our formulation is a special case of the the asymmetric variant, for a directed underlying graph, which is NP-hard even for m = 1 on general graphs  (although it is not known whether the specific instance of our problem is NP-hard as well). Moreover, the current best polynomialtime approximation  yields the fairly large approximation factor O(log n/ log log n), for a graph with n vertices. An additional challenge is the inability to assume the triangle inequality on our objective of travel times.A key element of m-MVP is the allocation graphis weighted according to an estimated travel time c uv from the location of u to that of v in the city. For every d \u2208 V D , p \u2208 V P we exclude the edge (d, p) from E A if it is impossible to reach p from d while using at most 1/2 of the flight range allowed (similarly for (p, d) edges). As we flagged in Section II-B, any dispatch constraints are modeled by excluding edges from the corresponding depot. We are now ready for the full definition of m-MVP: Definition 1. Given allocation graph G A , the m minimal visiting paths problem (m-MVP) consists of finding m paths P * 1:m on G A , such that (1) each path P * i starts at some depot d \u2208 V D and terminates at the same or different d \u2208 V D , (2) exactly one path visits each package p \u2208 V P , and (3) the maximum travel time of any of the paths is minimized.Solve MCT(G A ) to get t tours T := {T 1 , . . . , T t }; while |T| > 1 do Pick distinct tours T, T \u2208 T and depotsedges ; Split final tour T into m paths P 1 , . . . , P m , where LENGTH(P i ) is proportional to LENGTH(T )/m for each i (similar to ); Extend each P i to ensure it begins and ends at a depot; return P 1 , . . . , P m ; TABLE I: An integer programming formulation of the MCT problem.xuvd\u2208N + (p)x dp = d\u2208N \u2212 (p)wheredenote the in and out going neighborsLet OPT be the optimal makespan, i.e., OPT := max i\u2208[m] LENGTH(P * i ), where LENGTH(\u2022) denotes the total travel time along a given path or tour. We make three observations. First, if a path contains the sub-path We present our MERGESPLITTOURS algorithm for solving m-MVP (Algorithm 1); see a detailed description in Appendix I. A key step is generating an initial set of tours T by solving the minimal-connecting tours (MCT) problem (see ), which attempts to connect packages to depots within tours to minimize the total edge weight in eq. (1). The constraint in eq. (4) is that each package is connected to precisely one incoming and one outgoing edge from and to depots respectively. The final constraint in eq. (5) enforces inflow and outflow equality for every depot. Edges connecting packages can be used at most once, whereas edges connecting depots can be used multiple times. The solution to MCT is the assignment {x uv } (u,v)\u2208E A , i.e., which edges of G A are used and how many times. This assignment implicitly represents the desired collection of the tours T 1 , . . . , T t ; see Appendix I.All proofs from this secion are in Appendix I. The following theorem states that MERGESPLITTOURS is correct and that its makespan is close to optimal. whereThe key idea is that the total cost of the tours induced by the solution to MCT cannot exceed the total length of {P * 1 , . . . , P * m }. The MCT solution is then adapted to m paths with an additional overhead of \u03b1 + \u03b2 per path. When m |V P | (typically the case), \u03b1 and \u03b2 are small compared to OPT, making the bound tight. For instance, in our randomlygenerated scenarios in Section V-A, for m = 5 and k = 200, the approximation ratio max i\u2208[m] LENGTH(P i )/OPT = 1.09, and for m = 10, k = 500, the factor is 1.06.The computational bottleneck of the algorithm is MCT, while the other components can clearly be implemented polynomially in the input size. However, it suffices to solve a relaxed version of MCT to obtain the same integral solution. Lemma 1. The optimal solution to the fractional relaxation of MCT, in which x uv \u2208 [0, 1] for all u \u2208 V P \u2228 v \u2208 V P , and x uv \u2208 R + otherwise, yields the integer optimal solution.The lemma follows from casting MCT as the minimumcost circulation problem, for which the constraint matrix is totally unimodular . Therefore, MERGESPLITTOURS can be implemented in polynomial time.For each drone i \u2208 [m], the allocation layer yields a sequence of delivery tasksEach delivery sequence has one or more subsequences of dpd . The routeplanning layer treats each dpd subsequence as an individual drone task, i.e., leaving with the package from depot d, carrying it to package location p and returning to the (same or different) depot d , without exceeding the energy capacity. We seek an efficient and scalable method to obtain highquality (with respect to travel time) feasible paths, while using transit options to extend range, for m different drone dpd tasks simultaneously. The full set of delivery sequences can be satisfied by replanning when a drone finishes its current task and begins a new one; we discuss and compare two replanning strategies in Appendix IV. Thus, we formulate the problem of multi-drone routing to satisfy a set of delivery sequences as receding-horizon multi-agent path finding (MAPF) over transit networks. In this section, we describe the graph representation of our problem and present an efficient bounded sub-optimal algorithm.The problem of Multi-Agent Path Finding with Transit Networks (MAPF-TN) is the extension of standard MAPF to where agents can use one or more modes of transit in addition to moving. The incorporation of transit networks introduces additional challenges and underlying structure. The input to MAPF-TN is the set of m tasksIn Section III, the allocation graph G A only considered depots and packages, and edges between them. Here, G O also includes transit vertices, V T N = \u03c4 \u2208T R \u03c4 , where T is the set of trips, and each trip R \u03c4 = {(s 1 , t 1 ) . . .} is a sequence of timestamped stop locations (a given stop location may appear as several different nodes with distinct time-stamps). Similarly, we also use time-expanded versions ofThe edges are defined as follows: An edge e = (u, v) \u2208 E is a transit edge if u, v \u2208 V T N and are consecutive stops on the same trip R t . Any other edge is a flight edge. An edge is time-constrained if v \u2208 V T N and time-unconstrained otherwise. Every edge has three attributes: traversal time T , energy expended N , and capacity C. Since each vertex is associated with a location, v \u2212 u denotes the distance between them for a suitable metric. MAPF typically abstracts away agent dynamics; we have a simple model where drones move at constant speed \u03c3, and distance flown represents energy expended. Due to high graph density (drones can fly point-to-point between many stops), we do not explicitly enumerate edges but generate them on-the-fly during search.We now define the three attributes for E O . For timeconstrained edges, T (e) = v.t\u2212u.t is the difference between corresponding time-stamps (if u \u2208 V D \u222a V P , u.t is the chosen departure time), and for time-unconstrained edges, T (e) = v \u2212 u /\u03c3 is the time of direct flight. For flight edges, N (e) = v\u2212u (flight distance), and for transit edges, N (e) = 0. For transit edges, C(e) is bounded by the capacity of the vehicle, while for flight edges, C(e) = \u221e. Here, we assume that time-unconstrained flight in open space can be accommodated (thorougly examined in ).We now describe the remaining relevant MAPF-TN details. An individual path \u03c0 i for drone i from d i through p i to d i is feasible if the energy constraint e\u2208\u03c0i N (e) N is satisfied, whereN is the drone's maximum flight distance. In addition, the drone should be able to traverse the distance of a time-constrained flight edge in time, i.e., \u03c3 \u00d7For simplicity, we abstract away energy expenditure due to hovering in place by flying the drone at reduced speed to reach the transit just in time. Thus, the constraintN is only on the traversed distance. The cost of an individual path is the total traversal time, T (\u03c0 i ) = e\u2208\u03c0i T (e). A feasible solution \u03a0 = i=1:m \u03c0 i is a set of m individually feasible paths that does not violate any of the following two shared constraints (see ): (i) Boarding constraint, i.e., no two drones may board the same vehicle at the same stop; (ii) Capacity constraint, i.e., a transit edge e may not be used by more than C(e) drones. As with the allocation layer, the global objective for MAPF-TN is to minimize the solution makespan, argmin \u03a0 max \u03c0\u2208\u03a0 T (\u03c0), i.e., minimize the worst individual completion time.To tackle MAPF-TN, we modify the Conflict-Based Search (CBS) algorithm [41]. The multi-agent level of CBS identifies shared constraints and imposes corresponding path constraints on the single-agent level. The single-agent level computes optimal individual paths that respect all constraints. If individual paths conflict (i.e., violate a shared constraint), the multi-agent level adds further constraints to resolve the conflict, and invokes the single-agent level again, for the conflicting agents. In MAPF-TN, conflicts arise from boarding and capacity constraints. CBS obtains optimal multi-agent solutions without having to run (potentially significantly expensive) multi-agent searches. However, its performance can degrade heavily with many conflicts in which constraints are violated.  illustrates the generation and resolution of conflicts in our MAPF-TN problem.For scalability, we use a bounded sub-optimal variant of CBS called Enhanced CBS (ECBS), which achieves orders of magnitude speedups over CBS . ECBS uses bounded sub-optimal Focal Search [38] at both levels, instead of bestfirst A* . Focal search allows using an inadmissible heuristic that prioritizes efficiency. We now describe a crucial modification to ECBS required for MAPF-TN.Focal Weight-constrained Search: Unlike typical MAPF, the low-level graph search in MAPF-TN has a path-wide constraint (traversal distance) in addition to the objective function of traversal time. For the shortest path problems on graphs, adding a path-wide constraint makes it NP-hard . Several algorithms for constrained search require an explicit enumeration of the edges . We extend the A* for MultiConstraint Shortest Path (A*-MCSP) algorithm  (suitable for our implicit graph) to focal search (called Focal-MCSP). Focal-MCSP uses admissible heuristics on both objective and constraint and maintains only non-dominated paths to intermediate nodes. This extensive book-keeping requires a careful implementation for efficiency.Focal-MCSP inherits the properties of A*-MCSP and Focal Search; therefore, it yields a bounded-suboptimal feasible path to the target. Accordingly, ECBS with Focal-MCSP yields a bounded sub-optimal solution to MAPF-TN. The result follows from the analysis of ECBS . Also, note that a dpd path requires a bounded sub-optimal path from d to p and another from p to d , such that their concatenation is feasible. Since this is even more complicated, in practice, we run Focal-MCSP twice (from d to p and p to d ) with half the energy constraint each time and concatenate the paths, guaranteeing feasibility. In Appendix II-B we discuss other required modifications to standard MAPF and important speedup techniques that nonetheless retain the bounded suboptimality of Enhanced CBS for our MAPF-TN formulation.We implemented our approach using the Julia language and tested it on a machine with a 6-core 3.7 GHz 16 GiB RAM CPU.  For very large combinatorial optimization problems, solution quality and algorithm efficiency are of interest. We have already shown that the upper and lower layers are near-optimal and bounded-suboptimal respectively in terms of solution quality, i.e., makespan. Therefore, for evaluation we focus on their efficiency and scalability to large realworld settings. We do not attempt to baseline against a MILP approach for the full problem; we estimate that a typical setting of interest will have on the order of 10 7 variables in a MILP formulation, besides exponential constraints.We ran simulations with two large-scale public transit networks in San Francisco (SFMTA) and the Washington Metropolitan Area (WMATA). We used the open-source General Transit Feed Specification data  for each network. We considered only the bus network (by far the most extensive), but our formulation can accommodate multiple modes. We defined a geographical bounding box in each case, of area 150 km 2 for SFMTA and 400 km 2 for WMATA (illustrated in Appendix IV), within which depots and package locations were randomly generated. For the transit network, we considered all bus trips that operate within the bounding box. The size of the time-expanded network, |V T N |, is the total number of stops made by all trips; |V T N | = 4192 for SFMTA and |V T N | = 7608 for WMATA (recall that edges are implicit, so |E T N | varies between queries, but the full graph G O can be dense). The drone's flight range constraint is set (conservatively) to 7 km and average speed to 25 kph, based on the DJI Mavic 2 specifications . In this section, we evaluate the two main components -the task allocation and multi-agent path finding layers. In Appendix IV we compare the performance of two replanning strategies for III: (All times are in seconds) An extensive analysis of the MAPF-TN layer, on 100 trials for each setting of depots and agents (and 30 trials for 5 depots and 50 agents). Each trial uses different randomly generated depots and delivery locations. The integer carrying capacity of any transit edge C(e) was randomly chosen from {3, 4, 5} (single and double-buses). The sub-optimality factor for ECBS was 1.1. For settings with m / = 10, a number of trials timed out (over 180 s) and were discarded. when a drone finishes its current delivery, and two surrogate travel time estimates for coupling the layers.The scale of the allocation problem is determined by the number of depots and packages, i.e., + k. The runtimes for MERGESPLITTOURS with varying , k over SFMTA are displayed in . The roughly quadratic increase in runtimes along a specific row or column demonstrate that our provably near-optimal MERGESPLITTOURS algorithm is indeed polynomial in the size of the input. Even for up to 5000 deliveries, the absolute runtimes are quite reasonable. We do not compare with naive MILP even for allocation, as the number of variables would exceed ( \u2022 k) 2 , in addition to the expensive subtour elimination constraints .Solving multi-agent path finding optimally is NPhard . Previous research has benchmarked CBS variants and shown that Enhanced CBS is most effective . Therefore, we focus on extensively evaluating our own approach rather than redundant baselining.  quantifies several aspects of the MAPF-TN layer with varying numbers of depots ( ) and agents (m), the two most tunable parameters. Before each trial, we run the allocation layer and collect m dpd tasks, one for each agent. We then run the MAPF-TN solver on this set of tasks to compute a solution.We discuss broad observations here and provide a detailed analysis in Appendix IV. The results are very promising; our approach scales to large numbers of agents (200) and large transit networks (nearly 8000 vertices); the highest average makespan for the true delivery time is less than an hour (3380.9 s) for SFMTA and 2 hours (6140.2 s) for WMATA; drones are using up to 9 transit options per route to extend their range by up to 3.6x. As we anticipated, conflict resolution is a major bottleneck of MAPF-TN. A higher ratio of agents to depots increases conflicts due to shared transit, thereby increasing plan time (compare {5, 20} to {10, 20}). A higher number of depots puts more deliveries within flight range of a depot, reducing conflicts, makespan, and the need for transit usage and range extension (compare {10, 50} to {20, 50}). Plan times are much higher for WMATA due to a larger area and a larger and less uniformly distributed bus network, leading to higher single-agent search times and more multi-agent conflicts. Trials taking more than 3 minutes were discarded; two pathological cases with SFMATA and WMATA (each with {l = 10, m = 100}) took nearly 4 and 8 minutes, due to 30 and 10 conflicts respectively. In any case, a deployed system would have better compute and parallelized implementations. Finally, note that the running times reported here are actually pessimistic, because we consider cases where drones are released simultaneously from the depots, which increases conflicts. However, a gradual release by executing the MAPF solver over a longer horizon (as we discuss in Appendix IV-B) results in fewer conflicts, allowing us to cope with an even larger drone fleet.We designed a comprehensive algorithmic framework for solving the highly challenging problem of multi-drone package delivery with routing over transit networks. Our twolayer approach is efficient and highly scalable to large problem settings and obtains high-quality solutions that satisfy the many system constraints. We ran extensive simulations with two real-world transit networks that demonstrated the widespread applicability of our framework and how using ground transit judiciously allows drones to significantly extend their effective range.A key future direction is to perform case studies that estimate the operational cost of our framework, evaluate its impact on road congestion, and consider potential externalities like noise pollution and disparate impact on urban communities. Another direction is to extend our model to overcome its limitations: delays and uncertainty in the travel pattern of transit vehicles [35] and delivery time windows [43]; jointly routing ground vehicles and drones; optimizing for the placements of depots, whose locations are currently randomly generated and given as input. ", "videoStruct": []}, {"title": "X-Caps: Encoding Visual Attributes in Capsules for Explainable Medical Diagnoses | MICCAI 2020", "authors": "Rodney LaLonde", "abstract": "", "publicationOrg": "MICCAI", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/1909.05926.pdf", "pdfPath": "/data/cache/2/PDFs/XCapsEncodingVisualAttributesinCapsulesforExplainableMedicalDiagnosesMICCAI2020.pdf", "publicationUrl": "https://arxiv.org/pdf/1909.05926.pdf", "codeUrl": "https://github.com/lalonderodney/X-Caps", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/kJ4SLLJO0GM", "videoPath": "/data/cache/2/videos/X-Caps- Encoding Visual Attributes in Capsules for Explainable Medical Diagnoses - MICCAI 2020.mp4", "pdfText": "In machine learning, predictive performance typically comes at the cost of interpretability . While deep learning (DL) has impact many fields, there exist several high-risk domains which have yet to be comparably affected: military, security, transportation, finance, legal, and healthcare among others, often citing a lack of interpretability as the main concern . As features become less interpretable, and the functions learned more complex, model predictions become more difficult to explain . While some works have begun to press towards this goal of explainable DL, the problem remains largely unsolved.Interpretable vs. Explainable: There has been a recent push in the community to move away from the post-hoc interpretations of deep models and instead create explainable models from the outset . Since the terms interpretable and explainable are often used interchangeably, we want to be explicit about our definitions for the purposes of this study. An explainable model is one which provides explanations for its predictions at the human level for a specific task. An interpretable model is one for which some conclusions can be drawn about the internals/predictions of the model; however, they are not explicitly provided by the model and are typically at a lower level. For example, in image classification, when a deep model predicts an image to be of a cat, saliency/gradient or other methods can attempt to interpret the model/prediction. However, the model is not explaining why the object in the image is a cat in the same way as a human. Humans classify objects based on a taxonomy of characteristics/attributes (e.g. cat equals four legs, paws, whiskers, fur, etc.). If our goal is to create explainable models, we should design models which explain their decisions using a similar set of \"attributes\" to humans.(a) Lung nodules with high-level visual attribute scores as determined by expert radiologists. Scores were given from 1 -5 for six different visual attributes related to diagnosing lung cancer.(b) A symbolic plot showing the general tradeoff between explainability and predictive performance in deep learning (DL) . Our proposed X-Caps rebuts the trend of decreasing performance from state-of-the-art (SotA) as explainability increases and shows it is possible to create more explainable models and increase predictive performance with capsule networks. Why capsule networks? Capsule networks differ from convolutional neural networks (CNNs) by replacing the scalar feature maps with vectorized representations, responsible for encoding information (e.g. pose, scale, color) about each feature. These vectors are then used in a dynamic routing algorithm which seeks to maximize the agreement between lower-level predictions for the instantiation parameters (i.e. capsule vectors) of higher-level features. In their introductory work, a capsule network (CapsNet) was shown to produce promising results on the MNIST data set; but more importantly, was able to encode high-level visually-interpretable features of digits (e.g. stroke thickness, skew, localizedparts) within the dimensions of its capsule vectors .Lung cancer diagnosis with a multi-task capsule network: In diagnosing the malignancy of lung nodules, similar to describing why an image of a cat is catlike, radiologists explain their predictions through the language of high-level visual attributes (i.e., radiographical interpretations): subtlety (sub), sphericity (sph), margin (mar), lobulation (lob), spiculation (spi), and texture (tex), shown in , which are known to be predictive (with inherent uncertainty) of malignancy . To create a DL model with this same level of radiographical interpretation, we propose a novel multi-task capsule architecture, called X-Caps, for learning visually-interpretable feature representations within capsule vectors, then predicting malignancy based solely on these interpretable features. By supervising different capsules to embed specific visually-interpretable features, multiple visual attributes are learned simultaneously, with their weights being updated by both the radiologists visual interpretation scores as well as their contribution to the final malignancy score, regularized by the segmentation reconstruction error. Since these attributes are not mutually-exclusive, we introduce a new routing sigmoid function to independently route child capsules to parents. Further, to provide radiologists with an estimate of model confidence, we train our network on a distribution of expert labels, modeling inter-observer agreement and punishing over/under confidence during training, supervised by human-experts' agreement.We show even a relatively simple 2D capsule network can better capture high-level visual attribute information than the state-of-the-art deep dual-path dense 3D convolutional neural network (CNN) while also improving diagnostic accuracy, approaching that of even some black-box methods (e.g., ). X-Caps simultaneously learns attribute and malignancy scores from a multi-center dataset of over 1000 CT scans of lung cancer screening patients. Overall, the contributions of this study are summarized as:1. The first study to directly encode high-level visual attributes within the vectors of a capsule network to perform explainable image-based diagnosis at the radiologist-level. 2. Create a novel modification to the dynamic routing algorithm to independently route information from child capsules to parents when parent capsules are not mutually-exclusive.ing directly from expert label distributions to punish network over/under confidence. Visual attribute predictions are verified at test via the reconstruction branch of the network. 4. Demonstrate a simple 2D capsule network (X-Caps) trained from scratch outperforming a state-of-the-art deep pre-trained dense dual-path 3D CNN at capturing visually-interpretable high-level attributes and malignancy prediction, while providing malignancy prediction scores approaching that of non-explainable 3D CNNs.The majority of work in explainable deep learning has focused around post hoc deconstruction of already trained models (i.e. interpretation). These approaches typically rely on human-experts to examine their results and attempt to discover meaningful patterns. Zeiler and Fergus  attached a deconvolutional network to network layers to map activations back to pixel space for visualizing individual filters and activation maps, while also running an occlusion-based study of which parts of the input contribute most to the final predictions. Grad-CAM  is a popular approach which highlights the relative positive activation map of convolutional layers with respect to network outputs. InfoGAN  separates noise from the \"latent code\", maximizing the mutual information between the latent representations and the image inputs, encoding concepts such as rotation, width, and digit type for MNIST. In a similar way, capsule networks encode visuallyinterpretable concepts such as stroke thickness, skew, rotation, and others .A number of recent studies have proposed using CapsNet for a variety of medical imaging classification tasks . However, these methods nearly all follow the exact CapsNet architecture, or propose minor modifications which present nearly identical predictive performance ; hence, it is sufficient to compare only with CapsNet in reference to these works.In the area of lung nodule malignancy, many DL-based approaches have been proposed , with further methods being developed with complicated postprocessing techniques , curriculum learning methods , or gradient-boosting machines . However, adding such techniques is beyond the scope of this study and would lead to an unwieldy enumeration of ablation studies necessary to understand the contributions between our proposed capsule architecture and such techniques. For a fair comparison in this study, we compare our method directly against CapsNet and explainable CNN approaches. HSCNN  creates one of the first explainable methods, by designing a dense 3D CNN which first predicts visual attribute scores then predicts malignancy from those features. This decreased the overall performance as compared to other 3D networks  but provided some explanations for the final malignancy predictions.Our approach, referred to as explainable capsules, or X-Caps, was designed to remain as similar as possible to our control network, CapsNet, while allowing us to have more control over the visually-interpretable features learned. CapsNet already showed great promise when trained on the MNIST data set for its ability to model high-level visually-interpretable features. With this study, we examine the ability of capsules to model specific visual attributes within their vectors, rather that simply hoping these are learned successfully in the more challenging lung nodule data. As shown in , X-Caps shares a similar overall structure as CapsNet, with the major differences being the addition of the supervised labels for each of the X-Caps vectors, the fully-connected layer for malignancy prediction, the reconstruction regularization also performing segmentation, and the modifications to the dynamic routing algorithm. The first layer of our proposed network is a 2D convolutional layer which extracts low-level features. Next, we form our primary capsules of 32 capsule types with 8D vector capsules. Following this, we form our attribute capsules using a fully-connected capsule layer whose output is N 16D capsule types, one for each of the visual-attributes we want to predict. Unlike CapsNet where each of the parent capsules were dependant on one another (e.g. if the prediction is the digit 5 it cannot also be a 3), our parent capsules are not mutually-exclusive of each other (i.e. a nodule can score high or low in each of the attribute categories). For this reason, we needed to modify the dynamic routing algorithm presented in CapsNet to accommodate this significant difference. The key change is the \"routing softmax\" employed by CapsNet forces the contributions of each child to send their information to parents in a manner which sums to one, which in practice effectively makes them \"choose\" a parent to send their information to. However, when computing prediction vectors for independent parents, we want a child to be able to contribute to all parent capsules for attributes which are present in the given input. With that motivation, the specific algorithm, which we call \"routing sigmoid\", is computed aswhere r i,j are the routing coefficients determined by the dynamic routing algorithm for child capsule i to parent capsule j and the initial logits, b i,j are the prior probabilities that the prediction vector for capsule i should be routed to parent capsule j. Note the prior probabilities are initially set to 1 rather than 0 as in CapsNet, otherwise no routing could take place. The rest of the dynamic routing procedure follows the same as in .Predicting malignancy from visually-interpretable capsule vectors: In order to predict malignancy scores, we attach a fully-connected layer to our attribute capsules with output size equal to the range of scores. We wish to emphasize here, our final malignancy prediction is coming solely from the vectors whose magnitudes represent visually-interpretable feature scores. Every malignancy prediction score has a set of weights connected to the high-level attribute capsule vectors, and the activation from each tells us the exact contribution of the given visual attribute to the final malignancy prediction for that nodule. Unlike previous studies which look at the importance of these attributes on a global level, our method looks at the importance of each visual attribute in relation to a specific nodule being diagnosed. To verify the correctness of our attribute modeling, we reconstruct the nodules while varying the dimensions of the capsule vectors to ensure the desired visual attributes are being modeled. At test, these reconstructions give confidence that the network is properly capturing the attributes, and thus the scores can be trusted. Confidence in the malignancy prediction score, in addition to coming solely from these trusted attributes, is provided via an uncertainty modeling approach.Previous works in lung nodule classification follow the same strategy of averaging radiologists' scores for visual attributes and malignancy, and then either attempt to regress this average or performing binary classification of the average as below or above 3. To better model the uncertainty inherently present in the labels due to inter-observer variation, we propose a different approach: we attempt to predict the distribution of radiologists' scores. Specifically, for a given nodule where we have at minimum three radiologists' score values for each attribute and for malignancy prediction, we compute the mean and variance of those values and fit a Gaussian function to them, which is in turn used as the ground-truth for our classification vector. Nodules with strong inter-observer agreement produce a sharp peak, in which case wrong or unsure (i.e., low confidence score) predictions are severely punished. Likewise, for low inter-observer agreement nodules, we expect our network to output a more spread distribution and it will be punished for strongly predicting a single class label. This proposed approach allows us to model the uncertainty present in radiologists' labels in a way that no previous study has and provide a meaningful confidence metric at test time to radiologists.As in CapsNet, we also perform reconstruction of the input as a form of regularization. However, we extend the idea of regularization to perform a pseudo-segmentation, similar in nature to the reconstruction used by . Whereas in true segmentation, the goal is to output a binary mask of pixels which belong to the nodule region, in our formulation we attempt to reconstruct only the pixels which belong to the nodule region, while the rest are mapped to zero. More specifically, we formulate this loss aswhere L r is the supervised loss for the reconstruction regularization, \u03b3 is a weighting coefficient for the reconstruction loss, R x,y is the reconstruction target pixel, S x,y is the ground-truth segmentation mask value, and O x,y r is the output of the reconstruction network, at pixel location (x, y), respectively, and H and W are the height and width, respectively, of the input image. This adds another task to our multi-task approach and an additional supervisory signal which can help our network distinguish visual characteristics from background noise. The malignancy prediction score, as well as each of the visual attribute scores also provide a supervisory signal in the form ofwhere L a is the combined loss for the visual attributes, A n is the average of the attribute scores given by at minimum three radiologists for attribute n, N is the total number of attributes, \u03b1 n is the weighting coefficient placed on the n th attribute, O n a is the network prediction for the score of the n th attribute, L m is a KL divergence loss for the malignancy score, \u03b2 is the weighting coefficient for the malignancy score, \u00b5 and \u03c3 are the mean and variance of radiologists' scores, andis the softmax over the network malignancy prediction vector O m = {O 1 m , ..., O N m }. In this way, the overall loss for X-Caps is simply L = L m + L a + L r . For simplicity, the values of each \u03b1 n and \u03b2 are set to 1, and \u03b3 is set to 0.005 \u00d7 32 \u00d7 32 = 0.512.Experiments we performed on the LIDC-IDRI data set . Five-fold stratified cross-validation was performed, with 10% of each training set used for validation and early stopping. X-Caps was trained with a batch size of 16 using Adam with an initial learning rate of 0.02 reduced by a factor of 0.1 after validation loss plateau. Consistent with the literature, nodules of mean radiologists' score 3 were removed (leaving 646 benign and 503 malignant nodules) and predictions were considered correct if within \u00b11 of the radiologists' classification . The results summarized in  illustrate the prediction of visual attributes with the proposed X-Caps in comparison with an adapted version of CapsNet, a deep dense dual-path 3D explainable CNN (HSCNN ), and two state-of-the-art non-explainable methods which do not have extra post-processing or learning strategies. Compared methods results are from the original reported works. : Prediction accuracy of visual attributes with capsule networks. Dashes (-) represent values which the given method could not produce. X-Caps outperforms the state-of-the-art explainable method (HSCNN ) at attribute modeling (the main goal of both studies), while also producing higher malignancy prediction scores, approaching state-of-the-art non-explainable methods performance. Our results show that a X-Caps has the ability to model visual attributes far better than HSCNN while also achieving better malignancy prediction. Further, we wish to emphasize the significance of X-Caps providing increased predictive performance and explainability over CapsNet. This goes against the assumed trend in DL, illustrated with a symbolic plot in , that explainability comes at the cost of predictive performance, a trend we observe with HSCNN being outperformed by less powerful (i.e. not dense or dual-path) but nonexplainable 3D CNNs . While X-Caps slightly under-performs the best non-explainable models, it is reasonable to suspect that future research into more powerful 3D capsule networks would allow explainable capsules to surpass these methods; we hope this study will promote such future works.As two limitations of our work, we did not tune the weight balancing terms between the different tasks and further investigation could lead to superior performance. Also, we found capsule networks can be somewhat fragile; often random initializations failed to converge to good performance. However, this might be due to the small/shallow network size and its relation to the Lottery Ticket Hypothesis  rather than anything specific to capsules.Ablation studies: To analyze the impact of each component of our proposed approach, we performed ablation studies for: (1) learning the distribution of radiologists' scores rather than attempting to regress the mean value of these scores, (2) removing the reconstruction regularization from the network, and (3) performing our proposed \"routing sigmoid\" over the original \"routing softmax\" proposed in . The malignancy prediction accuracy for each of these ablations is (1) 83.09%, (2) 80.30%, and (3) 80.69%, respectively, as compared to the proposed model's accuracy of 86.39%. This shows retaining the agreement/disagreement information among radiologists proved useful, the reconstruction played a role in improving the network performance, and our proposed modifications to the dynamic routing algorithm were necessary for passing information from children to parents when the parent capsule types are independent.Available studies for explaining DL models, typically focus on post hoc interpretations of trained networks, rather than attempting to build-in explainability. This is the first study for directly learning an interpretable feature space by encoding high-level visual attributes within the vectors of a capsule network to perform explainable image-based diagnosis. We approximate visually-interpretable attributes through individual capsule types, then predict malignancy scores directly based only on these high-level attribute capsule vectors, in order to provide malignancy predictions with explanations at the human-level, in the same language used by radiologists. Our proposed multi-task explainable capsule network, X-Caps, successfully approximated visual attribute scores better than the previous state-of-the-art explainable diagnosis system, while also achieving higher diagnostic accuracy. We hope our work can provide radiologists with malignancy predictions which are explained via the same high-level visual attributes they currently use, while also providing a meaningful confidence metric to advise when the results can be more trusted, thus allowing radiologists to quickly interpret and verify our predictions. Lastly, we believe our approach should be applicable to any image-based classification task where high-level attribute information is available to provide explanations about the final prediction.", "videoStruct": [{"timeStart": "00-00-01", "timeEnd": "00-00-08", "sentence": "thank you for turning into my talk on encoding visual attributes and capsules for explainable medical diagnoses"}, {"timeStart": "00-00-08", "timeEnd": "00-00-10", "sentence": "presented here at MCC I twenty twenty"}, {"timeStart": "00-00-10", "timeEnd": "00-00-14", "sentence": "with my coauthors doctors through Troy and last box"}, {"timeStart": "00-00-16", "timeEnd": "00-00-22", "sentence": "this work is motivated by the immediate need for explain ability and medical imaging diagnosis"}, {"timeStart": "00-00-22", "timeEnd": "00-00-29", "sentence": "evidenced by the fact that CNN based cad systems have largely not been adopted into routine clinical workflows"}, {"timeStart": "00-00-29", "timeEnd": "00-00-33", "sentence": "despite their performance often significantly exceeding that of human doctors"}, {"timeStart": "00-00-34", "timeEnd": "00-00-37", "sentence": "this is not limited to the healthcare domain either"}, {"timeStart": "00-00-37", "timeEnd": "00-00-43", "sentence": "a similar hesitancy has seen in many high risk applications including military security and more"}, {"timeStart": "00-00-43", "timeEnd": "00-00-48", "sentence": "every time this reluctance of adoption is sited back to a lack of trust"}, {"timeStart": "00-00-48", "timeEnd": "00-00-54", "sentence": "caused by the highly an interpret ubl nature of so-called black box CN models"}, {"timeStart": "00-00-56", "timeEnd": "00-01-02", "sentence": "in machine learning there is a known tradeoff between model interpret ability and predicted performance"}, {"timeStart": "00-01-02", "timeEnd": "00-01-05", "sentence": "while some work has been done to make cnn's explainable"}, {"timeStart": "00-01-05", "timeEnd": "00-01-08", "sentence": "their success has been largely limited"}, {"timeStart": "00-01-08", "timeEnd": "00-01-11", "sentence": "and typically comes at the cost of worse and performance"}, {"timeStart": "00-01-11", "timeEnd": "00-01-15", "sentence": "capsule networks on the other hand have shown an innate ability"}, {"timeStart": "00-01-15", "timeEnd": "00-01-18", "sentence": "to encode visually interpret ubl features"}, {"timeStart": "00-01-18", "timeEnd": "00-01-20", "sentence": "without suffering from degraded performance"}, {"timeStart": "00-01-21", "timeEnd": "00-01-26", "sentence": "capsule networks aren't attractive potential solution to many shortcomings in CNS"}, {"timeStart": "00-01-26", "timeEnd": "00-01-29", "sentence": "where a capsule network is comprised of capsules"}, {"timeStart": "00-01-29", "timeEnd": "00-01-33", "sentence": "which are defined as a group of neurons representing a futures presence"}, {"timeStart": "00-01-33", "timeEnd": "00-01-35", "sentence": "and more importantly"}, {"timeStart": "00-01-35", "timeEnd": "00-01-36", "sentence": "the attributes of that feature"}, {"timeStart": "00-01-36", "timeEnd": "00-01-39", "sentence": "often called the features instantiation parameters"}, {"timeStart": "00-01-40", "timeEnd": "00-01-43", "sentence": "there's only requires two simple changes from"}, {"timeStart": "00-01-43", "timeEnd": "00-01-45", "sentence": "standard CNS"}, {"timeStart": "00-01-46", "timeEnd": "00-01-49", "sentence": "features are now represented by vectors or matrices"}, {"timeStart": "00-01-49", "timeEnd": "00-01-51", "sentence": "rather than scales"}, {"timeStart": "00-01-51", "timeEnd": "00-01-55", "sentence": "and this allows us to store important information about the features being learned"}, {"timeStart": "00-01-56", "timeEnd": "00-01-59", "sentence": "we're out this information from one layer to the next"}, {"timeStart": "00-01-59", "timeEnd": "00-02-01", "sentence": "via a dynamic routing algorithm"}, {"timeStart": "00-02-01", "timeEnd": "00-02-05", "sentence": "which weights both the present and internal relationships"}, {"timeStart": "00-02-05", "timeEnd": "00-02-09", "sentence": "of the lower level features with respect to each higher level combination"}, {"timeStart": "00-02-10", "timeEnd": "00-02-15", "sentence": "with that motivation before we move on discuss our proposed explainable capsule network"}, {"timeStart": "00-02-15", "timeEnd": "00-02-20", "sentence": "I need to take a brief aside to distinguish between model and target ability"}, {"timeStart": "00-02-20", "timeEnd": "00-02-21", "sentence": "unexplainable predictions"}, {"timeStart": "00-02-21", "timeEnd": "00-02-24", "sentence": "since these terms are often used interchangeably"}, {"timeStart": "00-02-24", "timeEnd": "00-02-26", "sentence": "in the context of this work"}, {"timeStart": "00-02-26", "timeEnd": "00-02-32", "sentence": "interpret ability deals with the post hoc analysis of the inner workings of a model"}, {"timeStart": "00-02-32", "timeEnd": "00-02-34", "sentence": "in relation to its predictions"}, {"timeStart": "00-02-34", "timeEnd": "00-02-38", "sentence": "such as grad cam and Sam saliency maps"}, {"timeStart": "00-02-38", "timeEnd": "00-02-41", "sentence": "or blacking out parts of the input to see how it changes the output"}, {"timeStart": "00-02-41", "timeEnd": "00-02-45", "sentence": "explainable methods on the other hand explicitly provide"}, {"timeStart": "00-02-45", "timeEnd": "00-02-48", "sentence": "explanations for their predictions"}, {"timeStart": "00-02-49", "timeEnd": "00-02-53", "sentence": "the argument for explainable predictions over post hoc interpretations"}, {"timeStart": "00-02-54", "timeEnd": "00-02-58", "sentence": "instead of a model predicting this is a picture of a cat"}, {"timeStart": "00-02-58", "timeEnd": "00-03-02", "sentence": "and a researcher trying to break down the neural activation patterns of"}, {"timeStart": "00-03-02", "timeEnd": "00-03-05", "sentence": "what parts of the image are stimulating which parts of the network"}, {"timeStart": "00-03-05", "timeEnd": "00-03-08", "sentence": "what endusers would really prefer"}, {"timeStart": "00-03-08", "timeEnd": "00-03-11", "sentence": "is for the model to explain its predictions"}, {"timeStart": "00-03-11", "timeEnd": "00-03-13", "sentence": "just as a human would"}, {"timeStart": "00-03-14", "timeEnd": "00-03-16", "sentence": "human level explain ability"}, {"timeStart": "00-03-16", "timeEnd": "00-03-19", "sentence": "when asking why is this a cat"}, {"timeStart": "00-03-19", "timeEnd": "00-03-25", "sentence": "a human would not vaguely point to regions of the input are parts of the brain"}, {"timeStart": "00-03-25", "timeEnd": "00-03-26", "sentence": "they would answer"}, {"timeStart": "00-03-27", "timeEnd": "00-03-31", "sentence": "because it has fur and whiskers and claws"}, {"timeStart": "00-03-31", "timeEnd": "00-03-34", "sentence": "humans explain their classifications of objects"}, {"timeStart": "00-03-34", "timeEnd": "00-03-38", "sentence": "based on a text sodomy of object attributes"}, {"timeStart": "00-03-38", "timeEnd": "00-03-42", "sentence": "and if we want our models to be explainable at the human level"}, {"timeStart": "00-03-42", "timeEnd": "00-03-46", "sentence": "they should provide end users with these same kinds of explanations"}, {"timeStart": "00-03-47", "timeEnd": "00-03-49", "sentence": "and that praise is to our research question"}, {"timeStart": "00-03-49", "timeEnd": "00-03-55", "sentence": "can we build a capsule network to model specific visually interpret ubl object attributes"}, {"timeStart": "00-03-55", "timeEnd": "00-03-56", "sentence": "and form predictions"}, {"timeStart": "00-03-56", "timeEnd": "00-03-58", "sentence": "they solely on their combination"}, {"timeStart": "00-03-59", "timeEnd": "00-04-04", "sentence": "as an application of this research we chose to focus on lung cancer diagnosis"}, {"timeStart": "00-04-04", "timeEnd": "00-04-08", "sentence": "lung cancer is a perfect application within medical imaging diagnosis"}, {"timeStart": "00-04-08", "timeEnd": "00-04-11", "sentence": "because radiologists already explained their predictions"}, {"timeStart": "00-04-11", "timeEnd": "00-04-17", "sentence": "for nodule malignancy based on a text on me of attributes shown here at right"}, {"timeStart": "00-04-19", "timeEnd": "00-04-24", "sentence": "to solve this problem we propose an explainable multitask capsule network"}, {"timeStart": "00-04-24", "timeEnd": "00-04-30", "sentence": "an object in this case of lung nodule is input to our three layer two d capsule network"}, {"timeStart": "00-04-30", "timeEnd": "00-04-32", "sentence": "to form a tribute"}, {"timeStart": "00-04-32", "timeEnd": "00-04-33", "sentence": "prediction capsule vectors"}, {"timeStart": "00-04-34", "timeEnd": "00-04-37", "sentence": "each of these vectors is supervised to encode"}, {"timeStart": "00-04-37", "timeEnd": "00-04-41", "sentence": "Pacific visually interpret ubl attribute of the target object"}, {"timeStart": "00-04-41", "timeEnd": "00-04-43", "sentence": "where the dimensions of each vector"}, {"timeStart": "00-04-43", "timeEnd": "00-04-48", "sentence": "capture the possible variations of that attribute over the dataset"}, {"timeStart": "00-04-48", "timeEnd": "00-04-52", "sentence": "and the magnitude of the vector represents the attributes present"}, {"timeStart": "00-04-52", "timeEnd": "00-04-53", "sentence": "or in our case its score"}, {"timeStart": "00-04-54", "timeEnd": "00-04-59", "sentence": "we predict the nodules malignancy by passing these visually interpret ubl capsules"}, {"timeStart": "00-04-59", "timeEnd": "00-05-01", "sentence": "through a linear function"}, {"timeStart": "00-05-01", "timeEnd": "00-05-03", "sentence": "and apply a soft max activation"}, {"timeStart": "00-05-03", "timeEnd": "00-05-07", "sentence": "the create a probability distribution over malignancy scores"}, {"timeStart": "00-05-07", "timeEnd": "00-05-11", "sentence": "while also passing them to a reconstruction branch to perform regular is a Sion"}, {"timeStart": "00-05-13", "timeEnd": "00-05-15", "sentence": "for creating these afternoon capsules"}, {"timeStart": "00-05-15", "timeEnd": "00-05-18", "sentence": "unlike in cap snap where parents were mutually exclusive"}, {"timeStart": "00-05-18", "timeEnd": "00-05-23", "sentence": "for example if a class prediction is the digit five it can't also be a three"}, {"timeStart": "00-05-23", "timeEnd": "00-05-27", "sentence": "our parent capsules are not mutually exclusive of each other"}, {"timeStart": "00-05-27", "timeEnd": "00-05-31", "sentence": "where nodule can score high or low in each of the attribute categories"}, {"timeStart": "00-05-31", "timeEnd": "00-05-38", "sentence": "for this reason we modified the dynamic routing algorithm to independently route information from children a parent"}, {"timeStart": "00-05-38", "timeEnd": "00-05-40", "sentence": "three routing sigmoid function"}, {"timeStart": "00-05-40", "timeEnd": "00-05-47", "sentence": "where the original routing soft max employed by cap snap and force is a one hot mapping of information from children to parents"}, {"timeStart": "00-05-47", "timeEnd": "00-05-50", "sentence": "our proposed routing sigmoid"}, {"timeStart": "00-05-50", "timeEnd": "00-05-54", "sentence": "learns a non mutually exclusive relationship between children and parents"}, {"timeStart": "00-05-54", "timeEnd": "00-05-57", "sentence": "to allow multiple children to be emphasized"}, {"timeStart": "00-05-59", "timeEnd": "00-06-03", "sentence": "while the rest of the dynamic routing procedure follows the same as in cap snap"}, {"timeStart": "00-06-05", "timeEnd": "00-06-10", "sentence": "for create technically and lung nodule classification data sets"}, {"timeStart": "00-06-10", "timeEnd": "00-06-15", "sentence": "a minimum of three radiologists provide their scores on a scale of one to five for natural malignancy"}, {"timeStart": "00-06-25", "timeEnd": "00-06-31", "sentence": "however such approaches throw away valuable information about the agreement or disagreement amongst experts"}, {"timeStart": "00-06-31", "timeEnd": "00-06-37", "sentence": "to better model the uncertainty inherently present in the labels due to inter observer variation"}, {"timeStart": "00-06-37", "timeEnd": "00-06-41", "sentence": "we propose to directly predict the distribution of radiologists corps"}, {"timeStart": "00-06-41", "timeEnd": "00-06-44", "sentence": "by filling a gaussian function to the mean and variance"}, {"timeStart": "00-06-44", "timeEnd": "00-06-46", "sentence": "as the ground truth for our classification vector"}, {"timeStart": "00-06-47", "timeEnd": "00-06-51", "sentence": "this allows us to model the uncertainty present in radiologist labels"}, {"timeStart": "00-06-51", "timeEnd": "00-06-55", "sentence": "and provide a meaningful confident metric at test time to radiologists"}, {"timeStart": "00-06-55", "timeEnd": "00-07-01", "sentence": "nodules was strong inter observer agreement will produce a sharp peak as the ground truth during training"}, {"timeStart": "00-07-01", "timeEnd": "00-07-04", "sentence": "in which case predictions with large variance"}, {"timeStart": "00-07-04", "timeEnd": "00-07-06", "sentence": "low confidence"}, {"timeStart": "00-07-07", "timeEnd": "00-07-17", "sentence": "likewise for nodules with poor enter observer agreement we expect our network to output a more spread distribution and will be punished for strongly predicting a single class label even if correct"}, {"timeStart": "00-07-17", "timeEnd": "00-07-21", "sentence": "at task this variance in the predicted distribution provides radiologist"}, {"timeStart": "00-07-21", "timeEnd": "00-07-24", "sentence": "with an estimate of the model's confidence in that prediction"}, {"timeStart": "00-07-26", "timeEnd": "00-07-31", "sentence": "x caps being a multi task framework has three losses in its overall objective function"}, {"timeStart": "00-07-31", "timeEnd": "00-07-36", "sentence": "first for the reconstruction branch we choose to reconstruct only the novel region of the input"}, {"timeStart": "00-07-36", "timeEnd": "00-07-38", "sentence": "mass by the ground through segmentation"}, {"timeStart": "00-07-38", "timeEnd": "00-07-41", "sentence": "we then compute the mean squared error between this"}, {"timeStart": "00-07-41", "timeEnd": "00-07-43", "sentence": "and the reconstruction branch output"}, {"timeStart": "00-07-43", "timeEnd": "00-07-52", "sentence": "next for our six after you predictions we compute the mean squared error between the network prediction and the normalized mean of the radiologist scores for each attribute"}, {"timeStart": "00-07-52", "timeEnd": "00-08-01", "sentence": "and lastly for predicting malignancy we compute the k Hall divergence between a gaussian distribution fit to the mean and variance of radiologists corps"}, {"timeStart": "00-08-01", "timeEnd": "00-08-04", "sentence": "and the soft max over the malignancy output prediction vector"}, {"timeStart": "00-08-04", "timeEnd": "00-08-07", "sentence": "the total loss is the sum of these functions"}, {"timeStart": "00-08-09", "timeEnd": "00-08-18", "sentence": "we performed experiments on the ally DC dataset where at least three radiologists annotated six hundred and forty six by nine and five hundred and three malignant nodules"}, {"timeStart": "00-08-18", "timeEnd": "00-08-21", "sentence": "excluding nodules of means score exactly three"}, {"timeStart": "00-08-21", "timeEnd": "00-08-28", "sentence": "our method was compared against the state of the art explainable CNN for lung cancer diagnosis s cm"}, {"timeStart": "00-08-29", "timeEnd": "00-08-32", "sentence": "dance old path three d CN"}, {"timeStart": "00-08-32", "timeEnd": "00-08-36", "sentence": "as well as to non explainable3Dcnn's and the original cap snap"}, {"timeStart": "00-08-36", "timeEnd": "00-08-43", "sentence": "the results of our experiments show that supervising the attributes learned within the vectors of our capsule network"}, {"timeStart": "00-08-43", "timeEnd": "00-08-46", "sentence": "significantly improved our performance over cap"}, {"timeStart": "00-08-46", "timeEnd": "00-08-52", "sentence": "while a CNN based method which built an identical explainable hierarchy of first predicting attributes"}, {"timeStart": "00-08-52", "timeEnd": "00-08-54", "sentence": "then malignancy"}, {"timeStart": "00-08-54", "timeEnd": "00-08-58", "sentence": "suffered from degraded performance compared to it's not explainable counterparts"}, {"timeStart": "00-08-58", "timeEnd": "00-09-00", "sentence": "shown here in this symbolic flock"}, {"timeStart": "00-09-01", "timeEnd": "00-09-07", "sentence": "here are the quantitative results of our experiments where are simple two d three layer ex cap"}, {"timeStart": "00-09-07", "timeEnd": "00-09-10", "sentence": "significantly outperform the explainable h s e n"}, {"timeStart": "00-09-10", "timeEnd": "00-09-12", "sentence": "I'm predicting attribute scores"}, {"timeStart": "00-09-12", "timeEnd": "00-09-15", "sentence": "also achieving a higher malignancy prediction accuracy"}, {"timeStart": "00-09-15", "timeEnd": "00-09-22", "sentence": "with performance comparable to that of the non explainable deep multi crop and multi scale3DCNN"}, {"timeStart": "00-09-24", "timeEnd": "00-09-30", "sentence": "we also performed an additional three ablation experiments which can be found in our paper"}, {"timeStart": "00-09-32", "timeEnd": "00-09-38", "sentence": "and then I'll conclude this presentation I would like to thank the NY h for providing funding support for this project"}, {"timeStart": "00-09-38", "timeEnd": "00-09-44", "sentence": "and thank you for everyone for joining I feel like to view the code or any other product pages you can follow the links provided here"}]}, {"title": "PEGASUS: Pre-training with Gap-Sentences for Abstractive Summarization | Research Paper Walkthrough", "authors": "prakharmishra137", "abstract": "", "publicationOrg": "ICML", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/1912.08777.pdf", "pdfPath": "/data/cache/2/PDFs/PEGASUSPretrainingwithGapSentencesforAbstractiveSummarizationResearchPaperWalkthrough.pdf", "publicationUrl": "https://arxiv.org/pdf/1912.08777.pdf", "codeUrl": "https://github.com/prakhar21", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/QY8oZxS0txs", "videoPath": "/data/cache/2/videos/PEGASUS- Pre-training with Gap-Sentences for Abstractive Summarization - Research Paper Walkthrough.mp4", "pdfText": "Text summarization aims at generating accurate and concise summaries from input document(s). In contrast to extractive summarization which merely copies informative fragments from the input, abstractive summarization may generate novel words. A good abstractive summary covers principal information in the input and is linguistically fluent.In abstractive summarization, sequence-to-sequence  has become a dominant framework using encoder-decoder architectures based on RNNs  and more recently . Most prior work on neural abstractive summarization relied on large-scale, high-quality datasets of supervised document-summary pairs  and achieved promising results . In recent years, there has been increased interest in collecting new summarization datasets that have more abstractive summaries , have longer documents, , utilize multiple documents , and are sourced from diverse domains ; arXiv:1912.08777v3  10 Jul 2020 however, there has been little work on systematic evaluation of models across these broad settings.Contemporaneously, the adoption of Transformer models  pre-trained using self-supervised objectives on large text corpora  have improved performance on many NLP tasks .Recent work leveraging such pre-training for Transformerbased sequence-to-sequence models  has extended the success to text generation, including abstractive summarization.In this work, we study pre-training objectives specifically for abstractive text summarization and evaluate on 12 downstream datasets spanning news , science , short stories , instructions (Koupaee & , emails , patents , and legislative bills . We find that masking whole sentences from a document and generating these gap-sentences from the rest of the document works well as a pre-training objective for downstream summarization tasks. In particular, choosing putatively important sentences outperforms lead or randomly selected ones. We hypothesize this objective is suitable for abstractive summarization as it closely resembles the downstream task, encouraging whole-document understanding and summary-like generation. We call this self-supervised objective Gap Sentences Generation (GSG). Using GSG to pre-train a Transformer encoder-decoder on large corpora of documents (Web and news articles) results in our method, Pre-training with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-sequence models, or PEGASUS.With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets.Large-scale document-summary datasets are rare and in practice there is a mismatch between research datasets and real-world use-cases where collecting summaries is expensive; the most common setting is that of low-resource summarization. We simulate this setting and show that our model is able to adapt very quickly when fine-tuning with small numbers of supervised pairs, obtaining state-of-the-art results in 6 datasets with only 1000 examples.Qualitatively we observed high quality outputs from our best models and validated this in human evaluation studies. We found that PEGASUS summaries are at least as good as reference summaries for the datasets we assessed -XSum, CNN/DailyMail, and Reddit TIFU -even at low-levels of supervision.To summarize our contributions:\u2022 We propose a new self-supervised pre-training objective for abstractive summarization, gap-sentences generation, and study strategies for selecting those sentences.\u2022 We evaluate the proposed pre-training objective on a broad range of downstream summarization tasks, with careful ablations to choose the best model settings, which we use to train a 568M parameter PEGASUS model that surpasses or is on-par with the state-of-theart on all 12 downstream datasets considered.\u2022 We show how good abstractive summarization performance can be achieved across broad domains with very little supervision by fine-tuning the PEGASUS model and surpassing previous state-of-the-art results on many tasks with as little as 1000 examples.\u2022 We conducted human evaluation studies to validate our experimental design and demonstrate human-level summarization performance on XSum, CNN/DailyMail, and Reddit TIFU.Dai & Le (2015); Ramachandran et al. (2017) used LM and autoencoder pre-training on in-domain data to improve performance of RNN sequence models. However, the combination of pre-training with much larger external text corpora (such as Wikipedia, books, or Web-pages) and Transformerbased sequence models has led to a dramatic improvement in performance when fine-tuned for both natural language understanding and text generation tasks . Most similar to our approach are Transformer encoder-decoder models pre-trained on some masked input pre-training objective.MASS  proposed masked sequence-tosequence generation that reconstructs a sentence fragment given the remaining part of the sentence. A single sentence fragment was randomly selected.UniLM  proposed jointly training on three types of language modeling tasks: unidirectional (leftto-right and right-to-left), bidirectional (word-level mask, with next sentence prediction), and sequence-to-sequence (word-level mask) prediction.T5  generalized the text-to-text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus derived from Common Crawl, which we also use in some of our models. T5 was pre-trained with randomly corrupted text spans of varying mask ratios and sizes of spans.BART  introduced a denoising autoencoder to pre-train sequence-to-sequence models. BART corrupted text with an arbitrary noising function and learned to reconstruct the original text. For generation tasks, the noising function was text infilling which used single mask tokens to mask random sampled spans of text.In contrast to MASS, UniLM, BART and T5, the proposed PEGASUS masks multiple whole sentences rather than smaller continuous text spans. In our final objective we deterministically choose sentences based on importance, rather than randomly. As in T5, PEGASUS does not reconstruct full input sequences, and only generates the masked sentences as a single output sequence. In this work we focus entirely on downstream summarization (generative) tasks and do not evaluate on NLU classification tasks.There has been some work on the low-resource, summarization setting using the CNN/DailyMail dataset.  showed that a large Transformer language model pre-trained on Web text could generate summaries if prompted with \"TL;DR\", achieving a ROUGE-2 of 8.27 on CNN/DailyMail.  pre-trained a Transformer language model on Wikipedia, and fine-tuned using 3000 examples, achieving 13.1 ROUGE-2.We propose a new pre-training objective, GSG, in this work, but for comparison, we also evaluate BERT's maskedlanguage model objective, in isolation and in conjunction with GSG.We hypothesize that using a pre-training objective that more closely resembles the downstream task leads to better and faster fine-tuning performance. Given our intended use for abstractive summarization, our proposed pre-training objective involves generating summary-like text from an input document. In order to leverage massive text corpora for pretraining, we design a sequence-to-sequence self-supervised objective in the absence of abstactive summaries. A naive option would be to pre-train as an extractive summarizer;however, such a procedure would only train a model to copy sentences, thus not suitable for abstractive summarization.Inspired by recent success in masking words and contiguous spans , we select and mask whole sentences from documents, and concatenate the gap-sentences into a pseudo-summary. The corresponding position of each selected gap sentence is replaced by a mask token [MASK1] to inform the model. Gap sentences ratio, or GSR, refers to the number of selected gap sentences to the total number of sentences in the document, which is similar to mask rate in other works.To even more closely approximate a summary, we select sentences that appear to be important/principal to the document. The resulting objective has both the empirically demonstrated benefits of masking, and anticipates the form of the downstream task.We consider 3 primary strategies for selecting m gap sentences without replacement from a document, D = {x i } n , comprised of n sentences:Random Uniformly select m sentences at random.Lead Select the first m sentences.Principal Select top-m scored sentences according to importance. As a proxy for importance we compute ROUGE1-F1  between the sentence and the rest of the document, s i = rouge(x i , D \\ {x i }), \u2200i.In this formulation sentences are scored independently (Ind) and the top m selected. We also consider selecting them sequentially (Seq) as in  by greedily maximizing the ROUGE1-F1 between selected sentences, S \u222a {x i }, and remaining sentences, D \\ (S \u222a {x i }) as in Algorithm 1.Algorithm 1 Sequential Sentence Selection 1: S := \u2205 2: for j \u2190 1 to m do 3:k := arg max i {s i } nS := S \u222a {x k } 6: end for When calculating ROUGE1-F1, we also consider n-grams as a set (Uniq) instead of double-counting identical n-grams as in the original implementation (Orig). This results in four variants of the principal sentence selection strategy, choosing Ind/Seq and Orig/Uniq options.An example containing lead, random and principal gap sentence selection strategies are shown in .  ONLY We are very excited to be co-hosting a major drinks reception with our friends at Progress. This event will sell out, so make sure to register at the link above. Speakers include Rajesh Agrawal, the London Deputy Mayor for Business, Alison McGovern, the Chair of Progress, and Seema Malhotra MP. Huge thanks to the our friends at the ACCA, who have supported this event. The Labour Business Fringe at this year's Labour Annual Conference is being co-sponsored by Labour in the City and the Industry Forum. Speakers include John McDonnell, Shadow Chancellor, and Rebecca Long-Bailey, the Shadow Chief Secretary to the Treasury, and our own Chair, Kitty Ussher. Attendance is free, and refreshments will be provided. Following BERT, we select 15% tokens in the input text, and the selected tokens are (1) 80% of time replaced by a mask token [MASK2], or (2) 10% of time replaced by a random token, or (3) 10% of time unchanged. We apply MLM to train the Transformer encoder as the sole pre-training objective or along with GSG. When MLM is the sole pre-training objective, the Transformer decoder shares all parameters with encoder when fine-tuning on downstream tasks following .Figure 1 simultaneously shows how both GSG and MLM are applied to the same example when used in conjunction. However, we found that MLM does not improve downstream tasks at large number of pre-training steps (section 6.1.2), and chose not to include MLM in the final model PEGASUS LARGE (section 6.2).For pre-training we considered two large text corpora:\u2022 C4, or the Colossal and Cleaned version of Common Crawl, introduced in Raffel et al. ; consists of text from 350M Web-pages (750GB).\u2022 HugeNews, a dataset of 1.5B articles (3.8TB) collected from news and news-like websites from 2013-2019. A whitelist of domains ranging from highquality news publishers to lower-quality sites such as high-school newspapers, and blogs was curated and used to seed a web-crawler. Heuristics were used to identify news-like articles, and only the main article text was extracted as plain text.For downstream summarization, we only used public abstractive summarization datasets, and access them through TensorFlow Summarization Datasets 1 , which provides publicly reproducible code for dataset processing and train/validation/test splits. We used train/validation/test ratio of 80/10/10 if no split was provided, and 10% train split as validation if there was no validation split.XSum  consists of 227k BBC articles from 2010 to 2017 covering a wide variety of subjects along with professionally written single-sentence summaries.  dataset contains 93k articles from the CNN, and 220k articles the Daily Mail newspapers. Both publishers supplement their articles with bullet point summaries. We use the non-anonymized variant used in .NEWSROOM ) is a large dataset containing 1.3M article-summary pairs written by authors and editors in the newsrooms of 38 major publications between 1998 and 2017.Multi-News (Fabbri et al., 2019) is a multi-document summarization dataset consisting of 56k pairs of news articles and their human-written summaries from the site newser.com.Gigaword  contains 4M examples extracted from news articles (seven publishers) from the Gigaword corpus . The task is to generate the headline from the first sentence.arXiv, PubMed  are two long document datasets of scientific publications from arXiv.org (113k) and PubMed (215k). The task is to generate the abstract from the paper body.BIGPATENT  consists of 1.3 million U.S. patents along with human summaries under nine patent classification categories.WikiHow (Koupaee & Wang, 2018) is a large-scale dataset of instructions from the online WikiHow.com website. Each of 200k examples consists of multiple instruction-step paragraphs along with a summarizing sentence. The task is to generate the concatenated summary-sentences from the paragraphs.Reddit  contains 120K posts of informal stories from the online discussion forum Reddit, more specifically the TIFU sub-reddit from 2013-Jan to 2018-Mar. The sub-reddit posts strictly follow the rule of writing a descriptive \"TL;DR\" summary and has higher qual-ity than (V\u00f6lske et al., 2017) (which used more subreddits) based on our manual inspection. We uses the TIFU-long subset (using TLDR as summaries) in the work.AESLC  consists of 18k email bodies and their subjects from the Enron corpus (Klimt & , a collection of email messages of employees in the Enron Corporation.BillSum  contains 23k US Congressional bills and human-written reference summaries from the 103rd-115th (1993-2018) sessions of Congress. We do not use the California test set which is out-of-distribution.Following Grusky et al., the number of examples and extractive fragment coverage/density for all downstream datasets is illustrated in Appendix A.In a similar strategy to , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size.PEGASUS BASE had L = 12, H = 768, F = 3072, A = 12 and PEGASUS LARGE had L = 16, H = 1024, F = 4096, A = 16, where L denotes the number of layers for encoder and decoder (i.e. Transformer blocks), H for the hidden size, F for the feed-forward layer size and A for the number of self-attention heads. We pre-trained PEGASUS BASE with a batch size of 256 and PEGASUS LARGE with a batch size of 8192. We refer to PEGASUS BASE without pre-training as Transformer BASE .We used sinusoidal positional encoding following . For optimization, both pre-training and finetuning used Adafactor  with square root learning rate decay and dropout rate of 0.1.We used greedy-decoding for studies in Section 6.1, and used beam-search with a length-penalty, \u03b1, as in  for the final large model. All experiments' hyper parameters can be found in Appendix C and reported numbers are in Appendix D and E.We used PEGASUS BASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. For reproducibility, we evaluated the latter two using the publicly available C4 corpus.Note that the y-axis in , 5 are normalized by the left-most bar using 1 3 ( R1 R1base + R2 R2base + RL RLbase ) where R1, R2, RL are ROUGE F1 scores and R1 base , R2 base , RL base are the scores of the configuration corresponding to the first bar.With more pre-training steps, the model observed more documents in the pre-training corpus. A PEGASUS BASE model trained for 500k (highest we tried) steps did not observe all training examples on C4 nor HugeNews. Appendix B shows the number of pre-training steps had an unsurprisingly positive impact on downstream dataset performance. We used 500k steps for the ablation studies and the large model. Figure 3: Effect of pre-training corpus. PEGASUS BASE pre-trained on C4 (350M Web-pages) and HugeNews (1.5B news-like documents).  shows that pre-training on HugeNews was more effective than C4 on the two news downstream datasets, while the non-news informal datasets (WikiHow and Reddit TIFU) prefer the pre-training on C4. This suggests pretraining models transfer more effectively to downstream tasks when their domains are aligned better.GSG We compared six variants of GSG (Lead, Random, Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq) while choosing 30% sentences as gap sentences. As shown in , Ind-Orig achieved the best performance followed by Seq-Uniq. Ind-Orig and Seq-Uniq were consistently better (or similar) than Random and Lead across the four downstream datasets. Lead had decent performance on the two news datasets but was significantly worse on the two non-news datasets, which agrees findings of lead bias in news datasets . The results suggest choosing principal sentences works best for downstream summarization tasks, and we chose Ind-Orig for the PEGASUS LARGE .A significant hyper-parameter in GSG is the gap-sentences ratio (GSR). A low GSR makes the pre-training less challenging and computationally efficient. On the other hand, choosing gap sentences at a high GSR loses contextual in- (b) Effect of gap sentences ratio with GSG (Ind-Orig). formation necessary to guide the generation. We compared GSRs from 15% to 75%. For a fair comparison, the original documents were truncated to have up to 400 words. The maximum input length, L input in the encoder and the maximum target length, L target in the decoder were set as 512 tokens.  shows that different downstream datasets had slightly different optima. The best performance always had GSR lower than 50%. The model with 15% gap sentences achieved the highest ROUGE scores on CNN/DailyMail, while XSum/Reddit TIFU and WikiHow did better with 30% and 45% respectively. When scaling up to PEGASUS LARGE (Section 6.2), we chose an effective GSR of 30%.MLM As mentioned, the MLM objective can either be applied solely or together with GSG. We jointly trained MLM with GSG Ind-Orig (MLM & Ind-Orig), which masks 30% sentences and extra 15% tokens in unselected sentences, as shown in .  shows that the model pretrained with MLM alone performed significantly worse and MLM & Ind-Orig had similar performance as Random. Interestingly, when comparing MLM & Ind-Orig to Ind-Orig, we empirically observed MLM improved fine-tuning performance at early pre-training checkpoints (100k -200k steps), but inhibited further gains with more pre-training steps (500k). Therefore, we chose not to include MLM in PEGASUS LARGE .We compared two tokenization methods 2 : Byte-pairencoding algorithm (BPE) , and SentencePiece Unigram algorithm (Unigram) proposed in . We evaluated Unigram with different vocabulary sizes ranging from 32k to 256k. In these experiments, models were pre-trained for 500k steps on the C4 corpus with the Ind-Orig objective and 15% GSR. As shown in ,   Figure 5: Effect of vocabulary with PEGASUS BASE trained on C4 (15% GSR, Ind-Orig).on non-news datasets, especially WikiHow. On XSum and CNN/DailyMail, Unigram 96k achieved the highest ROUGE scores. On WikiHow and Reddit TIFU, the best configurations were Unigram 128k and 64k respectively. Therefore, we used the overall best vocabulary option Unigram 96k in PEGASUS LARGE .Compared with PEGASUS BASE , the large model PEGASUS LARGE had increased capacity from larger hidden size (H : 768 \u2192 1024, F : 3072 \u2192 4096, A : 12 \u2192 16), number of layers (L : 12 \u2192 16) and traversed much more data, due to larger batch size (B : 256 \u2192 8192) (same number of pre-training steps, 500k). We adopted the best practices found in the PEGASUS BASE ablation studies using the GSG (Ind-Orig) pre-training objective without MLM and Unigram vocabulary size of 96k. In total, PEGASUS LARGE had 568M parameters.To encourage the model to copy, which is an important aspect of the more extractive datasets, we left 20% of selected sentences unchanged in the input instead of replacing with . We increased the GSR to 45% to achieve a similar number of \"gaps\" as the optimal 30% found above. We reported the performance of the models pre-trained on HugeNews and C4 separately. We conducted a simple hyper-parameter sweep of learning rate and length penalty, : Results of PEGASUS LARGE and PEGASUS BASE on all downstream datasets compared with the previous SOTA, which are fetched from . We only compared with previous abstractive models except on BillSum which had extractive results only. BIGPATENT, arXiv, PubMed and Multi-News datasets contain very long summaries and we truncate them to 256 tokens, in similar range compared to . Best ROUGE numbers on each dataset and numbers within 0.15 of the best numbers are bolded.  \u03b1, when fine-tuning PEGASUS LARGE on each downstream dataset.CNN/DailyMail, Multi-News, arXiv, PubMed, BIG-PATENT datasets contain input documents longer than the maximum input length (L input = 512 tokens) in pretraining. This would present a problem for position embeddings which would never be updated for longer input lengths, but we confirm the postulation that sinusoidal positional encodings (Vaswani et al., 2017) generalize well when fine-tuning PEGASUS LARGE beyond the input lengths observed in training up to L input = 1024 tokens. Since average input length in BIGPATENT, arXiv, PubMed and Multi-News are well beyond 1024 tokens, further scaling up L input or applying a two-stage approach  may improve performance even more, although this is outside the scope of this work.  show the performance improvements of PEGASUS BASE and PEGASUS LARGE on downstream datasets. While PEGASUS BASE exceeded current state-ofthe-art on many datasets, PEGASUS LARGE achieved better than state-of-the-art results on all downstream datasets using HugeNews, although C4 performed better on WikiHow.The improvement from a Transformer model without pretraining (Transformer BASE ) to PEGASUS LARGE was more significant on smaller datasets. For example, the ROUGE2-F1 scores nearly tripled on AESLC and quintupled on Reddit TIFU. The large jumps in performance suggest that small text summarization datasets benefit the most from pre-training. We further investigate low resource summarization in Section 6.3.In real-world practice, it is often difficult to collect a large number of supervised examples to train or fine-tune a summarization model. To simulate the low-resource summarization setting, we picked the first 10 k (k = 1, 2, 3, 4) training examples from each dataset to fine-tune PEGASUS LARGE (HugeNews) . We fine-tuned the models up to 2000 steps with batch size 256, learning rate 0.0005, and picked the checkpoint with best validation performance. In , in 8 out of 12 datasets, with just 100 examples  : Fine-tuning with limited supervised examples. The solid lines are PEGASUS LARGE fine-tuned on 0 (zero shot), 10, 100, 1k,10k examples. The dashed lines are Transformer BASE models, equivalent in capacity as PEGASUS BASE and trained using the full supervised datasets, but with no pre-training. All numbers are reported in Appendix E. : Human evaluation side-by-side results on Likert (1-5) scale (higher is better). Scores are bolded if they are not worse than human-level performance by p < 0.01.2.0 (3e-10) 2.9 (0.06) 1.4 (5e-23) Experiment 2: low resource Human-written 3.2 (-) 3.2(-) 3.3 (-) PEGASUSLARGE (HugeNews) 10 examples 2.8 (0.1) 3.4 (0.007) 2.6 (0.006) PEGASUSLARGE (HugeNews) 100 examples 3.2 (0.5) 3.4 (0.08) 2.1 (4e-8) PEGASUSLARGE (HugeNews) 1000 examples 3.4 (0.3)3.6 (0.07) 2.7 (0.01) PEGASUSLARGE (HugeNews) full supervision 3.4 (0.3)3.3 (0.1) 2.8 (0.05) PEGASUS LARGE could be fine-tuned to generate summaries at comparable quality to Transformer BASE trained on the full supervised datasets ranging from 20k to 200k examples. PEGASUS LARGE also beat previous state-of-the-art results on 6 out of 12 datasets with only 1000 fine-tuning examples.On CNN/DailyMail, with half the number of parameters PEGASUS LARGE demonstrated much better zero-shot (ROUGE2-F=13.28) performance than GPT-2 (ROUGE2-F=8.27). Using only 1000 examples, PEGASUS LARGE achieved ROUGE2-F of 19.35, much higher than the 13.1 obtained in  with 3000 examples.Overall, we observed high-linguistic quality (in terms of fluency and coherence), closely emulating the style of groundtruth summaries. While some previous work suggested that maximum likelihood training results in repetitive text in model outputs  we found this to be rare in our outputs and did not require additional countermeasures to mitigate dis-fluencies.Although ROUGE clearly has its draw-backs , over-penalizing abstractive approaches com-pared to extractive ones and having no sense of linguistic quality, we found that choosing perplexity-optimized models using aggregated ROUGE (rather than directly optimizing ROUGE as in Paulus et al. (2017)) resulted in qualitatively good models. Randomly sampled (by a program) model decodes across all datasets and a broad range of ROUGE scores can be found in Appendix I.We found that even low-ROUGE model summaries often were highquality, .1.To assess how close PEGASUS LARGE is to human performance we conducted human evaluation experiments on Amazon Mechanical Turk comparing model summaries with (human) reference summaries given the input document. The examples were drawn from the XSum, CNN/DailyMail, and Reddit TIFU datasets; the first two were chosen due to their popularity in past work, and the third was chosen for its significant difference in style. Workers were asked to rate the summaries on a 1-5 scale, with higher being better (full experiment details provided in Appendix F) and a paired t-test was used to assess whether scores were significantly different from human.In the first experiment, PEGASUS LARGE (HugeNews), PEGASUS LARGE (C4), and Transformer BASE were compared with reference summaries; in the second experiment, PEGASUS LARGE (HugeNews) fine-tuned using 10, 100, 1000, and all supervised examples were compared with references; the results are shown in . According to the significance level of p < 0.01, both PEGASUS LARGE (HugeNews) and PEGASUS LARGE (C4) outputs were at least as good as the reference summaries in all cases. Even at low-levels of supervision PEGASUS LARGE (HugeNews) was not measurably worse than human summaries on XSum and CNN/DailyMail. In the Reddit TIFU case, however, perhaps due to its diverse writing styles, human performance required full supervision.The pre-training corpora are a large collection of documents from the Internet and potentially have overlap with the downstream test sets. In this section, we measured the extent of overlap between the pre-training corpus and downstream datasets. We also studied if the pre-trained model was able to exploit memorization to achieve higher performance on the downstream datasets.To measure the overlap, we calculated similarities between all pairs of downstream test set targets and pre-training documents. We use the ROUGE-2 recall as a similarity measure (common 2-grams / test set targets 2-grams). It is not necessarily exact match even if the similarity score is 1.0. We filtered all test set examples that have similarity to any pre-training example above a threshold, and recalculated the ROUGE scores on the remaining test set. In , we conducted this study on the pre-training corpus C4 and test set of XSum, CNN/Dailymail, Reddit TIFU and WikiHow, with a similarity threshold of 1.0 and 0.8. Results show that only XSum has significant amount of overlap 15% to 20%, and filtering those examples does not change ROUGE scores more than 1%. We also manually examined those overlapped examples with similarity of 1.0, and found that the models produce very different summaries compared to the human written ones, suggesting that there was no clear memorization. Figure 7: Percentage of overlap between C4 and downstream test sets, and ROUGE score changes after removing those overlapped examples in test sets.Following our experiments on PEGASUS LARGE pre-trained on C4 and HugeNews, we pre-trained a PEGASUS LARGE model on both corpora and stochastically sampled important sentences. The PEGASUS LARGE (mixed,stochastic) model includes the changes: (1) The model was pre-trained on the mixture of C4 and HugeNews weighted by their number of examples.(2) The model dynamically chose gap sen- tences ratio uniformly between 15%-45%. (3) Importance sentences were stochastically sampled with 20% uniform noise on their scores. (4) The model was pre-trained for 1.5M steps instead of 500k steps, as we observed slower convergence of pre-training perplexity. (5) The SentencePiece tokenizer was updated to encode the newline character. The PEGASUS LARGE (mixed, stochastic) model achieved best results on almost all downstream tasks, as shown in .In this work, we proposed PEGASUS, a sequence-tosequence model with gap-sentences generation as a pretraining objective tailored for abstractive text summarization. We studied several gap-sentence selection methods and identified principle sentence selection as the optimal strategy. We demonstrated the effects of the pre-training corpora, gap-sentences ratios, vocabulary sizes and scaled up the best configuration to achieve state-of-the-art results on all 12 diverse downstream datasets considered. We also showed that our model was able to adapt to unseen summarization datasets very quickly, achieving strong results in as little as 1000 examples. We finally showed our model summaries achieved human performance on multiple datasets using human evaluation.", "videoStruct": [{"timeStart": "00-00-01", "timeEnd": "00-00-02", "sentence": "welcome back to the new video"}, {"timeStart": "00-00-02", "timeEnd": "00-00-06", "sentence": "so today I'll be doing for this paper which is titled as Pegasus"}, {"timeStart": "00-00-06", "timeEnd": "00-00-12", "sentence": "pretending with the extracted gap sentences for obstructive summer is ation it is from authors from google"}, {"timeStart": "00-00-12", "timeEnd": "00-00-14", "sentence": "and was released early this year"}, {"timeStart": "00-00-14", "timeEnd": "00-00-17", "sentence": "so as he could see what those have given an acronym for the paper"}, {"timeStart": "00-00-17", "timeEnd": "00-00-18", "sentence": "which basically stands for"}, {"timeStart": "00-00-21", "timeEnd": "00-00-27", "sentence": "is you and the last is stands for sequence sequence model the s from their"}, {"timeStart": "00-00-27", "timeEnd": "00-00-32", "sentence": "so if a dog with important keywords in title abstract of some relation that will first discuss"}, {"timeStart": "00-00-32", "timeEnd": "00-00-39", "sentence": "then we'll talk about what is pre training then we'll talk about what is gap sentence ok so starting with obstructive summer is a"}, {"timeStart": "00-00-40", "timeEnd": "00-00-43", "sentence": "so obstructive some relation is a technique"}, {"timeStart": "00-00-43", "timeEnd": "00-00-48", "sentence": "and which are a system tries to write the summary of a given text just like a human would do"}, {"timeStart": "00-00-48", "timeEnd": "00-00-50", "sentence": "we did good for three d input text"}, {"timeStart": "00-00-50", "timeEnd": "00-00-54", "sentence": "and then try to write a summary and its own language so when I say in its own language"}, {"timeStart": "00-00-54", "timeEnd": "00-00-57", "sentence": "the mortal is essentially trying to rephrase the things"}, {"timeStart": "00-00-57", "timeEnd": "00-01-06", "sentence": "that it has seen as a part of input sentences around what is important also you might see some novel words here and there so that is abstract IV summer edition shirt"}, {"timeStart": "00-05-38", "timeEnd": "00-05-42", "sentence": "so they put in the model on sea for corpus"}, {"timeStart": "00-05-42", "timeEnd": "00-05-45", "sentence": "also on and other quarters that has news like articles"}, {"timeStart": "00-13-45", "timeEnd": "00-13-54", "sentence": "there's something wrong fifteen and that is happening consistently across all the matrix if you consider which one f two f or route l f"}, {"timeStart": "00-13-54", "timeEnd": "00-13-59", "sentence": "and not only that its happening almost across all the data sets against which they have evaluated"}, {"timeStart": "00-14-26", "timeEnd": "00-14-29", "sentence": "so I guess we are done with the paper now"}, {"timeStart": "00-14-33", "timeEnd": "00-14-36", "sentence": "yeah so I found this people to be extensively written"}, {"timeStart": "00-14-36", "timeEnd": "00-14-43", "sentence": "so this people is actually around sixty sixty five days is long after the references ortho given ample number of examples"}, {"timeStart": "00-14-43", "timeEnd": "00-14-45", "sentence": "to the somebodies were the model have generated"}, {"timeStart": "00-14-45", "timeEnd": "00-14-47", "sentence": "I would encourage you to go through that as well"}, {"timeStart": "00-14-47", "timeEnd": "00-14-49", "sentence": "I put the link of the paper in the description box"}, {"timeStart": "00-14-49", "timeEnd": "00-14-53", "sentence": "ok so make sure you hit that like button and subscribe to the channel"}, {"timeStart": "00-14-53", "timeEnd": "00-14-56", "sentence": "also coming down below for any people that you want me to walk through"}, {"timeStart": "00-14-56", "timeEnd": "00-14-58", "sentence": "make sure you share this video with the beers"}, {"timeStart": "00-14-58", "timeEnd": "00-15-00", "sentence": "who so it is interested in such"}, {"timeStart": "00-15-00", "timeEnd": "00-15-02", "sentence": "I'll meet you in the next one"}]}, {"title": "Scalable Identification of Partially Observed Systems with Certainty-Equivalent EM", "authors": "kmenda", "abstract": "", "publicationOrg": "ICML", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2006.11615.pdf", "pdfPath": "/data/cache/2/PDFs/ScalableIdentificationofPartiallyObservedSystemswithCertaintyEquivalentEM.pdf", "publicationUrl": "https://arxiv.org/pdf/2006.11615.pdf", "codeUrl": "https://github.com/sisl/CEEM", "datasetUrl": "", "videoUrl": "https://stream.crossminds.ai/5f937bbc9e59e93d2f61a8c1-1603501004805/hls-5f937bbc9e59e93d2f61a8c1.m3u8", "videoPath": "/data/cache/2/videos/Scalable Identification of Partially Observed Systems with Certainty-Equivalent EM - Crossminds.ts", "pdfText": "The performance of controllers and state-estimators for non-linear systems depends heavily on the quality of the model of system dynamics . Systemidentification addresses the problem of learning or calibrating dynamics models from data , which is often a time-history of observations of the system and control inputs. In this work, we address the problem of learning dynamics models of partially observed systems (shown in Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s).x tx t+1u t y t y t+1x txt+1 f \u03b8  g \u03b8 (x, u, t) g \u03b8 (x, u, t + 1) . A graphical model representing a partially observed dynamical system. Gray-box identification algorithms attempt to search a model class of dynamics and observation models for the model that maximizes the likelihood of the observations. ) that are high-dimensional and non-linear. We consider situations in which the system's state cannot be inferred from a single observation, but instead requires inference over time-series of observations.The problem of identifying systems from partial observations arises in robotics  as well as domains such as chemistry  and biology . In many robotic settings, we have direct measurements of a robot's pose and velocity, but in many cases we cannot directly observe relevant quantities such as the temperature of actuators, the state the environment around the robot, or the intentions of other agents. For example,  attempted to map the pose and velocity of an aerobatic helicopter to its acceleration. They found their model to be inaccurate when predicting aggressive maneuvers because of the substantial airflow generated by the helicopter that affected the dynamics. Since it is often impossible to directly measure the state of the airflow around a vehicle, identification must be with only partial observability.System identification is a mature field with a rich history . A variety of techniques have been proposed to learn predictive models from time series arXiv:2006.11615v1 LG] 20 Jun 2020data. Autoregressive approaches directly map a time-history of past inputs to observations, without explicitly reasoning about unobserved states , and are the stateof-the-art approach to the aforementioned problem of modeling the aerobatic helicopter . In contrast, state-space models (SSM) assume an unobserved state x t that evolves over time and emits observations y t that we measure, as shown in . Recurrent Neural Networks (RNNs) (Bailer-  are a form of black-box non-linear SSM that can be fit to observation and input time-series, and Subspace Identification (SID) methods  can be used to fit linear SSMs.However, in many cases prior knowledge can be used to specify structured, parametric models of the system  in state-space form, commonly refered to as gray-box models. Such models can be trained with less data and used with a wider array of control and stateestimation techniques than black-box models .To identify partially observed gray-box models, the unobserved state-trajectory is often considered as a missing data, and techniques based on Expectation-Maximization (EM) are used . The smoothing step (E-step) deals with state inference-the current system dynamics estimate is used to infer the distribution of unobserved state-trajectories conditioned on the observations p(x 1:T | y 1:T ). This distribution is sometimes called joint smoothing distribution in the literature, and it is used to estimate the expected log-likelihood of the observations. In the learning step (M-step), the system's dynamics estimate is updated such that it maximizes the expected log-likelihood. The smoothing step can typically be approached with particle approximations of p(x 1:T | y 1:T ), but naive implementations of particle smoothers can be computationally intensive and become rapidly intractable in high dimensions. Across various fields and disciplines, numerous methods have been developed to alleviate the computational burden, several of which are discussed in Section 2.3.This work is motivated by robotics applications, in which the following assumptions are often valid:\u2022 Systems evolve nearly deterministically, which implies that the process noise is small and unimodal, and, \u2022 The distribution of states conditioned on the observations p(x 1:We study the benefits of making the certainty-equivalent approximation in the E-step of the EM procedure, and we refer to this approach as CE-EM. Specifically, we use non-linear programming to tractably find the maximum-likelihood (ML) point-estimate of the unobserved states, and use it in lieu of a fully characterized approximate distribution. The contributions of this paper are to describe an efficient implementation of CE-EM, and to test the approximation against state-of-the-art approaches on a variety of systemidentification problems. We demonstrate on a system of Lorenz attractors that:\u2022 CE-EM can be faster and more reliable than approaches using particle approximations, \u2022 CE-EM scales to high-dimensional problems, and, \u2022 CE-EM learns unbiased parameter estimates on deterministic systems with unimodal p(x 1:We also demonstrate the algorithm on the problem of identifying the dynamics of an aerobatic helicopter. We show that a non-linear SSM can be trained with CE-EM that outperforms various approaches including the most recent work done on this dataset . A codebase implementing CE-EM and other supplementary material can be found at our website: https://sites.google. com/stanford.edu/ceem/.This section states the nonlinear system identification problem with partial observations and discusses approaches that use EM to solve it.In this work, we assume that we are given a batch of trajectories containing observations y 1:T \u2208 R m\u00d7T of a dynamical system as it evolves over a time horizon T , possibly forced by some known input sequence u 1:T . We assume that this dynamical system has a state x \u2208 R n that evolves and generates observations according to the following equations,where w t is referred to as the process noise and v t as the observation noise. Both w t and v t are assumed to be additive for notational simplicity, but this is not a required assumption. 1 Without loss of generality, we can drop the dependence on u t , absorbing it into the dependence on t.We further assume that we are provided a class of parameterized models f \u03b8 (x, t) and g \u03b8 (x, t) for \u03b8 \u2208 \u0398 that approximate the dynamical system's evolution and observation processes. The goal of our algorithm is to find the parameters \u03b8 that maximize the likelihood of the observations. ThatThe evaluation of Q(\u03b8, \u03b8 k ) in the E-step requires the estimation the p(x 1:T | y 1:T , \u03b8 k ), as well as the integration of the likelihood over this distribution. In the linear Gaussian case, EM can be performed exactly using Kalman smoothing . In the more general case, there is no analytic solution, and previous work on approximating the E-step is summarized in the next section.There are general approaches that are based on particle representations of the joint states-observations likelihood which use Sequential Monte-Carlo (SMC) methods such as Particle Smoothing (PS) . With enough particles, PS can handle any system and any joint distribution of states and observations. However, PS suffers from the curse of dimensionality, requiring an intractably large number of particles if the state space is high-dimensional . In the simplest form, both the E-step and the M-step can be quadratic in complexity with respect to the number of particles . An important body of work has attempted to alleviate some of this burden by using Nested SMC  and forward filtering-backward simulation (FFBSi), , and conditional particle filtering  in the E-step. PS can also require variance reduction techniques, such as fulladaptation, to perform reliably, and likelihood estimates in the E-step can be noisy . Stochastic Approximation EM can be used to stabilize learning in the presence of this noise .Another group of methods is based on linearizing the dynamics around the current state estimates to obtain a timedependant linear Gaussian dynamical system, to which we can apply Kalman smoothing techniques.  use Extended Kalman Smoothing (EKS) for a fast E-step and show that, when using radial basis functions to describe the system dynamics, no assumption is required in the M-step. The drawbacks of this approach are that the number of radial basis functions required to accurately represent a function grows exponentially with the input dimension, that the user cannot specify a gray-box SSM, and that EKS performance can vary dramatically with the hyperparameters.  proposed a method called MAP-EM, which linearizes around the state-trajectory that maximizes joint state and observation likelihood, approximates process and observation noise as Gaussian, and performs a local version of EM using this linearization. In comparison, the method we consider assumes that the maximum likelihood state-trajectory estimate concentrates all of the probability mass.  call this simplification Certainty Equivalent EM (CE-EM) and report worse results than their approach on simple examples.This paper shows that CE-EM can actually be a fast, simple, and reliable approach. The method we use here is tailored to systems that are not dominated by process noise, in which the state-trajectory distribution is unimodal, and whose high dimensional state-space make other methods intractable. Such scenarios are regularly encountered in robotics, but perhaps less so in other applications domains of system identification such as chemistry, biology, or finance.This section introduces the CE-EM approximation and presents an efficient algorithm to identify high-dimensional robotic systems.Under the certainty-equivalence condition, the distribution of states conditioned on observations is assumed to be a Dirac delta function. That is, we assume:whereIn other words, this assumption implies that there is only one state trajectory that satisfies the system dynamics, and is coherent with the data.The E-step can be rewritten as:By observing the maximization of the expression in Equation (7) as the E-step, and maximizing the expression in Equation as the M-step, we see that CE-EM is simply block coordinate-ascent on the single joint-objective:Jointly maximizing this objective over x 1:T and \u03b8 yields \u03b8 ML .Though this objective can be optimized as is by a non-linear optimizer, it is not necessarily efficient to do so since \u03b8 and x 1:T are highly coupled, leading to inefficient and potentially unstable updates. For this reason, we still opt for the block coordinate-ascent approach similar to EM, where \u03b8 and x 1:T are each sequentially held constant while the other Algorithm 1. CE-EM Implementation Input: observations y 1:T , control inputs u 1:T , stopping criterion tol Initialize: model parameters \u03b8 Initialize: hidden state estimatesis optimized. By viewing EM as block coordinate-ascent, we may also borrow good practices typically employed when running the algorithm . In particular, we employ trust-region regularization in order to guarantee convergence in even non-convex settings .At iteration k, we first perform smoothing by holding \u03b8 constant and finding the ML point-estimate for x 1:T as follows:(10) where \u03c1 x scales a soft trust-region regularizer similar to damping terms found in Levenberg-Marquardt methods . We note that the Hessians of this objective with respect to x 1:T are blocksparse, and as a result this step can be efficiently solved with a second-order optimizer in O(n 2 T ).If the cost function J(x 1:T , \u03b8) is a sum of quadratic terms, i.e. if all stochasticity is Gaussian, then the smoothing can be solved with a Gauss-Newton method, where the Hessian matrix is approximated using first-order (Jacobian) information. In this case, the solution to the smoothing step is equivalent to iteratively performing EKS . However, EKS typically involves forward and backward passes along each trajectory, performing a Riccati-like update at each time step. Though square-root versions of the algorithm exist to improve its numerical stability , solving the problem with batch non-linear least-squares as opposed to iteratively performing a forward-backward method can improve stability for long time-series (Van Dooren, 1981).In the learning step, we hold x 1:T constant and find:Again, \u03c1 \u03b8 scales a soft trust-region regularizer, and specifying log p(\u03b8) allows us to regularize \u03b8 toward a prior. The above optimization problem can be solved using any nonlinear optimizer. We find the Nelder-Mead ) scheme well-suited for small parameter spaces, and first-order schemes such as Adam , or quasi-second-order schemes such as L-BFGS ) suited for larger parameter spaces such as those of neural networks. The routine, summarized in Algorithm 1, iterates between the smoothing and learning steps until convergence.It should be noted that making the certainty-equivalent approximation is generally known to bias parameter estimates , but can yield the correct solutions under the assumptions we make .Readers of  will likely be left with the incorrect impression that CE-EM is an inferior method that would perform poorly in practice. The objective of our experiments is to demonstrate that the CE-EM algorithm is capable of identifying high-dimensional nonlinear systems in partially observed settings. We do so in simulation by identifying the parameters of a system of partially observed coupled Lorenz attractors, as well as by identifying the dynamics of a real aerobatic helicopter. In the second experiment, we build on previous analysis of the dataset  by attempting to characterize the interaction of the helicopter with the fluid around it, without having any direct observation of the fluid state. Instructions for reproducing all experiments are included in the supplementary material.In this experiment, we show that:1. CE-EM learns unbiased parameter estimates of systems that are close to deterministic, and, 2. CE-EM scales to high-dimensional problems in which particle-based methods can be intractable.To justify these claims, we use a system that is sufficiently non-linear and partially observable to make particle-based smoothing methods intractable. We choose a system of coupled Lorenz attractors for this purpose, owing to their ability to exhibit chaotic behavior and their use in nonlinear atmospheric and fluid flow models . Arbitrary increases in state dimensionality can be achieved by coupling multiple individual attractors. The state of a system with K coupled Lorenz attractors isThe dynamics of the system are as follows:\u1e8bx =\u1e8b + Hxwhere H is an R 3K\u00d73K matrix.We nominally set the parameters (\u03c3 k , \u03c1 k , \u03b2 k ) to the values (10, 28, 8/3), and randomly sample the entries of H from a normal distribution to generate chaotic and coupled behavior between attractors, while avoiding self-coupling. These parameters are estimated during identification. In order to make the system partially observed, the observation y \u2208 R (3K\u22122) is derived from x as follows:where C \u2208 R (3K\u22122)\u00d73K is a known matrix with full rowrank, and v is the observation noise sampled from a Gaussian with diagonal covariance \u03c3 2 v I. The entries of C are also randomly sampled from a standard normal distribution. In the following experiments, we simulate the system for T = 128 timesteps at a sample rate of \u2206t = 0.04s, and integrate the system using a 4th-order Runge-Kutta method. Initial conditions for each trajectory are sampled such thatTo test the conditions under which CE-EM learns unbiased parameter estimates, we simulate a single Lorenz system with H = 0 and known C \u2208 R 2\u00d73 . We introduce and vary the process noise w \u223c N (0, \u03c3 2 w I), and vary the observation noise coefficient \u03c3 v , and then attempt to estimate the parameters (\u03c3, \u03c1, \u03b2). Using initial guesses within 10% of the system's true parameter values, we run CE-EM on a single sampled trajectory. For each choice of \u03c3 w and \u03c3 v , we repeat this process for 10 random seeds.  shows the mean and standard errors of parameter estimates for various \u03c3 w and \u03c3 v . We highlight in red the mean estimates that are not within two standard errors of their true value. We see that \u03c3 and \u03c1 are estimated without bias for all scenarios. However, the estimate of \u03b2 appears to become biased as the process noise is increased, but not as the observation noise is increased. This supports the assumption that the objective used in CE-EM is sound when systems evolve close to deterministically, but can be biased if it is not.In Section 2, we discussed methods for parameter estimation in state-space systems that are based on particle-filtering and smoothing . Since in their E-step, these methods approximate the distribution over unobserved state-trajectories as opposed to only their point estimate, such methods can be asymptotically unbiased. However, for a finite number of particles, such methods can result in high-variance estimates. In this experiment, we compare the bias resulting from using CE-EM with the variance of using a state-of-the-art Particle EM algorithm.We attempt to identify the parameters (\u03c3, \u03c1, \u03b2) of the same single Lorenz system as in Section 4.1.1. However, we introduce process noise w \u223c N (0, 0.1 2 I) and observation noise v \u223c N (0, 0.5 2 I). We use a training dataset of four trajectories sampled with conditions specified in Section 4.1.The performance of Particle EM can vary substantially depending on implementation of the particle filter and smoother in the E-step, and implementation of the Mstep. We use a fully-adapted particle filter with systematic and adaptive resampling, following the recommendations of ; . Furthermore, we use the FFBSi algorithm  in order to generate iid samples of smoothed state-trajectories, while avoiding complexity that is quadratic in the number of particles experienced by the forward filter-backward smoother (FFBSm) approach . We then use Stochastic Approximation EM (SAEM)  to perform the M-step.  We use N p = 100 particles for filtering, and sample N s = 10 smoothed trajectories using FFBSi.  shows the estimated parameters versus EM epoch using CE-EM and Particle EM. We plot learning curves for 10 random seeds, each of which initializes parameter estimates to within 10% of their true value, and uses a different set of training trajectories. We see that CE-EM consistently converges to accurate parameter estimates in approximately 5 epochs. Estimates of Particle EM appear to initially diverge but in all but one case converge to a similar accuracy in 50 epochs. Furthermore, since the com-   , 3 the runtime per epoch of Particle EM is 47\u00d7 more than that of CE-EM.  Since the variance of particle-based methods generally increases with the effective dimension of the system, the bias induced by CE-EM may be a worthwhile trade-off for fast and reliable parameter estimation.To demonstrate that CE-EM is capable of identifying highdimensional systems, we show that we can estimate the dynamics of an 18 dimensional system of six coupled Lorenz attractors. Moreover, we test whether CE-EM converges to more accurate estimates when more trajectories are provided. To test these claims, we sample 2, 4, and 8 trajectories from a deterministic system with parameters \u03b8 true , and \u03c3 v = 0.01. We randomly initialize each element of the parameters being optimized (\u03b8 = [\u03c3 1:K , \u03c1 1:K , \u03b2 1:K , H]) to within 10% of the their value in \u03b8 true . We then run CE-EM on each batch, tracking the error in the estimated dynamics as training proceeds. We measure this error, which we call (\u03b8), as follows: In the learning step, we do not regularize \u03b8 to a prior and set \u03c1 \u03b8 = 0.For comparison, we also run the same Particle EM implementation as in the previous experiment on this problem. We use the same N p , N s , and hyperparameters as before.  shows the results of this experiment for four random seeds for each batch size. We can see that, as the number of trajectories used in training increases, the error in the estimated dynamics tends toward zero. Furthermore, we see that CE-EM convergences monotonically to a local optimum in all cases. In contrast, Particle EM appears to initially improve but then converges to very poor parameter estimates.The experiments conducted thus far have demonstrated that CE-EM can learn unbiased parameter estimates of nearlydeterministic systems, and can scale to high-dimensional problems for which particle-based methods are intractable. In the next experiment, we use CE-EM to characterize the effect of unobserved states on the dynamics of an aerobatic helicopter.Characterizing the dynamics of a helicopter undergoing aggressive aerobatic maneuvers is widely considered to be a challenging system-identification problem . The primary challenge is that the forces on the helicopter depend on the induced state of the fluid around it. The state of the fluid cannot be directly observed and its dynamics model is unknown. Merely knowing the state of the helicopter and the control commands at a given time does not contain enough information to accurately predict the forces that act on it.In order to address this issue,  use an approach based on Takens theorem, which suggests that a system's state can be reconstructed with a finite number of lagged-observations of it . Instead of attempting to estimate the unobserved fluid state, they directly learn a mapping from a 0.5 s history of observed state measurements and control commands to the forces acting on the helicopter.This approach is equivalent to considering the past 0.5 s of observations as the system's state. However, it can require a very large number of lagged observations to represent complex phenomena. In reality, the characteristic time of unsteady flows around helicopters can easily be up to tens of seconds. Having such a high dimensional state can make the control design and state-estimation more complicated. To avoid large input dimensions, a trade-off between the duration of the history and sample frequency is necessary. This trade-off will either hurt the resolution of low-frequency content or will alias high-frequencies. We attempt to instead explicitly model the unobserved states affecting the system.The objective of this learning problem is to predict y t , the helicopter's acceleration at time t, from an input vector u t containing the current measured state of the helicopter (its velocity and rotation rates) and the control commands.We use data collected by the Stanford Autonomous Helicopter Project . Trajectories are split into 10 s long chunks and then randomly distributed into train, test, and validation sets according to the established protocol  and summarized in Appendix A.1. The train, test, and validation sets respectively contain 466, 100, and 101 trajectories of 500 time-steps each.A simple success metric on a given trajectory is the root mean squared prediction error,where y  t is the measured force from the dataset, y  t is the force predicted by the model, and T is the number of time-steps in each trajectory.Naive: We first consider a naive baseline that does not attempt to account for the time-varying nature of the fluidstate. We train a neural-network to map only the current helicopter state and control commands to the accelerations:where NN \u03b8n is a neural-network with parameters \u03b8 n . We refer to this model as the naive model.We also compare to the work of . They predict y t using a time-history u t\u2212H:t of H = 25 lagged observations of the helicopter's measured  state and control commands. This input is passed through a ReLU-activated neural network with a single hidden-layer combined with what they call a Quadratic Lag Model. As a baseline, we reproduce their performance with a single deep neural network y t = NN \u03b8 h (u t\u2212H:t ) with parameters \u03b8 h . We call this neural network model the H25 model. Both of these models can be trained via stochastic gradient descent to minimize the Mean-Squared-Error (MSE) of their predictions for y. The optimization methodology for these models is described in Appendix A.2.As a third baseline, we compare with subspaceidentification methods . We let\u1ef9 t = y t \u2212 NN \u03b8n (u t ) be the prediction errors of the trained naive model. We use the MATLAB command n4sid to fit a linear dynamical system of the following form:Here, x \u2208 R d is the unobserved state with arbitrary dimension d. The learned parameters areWe use a state dimension of 10 and call this model the SID model. The n4sid algorithm scales super-linearly with the amount of data supplied, and thus we train on 10 randomly sampled subsets of 100 trajectories each, and report the distribution in prediction performance. This approach fits a linear system to the residual error of the naive model, therefore the prediction of y t obtained from the SID model is a nonlinear function of the states x t .We also train an LSTM  on the residual time-series\u1ef9 1:T and u 1:T .We use CE-EM to train a non-linear SSM. Similar to the parameterization used for subpace-identification, we fit the prediction errors of the naive model using the following dynamical system:where NN\u03b8 NL is a neural network, and \u03b8 NL = [A NL , B NL , C NL , D NL ,\u03b8 NL ] are the learned parameters. We introduce non-linearity only in the observation function because it is known from Koopman theory that a non-linear system can be approximated by a high-dimensional linear system provided the correct non-linear mapping between them .While learning, we assume that both process and observation noise are distributed with diagonal Gaussian covariance matrices \u03c3 w I and \u03c3 v I respectively. The values of \u03c3 w and \u03c3 v are treated as hyperparmeters of CE-EM, and are both set to 1. Here as well, we use a state dimension of 10 and call this model the NL model. The optimization methodology for this model is described in Appendix A.2.It should be noted that the system we learn need not actually correspond to an interpretable model of the fluid-state, but only of time-varying hidden-states that are useful for predicting the accelerations of the helicopter. Expert knowledge of helicopter aerodynamics could be used to further inform a gray-box model trained with CE-EM.The test RMSE of the naive, H25, and LSTM models can be evaluated directly on the test trajectories using next-step prediction. However, the SID and NL models require an estimate of the unobserved state before making a prediction. The natural analog of next-step prediction is extended Kalman filtering (EKF), during which states are recursively predicted and corrected given observations. At a given timestep, a prediction of\u1ef9 t is made using the current estimate of x t , and is used in the computation of RMSE. The stateestimate is then corrected with the measured\u1ef9 t .  shows the RMSE of the compared models on trajectories in the test-set. We see that the NL model is able to consistently predict the accelerations on the helicopter with better accuracy than any of the other models. The naive model performs on average 2.9 times worse than the H25 model, and its results can be found in Appendix A.3. The LSTM model also performs poorly, on average 1.4 times worse than the H25 model. The SID model notably outperforms the state-of-the-art H25 model, suggesting that a large linear dynamical system can be used to approximate a non-linear and partially observable system . However, introducing non-linearity as in the NL model noticeably improves performance.  depicts the errors in prediction over a sample trajectory in the test-set. Here, we also see that the NL model is able to attenuate the time-varying error present in predictions made by the H25, suggesting that it has accurately characterized the dynamics of unobserved, time-varying states.This experiment validates the effectiveness of CE-EM to identify a non-linear dynamical model of unobserved states that affect the forces acting an aerobatic helicopter.This paper presented an algorithm for system identification of non-linear systems given partial state observations. The algorithm optimizes system parameters given a time history of observations by iteratively finding the most likely state-history, and then using it to optimize the system parameters. The approach is particularly well suited for highdimensional and nearly deterministic problems.In simulated experiments on a partially observed system of coupled Lorenz attractors, we showed that CE-EM can perform identification on a problem that particle-based EM methods are ill-suited for. However, we also find that CE-EM yields biased parameter estimates in the presence of large process noise. This bias can be partially mitigated by locally approximating the posterior state-marginals as Gaussian, as is done by MAP-EM . We then used the algorithm to model the timevarying hidden-states that affect the dynamics of an aerobatic helicopter. The model trained with CE-EM outperforms state-of-the-art methods because it is able to fit large non-linear models to unobserved states.Numerous system-identification problems can be studied using CE-EM. Recently, there have been tremendous efforts to characterize predictive models for the spread of COVID-19 . Limited capacity for testing the prevalence of the disease makes relevant states partially observed, and thus CE-EM may be useful for its modeling. We also hope to apply CE-EM to very high-dimensional systems with sparsely coupled dynamics using general-form consensus optimization .", "videoStruct": [{"timeStart": "00-00-01", "timeEnd": "00-00-05", "sentence": "my name is Khan Amanda and I'll be talking about the scalable"}, {"timeStart": "00-00-05", "timeEnd": "00-00-09", "sentence": "identification of partially observed systems using certainty equivalent"}, {"timeStart": "00-00-09", "timeEnd": "00-00-10", "sentence": "expectation maximization"}, {"timeStart": "00-00-11", "timeEnd": "00-00-16", "sentence": "we are interested in learning models of dynamical systems in order to enable"}, {"timeStart": "00-00-16", "timeEnd": "00-00-19", "sentence": "autonomous agents to act intelligent Lee"}, {"timeStart": "00-00-19", "timeEnd": "00-00-22", "sentence": "and so imagine we have this robot over here"}, {"timeStart": "00-00-22", "timeEnd": "00-00-26", "sentence": "is trying to decide between choosing this red or the screen button"}, {"timeStart": "00-00-27", "timeEnd": "00-00-28", "sentence": "it would be useful if"}, {"timeStart": "00-00-29", "timeEnd": "00-00-34", "sentence": "whether pressing this red button is going to take it to favorable or unfavorable sets"}, {"timeStart": "00-00-35", "timeEnd": "00-00-39", "sentence": "to real world applications of such model predictive control"}, {"timeStart": "00-00-39", "timeEnd": "00-00-43", "sentence": "autonomous driving and more recently the covert nineteen pandemic"}, {"timeStart": "00-00-43", "timeEnd": "00-00-48", "sentence": "it would be great if we had a model that could tell us exactly how much we could reopen"}, {"timeStart": "00-00-49", "timeEnd": "00-00-52", "sentence": "while constraining the load on the health care system"}, {"timeStart": "00-00-54", "timeEnd": "00-00-58", "sentence": "so in learning more love a dynamical system imagine we're trying to learn"}, {"timeStart": "00-00-58", "timeEnd": "00-01-00", "sentence": "the dynamics of this basketball over here"}, {"timeStart": "00-01-00", "timeEnd": "00-01-02", "sentence": "you would typically go out and collect"}, {"timeStart": "00-01-02", "timeEnd": "00-01-04", "sentence": "trajectory"}, {"timeStart": "00-01-04", "timeEnd": "00-01-06", "sentence": "of how the state's evolved over time"}, {"timeStart": "00-01-06", "timeEnd": "00-01-10", "sentence": "and we will try to do is parameter is some model data"}, {"timeStart": "00-01-11", "timeEnd": "00-01-14", "sentence": "the next state given the current state"}, {"timeStart": "00-01-14", "timeEnd": "00-01-17", "sentence": "and then we would do more maximum likelihood estimation"}, {"timeStart": "00-01-17", "timeEnd": "00-01-19", "sentence": "so we would try to try to find"}, {"timeStart": "00-01-19", "timeEnd": "00-01-23", "sentence": "choice of model parameters that maximize the likelihood of the trajectory that we saw"}, {"timeStart": "00-01-24", "timeEnd": "00-01-28", "sentence": "however were interested in systems that are partially absorbed"}, {"timeStart": "00-01-28", "timeEnd": "00-01-31", "sentence": "so imagine that this basketball was spinning"}, {"timeStart": "00-01-31", "timeEnd": "00-01-33", "sentence": "and so it has some angular velocity"}, {"timeStart": "00-01-33", "timeEnd": "00-01-39", "sentence": "a lot of us have probably seen a video of a basketball being thrown off a dam"}, {"timeStart": "00-01-39", "timeEnd": "00-01-43", "sentence": "and know that if you introduce this angular velocity can dramatically affect the dynamics"}, {"timeStart": "00-01-43", "timeEnd": "00-01-47", "sentence": "and so in order to learn the dynamics in such a situation"}, {"timeStart": "00-01-47", "timeEnd": "00-01-50", "sentence": "we would need to reason about what the angular velocity was"}, {"timeStart": "00-01-50", "timeEnd": "00-01-55", "sentence": "and would typically needs to take the expectation and integrate"}, {"timeStart": "00-01-56", "timeEnd": "00-01-59", "sentence": "and we would do this typically with expectation maximization"}, {"timeStart": "00-01-59", "timeEnd": "00-02-01", "sentence": "but this can be intractable in high dimensions"}, {"timeStart": "00-02-02", "timeEnd": "00-02-06", "sentence": "so we study a certainty equivalent approximation"}, {"timeStart": "00-02-06", "timeEnd": "00-02-09", "sentence": "where instead of trying to characterize the distribution"}, {"timeStart": "00-02-09", "timeEnd": "00-02-11", "sentence": "over the trajectory"}, {"timeStart": "00-02-11", "timeEnd": "00-02-12", "sentence": "of States that we didn't see"}, {"timeStart": "00-02-12", "timeEnd": "00-02-15", "sentence": "we simply look for that most likely explanation"}, {"timeStart": "00-02-15", "timeEnd": "00-02-17", "sentence": "so we find that this"}, {"timeStart": "00-02-17", "timeEnd": "00-02-19", "sentence": "approach to e m"}, {"timeStart": "00-02-19", "timeEnd": "00-02-24", "sentence": "scales very well but can introduce a small degree of model bias which we study in our experiments"}, {"timeStart": "00-02-24", "timeEnd": "00-02-26", "sentence": "and we study"}, {"timeStart": "00-02-27", "timeEnd": "00-02-31", "sentence": "a system of coupled Lorenz attractors so as a simulated experiment"}, {"timeStart": "00-02-31", "timeEnd": "00-02-35", "sentence": "and we also look at a real world application to an aerobatic helicopter"}, {"timeStart": "00-02-37", "timeEnd": "00-02-40", "sentence": "are a common challenge in learning dynamical systems"}, {"timeStart": "00-02-40", "timeEnd": "00-02-43", "sentence": "so again looking at the cove are nineteen pandemic"}, {"timeStart": "00-02-43", "timeEnd": "00-02-51", "sentence": "ah the hidden States of the true number of infections the true number of recovered individuals and perhaps an accurate estimate of the amount of social distancing"}, {"timeStart": "00-02-52", "timeEnd": "00-02-56", "sentence": "these quantities in order to be able to learn accurate models"}, {"timeStart": "00-02-57", "timeEnd": "00-02-59", "sentence": "an aerodynamic systems"}, {"timeStart": "00-03-00", "timeEnd": "00-03-02", "sentence": "typically"}, {"timeStart": "00-03-02", "timeEnd": "00-03-06", "sentence": "shed board sees into the fluid behind them and those vertices can in turn affect"}, {"timeStart": "00-03-06", "timeEnd": "00-03-07", "sentence": "the dynamics of the vehicle"}, {"timeStart": "00-03-07", "timeEnd": "00-03-09", "sentence": "temperature is also"}, {"timeStart": "00-03-09", "timeEnd": "00-03-12", "sentence": "a key variable that sometimes we may not have in our data sets"}, {"timeStart": "00-03-14", "timeEnd": "00-03-17", "sentence": "and financial time series we might want to predict"}, {"timeStart": "00-03-17", "timeEnd": "00-03-20", "sentence": "how the value of the company evolved over time"}, {"timeStart": "00-03-20", "timeEnd": "00-03-24", "sentence": "and we would ideally want to know its cash flows the"}, {"timeStart": "00-03-24", "timeEnd": "00-03-26", "sentence": "the demand for its product as well as"}, {"timeStart": "00-03-26", "timeEnd": "00-03-27", "sentence": "as its supply"}, {"timeStart": "00-03-27", "timeEnd": "00-03-32", "sentence": "but sometimes we may only have access to the markets perception of what the company's values"}, {"timeStart": "00-03-34", "timeEnd": "00-03-36", "sentence": "so the problem statement here"}, {"timeStart": "00-03-36", "timeEnd": "00-03-37", "sentence": "is that"}, {"timeStart": "00-03-37", "timeEnd": "00-03-39", "sentence": "imagine we have some hidden state"}, {"timeStart": "00-03-39", "timeEnd": "00-03-41", "sentence": "that evolves over time"}, {"timeStart": "00-03-41", "timeEnd": "00-03-45", "sentence": "ah and what we have access to our observations life of that state"}, {"timeStart": "00-03-45", "timeEnd": "00-03-47", "sentence": "as well as the inputs"}, {"timeStart": "00-03-47", "timeEnd": "00-03-48", "sentence": "that we drove the system with"}, {"timeStart": "00-03-50", "timeEnd": "00-03-52", "sentence": "so we want to perform system identification"}, {"timeStart": "00-03-52", "timeEnd": "00-03-55", "sentence": "so we wanted to learn the model half of data"}, {"timeStart": "00-03-55", "timeEnd": "00-03-57", "sentence": "which maps a state through the next day"}, {"timeStart": "00-03-57", "timeEnd": "00-04-03", "sentence": "as well as the model g if data which maps the state through the observation that we saw"}, {"timeStart": "00-04-04", "timeEnd": "00-04-06", "sentence": "so given the trajectory"}, {"timeStart": "00-04-07", "timeEnd": "00-04-09", "sentence": "observations y wanted"}, {"timeStart": "00-04-09", "timeEnd": "00-04-10", "sentence": "inputs you wanted to"}, {"timeStart": "00-04-11", "timeEnd": "00-04-13", "sentence": "and a parameter eyes model"}, {"timeStart": "00-04-13", "timeEnd": "00-04-19", "sentence": "so f if data maps the state and input to do the next day but corrupted by some process nice w"}, {"timeStart": "00-04-20", "timeEnd": "00-04-22", "sentence": "as well as an observation model"}, {"timeStart": "00-04-22", "timeEnd": "00-04-26", "sentence": "JF data which maps the state and input to the observation but corrupted by observation noise"}, {"timeStart": "00-04-30", "timeEnd": "00-04-32", "sentence": "maximum likelihood estimate of the model parameters"}, {"timeStart": "00-04-32", "timeEnd": "00-04-37", "sentence": "so we want to choose the model parameters that maximize the likelihood of the observations we saw"}, {"timeStart": "00-04-39", "timeEnd": "00-04-46", "sentence": "however we would need to reason about what those city States were so typically we will try to integrate out of the joint probability distribution"}, {"timeStart": "00-04-46", "timeEnd": "00-04-48", "sentence": "the trajectories over hidden States"}, {"timeStart": "00-04-49", "timeEnd": "00-04-52", "sentence": "the procedure for doing this is typically expectation maximization"}, {"timeStart": "00-04-53", "timeEnd": "00-04-54", "sentence": "so in the east ep"}, {"timeStart": "00-04-54", "timeEnd": "00-04-57", "sentence": "we would hold the model parameters constant"}, {"timeStart": "00-04-57", "timeEnd": "00-04-59", "sentence": "and then try to characterize the distribution"}, {"timeStart": "00-04-59", "timeEnd": "00-05-01", "sentence": "over the trajectories of hidden States"}, {"timeStart": "00-05-02", "timeEnd": "00-05-07", "sentence": "and then using that distribution we can compute the expectation for a new choice of parameters"}, {"timeStart": "00-05-07", "timeEnd": "00-05-10", "sentence": "the joint log probability of States and observations"}, {"timeStart": "00-05-11", "timeEnd": "00-05-14", "sentence": "we can then optimize this function"}, {"timeStart": "00-05-14", "timeEnd": "00-05-16", "sentence": "to find a new choice of parameters in the m step"}, {"timeStart": "00-05-17", "timeEnd": "00-05-19", "sentence": "by iterating through these two steps"}, {"timeStart": "00-05-19", "timeEnd": "00-05-21", "sentence": "we will converge on a local optimum"}, {"timeStart": "00-05-21", "timeEnd": "00-05-24", "sentence": "however for nonlinear dynamical systems"}, {"timeStart": "00-05-24", "timeEnd": "00-05-27", "sentence": "we cannot analytically compute this distribution"}, {"timeStart": "00-05-27", "timeEnd": "00-05-29", "sentence": "overstate trajectories"}, {"timeStart": "00-05-29", "timeEnd": "00-05-35", "sentence": "and so the typical approach to dealing with such a situation is to use something called particle m"}, {"timeStart": "00-05-35", "timeEnd": "00-05-37", "sentence": "so we here a sequential Monte Carlo"}, {"timeStart": "00-05-37", "timeEnd": "00-05-41", "sentence": "is used in order to approximately characterize the distribution"}, {"timeStart": "00-05-41", "timeEnd": "00-05-43", "sentence": "over hidden States in the east ep"}, {"timeStart": "00-05-43", "timeEnd": "00-05-47", "sentence": "and then in the m step an update is made using that approximate distribution"}, {"timeStart": "00-05-48", "timeEnd": "00-05-53", "sentence": "however this can be intractable on high dimensional problems a lot of work has gone in"}, {"timeStart": "00-05-53", "timeEnd": "00-05-57", "sentence": "scaling this sort of fun approach to a specific set of systems"}, {"timeStart": "00-05-57", "timeEnd": "00-06-02", "sentence": "but on a general problem if you can require a large number of particles"}, {"timeStart": "00-06-02", "timeEnd": "00-06-04", "sentence": "in order to characterize the distribution of wealth"}, {"timeStart": "00-06-04", "timeEnd": "00-06-09", "sentence": "and it can also be unstable because we were doing stochastic approximation m"}, {"timeStart": "00-06-09", "timeEnd": "00-06-13", "sentence": "and it can acquire a lot of time or effort to get this studer lively converge"}, {"timeStart": "00-06-15", "timeEnd": "00-06-17", "sentence": "so in this work we study"}, {"timeStart": "00-06-17", "timeEnd": "00-06-19", "sentence": "the certainty equivalent approximation"}, {"timeStart": "00-06-19", "timeEnd": "00-06-23", "sentence": "so essentially what was saying is instead of using sequential Monte Carlo"}, {"timeStart": "00-06-23", "timeEnd": "00-06-26", "sentence": "the compute this approximate posterior distribution over state trajectories"}, {"timeStart": "00-06-26", "timeEnd": "00-06-31", "sentence": "what if we just looked for a one point estimate so if we look for the most likely explanation"}, {"timeStart": "00-06-31", "timeEnd": "00-06-36", "sentence": "but turns out that we can use nonlinear least squares to find this most likely explanation"}, {"timeStart": "00-06-36", "timeEnd": "00-06-39", "sentence": "and nonlinear least squares introduces tractability"}, {"timeStart": "00-06-39", "timeEnd": "00-06-42", "sentence": "it can scale very well to even high dimensional problems"}, {"timeStart": "00-06-42", "timeEnd": "00-06-43", "sentence": "but this out of an approach"}, {"timeStart": "00-06-43", "timeEnd": "00-06-47", "sentence": "we'll introduce some degree of model by us which we will study in our expense"}, {"timeStart": "00-06-49", "timeEnd": "00-06-52", "sentence": "so the first thing to do and this approach"}, {"timeStart": "00-06-52", "timeEnd": "00-06-56", "sentence": "is to characterize and negative log likelihood objectives so this is a joint objective"}, {"timeStart": "00-06-56", "timeEnd": "00-06-58", "sentence": "over the hidden state trajectory"}, {"timeStart": "00-06-58", "timeEnd": "00-07-00", "sentence": "as well as the model parameters"}, {"timeStart": "00-07-00", "timeEnd": "00-07-03", "sentence": "so the first component of this objective"}, {"timeStart": "00-07-03", "timeEnd": "00-07-04", "sentence": "is the observation discrepancy"}, {"timeStart": "00-07-05", "timeEnd": "00-07-07", "sentence": "so if we were to choose some set of parameters"}, {"timeStart": "00-07-07", "timeEnd": "00-07-09", "sentence": "and some trajectory of hidden States"}, {"timeStart": "00-07-10", "timeEnd": "00-07-13", "sentence": "do those hidden States predict the observations that we saw"}, {"timeStart": "00-07-14", "timeEnd": "00-07-16", "sentence": "we also add the dynamic discrepancy"}, {"timeStart": "00-07-17", "timeEnd": "00-07-19", "sentence": "if you have two States"}, {"timeStart": "00-07-19", "timeEnd": "00-07-21", "sentence": "and exit plus one"}, {"timeStart": "00-07-21", "timeEnd": "00-07-22", "sentence": "and some choice of model parameters"}, {"timeStart": "00-07-23", "timeEnd": "00-07-25", "sentence": "how well does exit e predict"}, {"timeStart": "00-07-27", "timeEnd": "00-07-29", "sentence": "so we can find votes"}, {"timeStart": "00-07-29", "timeEnd": "00-07-31", "sentence": "the set of hidden States"}, {"timeStart": "00-07-31", "timeEnd": "00-07-35", "sentence": "as well as the model parameters using block coordinate descent on subjective"}, {"timeStart": "00-07-37", "timeEnd": "00-07-39", "sentence": "so in the east ep"}, {"timeStart": "00-07-39", "timeEnd": "00-07-43", "sentence": "what we would do is hold the choice of parameters constant and try to find the hidden States"}, {"timeStart": "00-07-43", "timeEnd": "00-07-45", "sentence": "that minimize the objective"}, {"timeStart": "00-07-45", "timeEnd": "00-07-48", "sentence": "however since what typically dealing with a non convex problem"}, {"timeStart": "00-07-48", "timeEnd": "00-07-50", "sentence": "to get this algorithm to converge"}, {"timeStart": "00-07-50", "timeEnd": "00-07-53", "sentence": "we will need to add a proximal regulation"}, {"timeStart": "00-07-53", "timeEnd": "00-07-56", "sentence": "it's at this dumb basically just says"}, {"timeStart": "00-07-56", "timeEnd": "00-07-59", "sentence": "ah that don't stray too far away from the previous guest"}, {"timeStart": "00-08-00", "timeEnd": "00-08-04", "sentence": "then in the m step we hold that state trajectory that we just found constant"}, {"timeStart": "00-08-04", "timeEnd": "00-08-06", "sentence": "and optimize the parameters"}, {"timeStart": "00-08-06", "timeEnd": "00-08-09", "sentence": "with a proximal regular is a sham"}, {"timeStart": "00-08-09", "timeEnd": "00-08-14", "sentence": "and we iterate with me in this e step and am step until we converge to global Optima"}, {"timeStart": "00-08-15", "timeEnd": "00-08-19", "sentence": "this summarizes the certainty equivalent am algorithm"}, {"timeStart": "00-08-20", "timeEnd": "00-08-22", "sentence": "so an experimental validation"}, {"timeStart": "00-08-23", "timeEnd": "00-08-25", "sentence": "we explore"}, {"timeStart": "00-08-25", "timeEnd": "00-08-27", "sentence": "by comparing algorithm to partical m"}, {"timeStart": "00-08-27", "timeEnd": "00-08-31", "sentence": "the bias variance trade-off of making the approximation that we did"}, {"timeStart": "00-08-31", "timeEnd": "00-08-32", "sentence": "as well as performance"}, {"timeStart": "00-08-33", "timeEnd": "00-08-35", "sentence": "is a Lawrence attractor system"}, {"timeStart": "00-08-35", "timeEnd": "00-08-37", "sentence": "so this is a highly nonlinear system"}, {"timeStart": "00-08-37", "timeEnd": "00-08-38", "sentence": "that has chaotic dynamics"}, {"timeStart": "00-08-40", "timeEnd": "00-08-48", "sentence": "so first let's look at our algorithm alone and see what happens as we vary the amount of process nice and the amount of observation noise"}, {"timeStart": "00-08-48", "timeEnd": "00-08-51", "sentence": "from the true system that we generate trajectories from"}, {"timeStart": "00-08-52", "timeEnd": "00-08-54", "sentence": "is that I would approach"}, {"timeStart": "00-08-54", "timeEnd": "00-08-57", "sentence": "is able to reliably converge to the global optimum"}, {"timeStart": "00-08-57", "timeEnd": "00-08-59", "sentence": "with some small degree of variance"}, {"timeStart": "00-09-00", "timeEnd": "00-09-03", "sentence": "however as we crank up the process nice"}, {"timeStart": "00-09-03", "timeEnd": "00-09-08", "sentence": "ah in the true system we actually find that our approach leads to some degree of expected model"}, {"timeStart": "00-09-12", "timeEnd": "00-09-18", "sentence": "we also find that from a variety of initial conditions we robustly converge to the global optimum in less than five epoxy frame"}, {"timeStart": "00-09-19", "timeEnd": "00-09-22", "sentence": "when we come back this two particle yam we find that"}, {"timeStart": "00-09-22", "timeEnd": "00-09-26", "sentence": "Berkeley am actually initially diverged from the global optimal solutions"}, {"timeStart": "00-09-26", "timeEnd": "00-09-28", "sentence": "but over time will converge"}, {"timeStart": "00-09-28", "timeEnd": "00-09-29", "sentence": "over the course of maybe one hundred epochs"}, {"timeStart": "00-09-29", "timeEnd": "00-09-32", "sentence": "to solutions of a similar quality to ours"}, {"timeStart": "00-09-33", "timeEnd": "00-09-39", "sentence": "in terms of poor epoch time we find that our algorithm is able to run forty seven times faster"}, {"timeStart": "00-09-39", "timeEnd": "00-09-43", "sentence": "and this is because non-linear least squares can be implemented much more efficiently"}, {"timeStart": "00-09-43", "timeEnd": "00-09-44", "sentence": "sometimes then"}, {"timeStart": "00-09-44", "timeEnd": "00-09-45", "sentence": "sequential Monte Carlo"}, {"timeStart": "00-09-47", "timeEnd": "00-09-53", "sentence": "let's see if these results scale to higher dimensional problems so we are here we couple six"}, {"timeStart": "00-09-53", "timeEnd": "00-09-55", "sentence": "learning subject ERS together to get a"}, {"timeStart": "00-09-55", "timeEnd": "00-09-57", "sentence": "highly non-linear by high dimensional system"}, {"timeStart": "00-09-59", "timeEnd": "00-10-01", "sentence": "as one would"}, {"timeStart": "00-10-01", "timeEnd": "00-10-03", "sentence": "expect that as we give out algorithm more data"}, {"timeStart": "00-10-03", "timeEnd": "00-10-06", "sentence": "we converge to better and better solutions"}, {"timeStart": "00-10-07", "timeEnd": "00-10-11", "sentence": "we also find that our approach can Mana ta Nik LI converge"}, {"timeStart": "00-10-11", "timeEnd": "00-10-12", "sentence": "to local Optima"}, {"timeStart": "00-10-12", "timeEnd": "00-10-15", "sentence": "ah whereas if we look at particle e m"}, {"timeStart": "00-10-15", "timeEnd": "00-10-20", "sentence": "actually diverges to solutions that are worse than the initial conditions we provided it"}, {"timeStart": "00-10-22", "timeEnd": "00-10-27", "sentence": "in the next experiment we apply our approach to a real world problem"}, {"timeStart": "00-10-27", "timeEnd": "00-10-33", "sentence": "so if a here we look at an aerobatic helicopter so this is a helicopter that has done the kind of back flips and"}, {"timeStart": "00-10-33", "timeEnd": "00-10-35", "sentence": "and other maneuvers in the sky"}, {"timeStart": "00-10-36", "timeEnd": "00-10-40", "sentence": "but while doing so it imparts a significant amount of momentum into the fluid around it"}, {"timeStart": "00-10-40", "timeEnd": "00-10-46", "sentence": "and so we need to reason about what the state of the fluid is if we're going to accurately predict its"}, {"timeStart": "00-10-48", "timeEnd": "00-10-54", "sentence": "so what we explore is can we loan and neural network state space model of the system using our algorithm"}, {"timeStart": "00-10-56", "timeEnd": "00-10-59", "sentence": "so the bass lines that we compare against our first"}, {"timeStart": "00-10-59", "timeEnd": "00-11-01", "sentence": "standard machine learning approaches"}, {"timeStart": "00-11-01", "timeEnd": "00-11-07", "sentence": "so let's just pretend like this wasn't a partially absorb problem and try to map the States that we do not do the next day"}, {"timeStart": "00-11-07", "timeEnd": "00-11-08", "sentence": "so this would be naive regression"}, {"timeStart": "00-11-08", "timeEnd": "00-11-10", "sentence": "we try to make this mapping"}, {"timeStart": "00-11-11", "timeEnd": "00-11-13", "sentence": "we could also use an LSD"}, {"timeStart": "00-11-13", "timeEnd": "00-11-18", "sentence": "that would take a time series of all previous observations to try to predict the next observation you see"}, {"timeStart": "00-11-20", "timeEnd": "00-11-23", "sentence": "we could also use an approach from standard control called subspace identification"}, {"timeStart": "00-11-25", "timeEnd": "00-11-27", "sentence": "and we also compared to the state of the odd approach"}, {"timeStart": "00-11-27", "timeEnd": "00-11-29", "sentence": "which we call the age twenty five model"}, {"timeStart": "00-11-30", "timeEnd": "00-11-35", "sentence": "ah it's actually what we're doing is taking the twenty-five historical observations"}, {"timeStart": "00-11-35", "timeEnd": "00-11-38", "sentence": "and trying to use those through neural network to predict the next observation"}, {"timeStart": "00-11-39", "timeEnd": "00-11-43", "sentence": "we note that this approach is not a state space model"}, {"timeStart": "00-11-44", "timeEnd": "00-11-45", "sentence": "so I would approach"}, {"timeStart": "00-11-45", "timeEnd": "00-11-47", "sentence": "tries to"}, {"timeStart": "00-11-47", "timeEnd": "00-11-49", "sentence": "fit a neural network"}, {"timeStart": "00-11-49", "timeEnd": "00-11-51", "sentence": "a state space model"}, {"timeStart": "00-11-51", "timeEnd": "00-11-54", "sentence": "do this out of a system to reason about what those fluid States could have been"}, {"timeStart": "00-11-54", "timeEnd": "00-11-55", "sentence": "and use this to make predictions"}, {"timeStart": "00-11-57", "timeEnd": "00-12-01", "sentence": "so what we find is that when we look at trajectories in our test set"}, {"timeStart": "00-12-01", "timeEnd": "00-12-03", "sentence": "the model that we learned"}, {"timeStart": "00-12-03", "timeEnd": "00-12-06", "sentence": "actually has the lowest arms arrow across the boat"}, {"timeStart": "00-12-08", "timeEnd": "00-12-11", "sentence": "so what we find is that certainty equals not yet allows the training"}, {"timeStart": "00-12-11", "timeEnd": "00-12-14", "sentence": "of a non-linear neural network model that achieves the best accuracy"}, {"timeStart": "00-12-17", "timeEnd": "00-12-21", "sentence": "is that learning nonlinear dynamical systems from partial observation"}, {"timeStart": "00-12-21", "timeEnd": "00-12-25", "sentence": "requires some form of approximate expectation maximization"}, {"timeStart": "00-12-25", "timeEnd": "00-12-30", "sentence": "and that certainty equivalent am is a tractable algorithm that tends to perform well"}, {"timeStart": "00-12-31", "timeEnd": "00-12-35", "sentence": "some future application areas that were looking into"}, {"timeStart": "00-12-35", "timeEnd": "00-12-36", "sentence": "I predicting the pandemic"}, {"timeStart": "00-12-36", "timeEnd": "00-12-39", "sentence": "so here we are interested in learning epidemiological models"}, {"timeStart": "00-12-39", "timeEnd": "00-12-42", "sentence": "from the incomplete data"}, {"timeStart": "00-12-42", "timeEnd": "00-12-43", "sentence": "results from limited testing"}, {"timeStart": "00-12-44", "timeEnd": "00-12-46", "sentence": "while also looking into distributed versions"}, {"timeStart": "00-12-46", "timeEnd": "00-12-48", "sentence": "of our algorithm"}, {"timeStart": "00-12-48", "timeEnd": "00-12-50", "sentence": "that can scale to even higher dimensional problems"}]}, {"title": "ICRA 2020: Visual-Inertial Mapping with Non-Linear Factor Recovery.", "authors": "cvprtum", "abstract": "", "publicationOrg": "ICRA", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/1904.06504.pdf", "pdfPath": "/data/cache/2/PDFs/ICRA2020VisualInertialMappingwithNonLinearFactorRecovery.pdf", "publicationUrl": "https://arxiv.org/pdf/1904.06504.pdf", "codeUrl": "https://github.com/VladyslavUsenko/basalt-mirror", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/X4rr3HTmHV4", "videoPath": "/data/cache/2/videos/ICRA 2020- Visual-Inertial Mapping with Non-Linear Factor Recovery..mp4", "pdfText": "Visual-inertial odometry (VIO) is a popular approach for tracking the motion of a camera in application domains such as robotics or augmented reality. By combining visual and IMU measurements, one can exploit the complementary strengths of both sensors and thereby increase accuracy and robustness. Commonly, the optimization of camera trajectory and map is performed locally on a small window of recent camera frames and IMU measurements. This approach, however, is inevitably prone to drift in the estimates.Globally consistent optimization for visual-inertial mapping is less explored in the computer vision community. While in principle the optimization could be formulated as bundle adjustment with additional IMU measurements, this approach would quickly become computationally infeasible due to the high number of frames which would lead to a large number of optimization parameters in a naive formulation. To keep the computational burden in bounds, bundle adjustment subsamples the high-frame rate images of the camera to a smaller set of keyframes. The common choice in VIO is to preintegrate IMU measurements between consecutive frames. If we select keyframes temporally far apart to make the optimization efficient, the preintegrated IMU measurements : Orthographic top-down projection of the map (MH 05 sequence of the EuRoC dataset ) rendered using the estimated gravity direction. To obtain a gravity-aligned globally consistent map, non-linear factors are recovered from the marginalization prior of the VIO and combined with keypointbased bundle adjustment. Green lines visualize keyframe connections resulting from bundle adjustment factors and red lines connections from the recovered relative pose factors. Additionally each keyframe has a recovered factor that penalizes deviation from the gravity direction observed in VIO. provide only little information to constrain the trajectory due to the accumulated sensor noise. The small frame rate also affects the quality of the estimated velocities and biases from visual and inertial cues which are required for pose prediction using preintegrated IMU measurements.We propose a novel approach that formulates visual-inertial mapping as bundle adjustment on a high-frame-rate set of visual and inertial measurements. Instead of directly optimizing the camera trajectory for all frames, we propose a hierarchical approach which first recovers a local VIO estimate at the frame rate of the camera. Once keyframes are removed and marginalized from the current local VIO optimization window, we extract non-linear factors  that approximate the accumulated visual-inertial information about the camera motion between keyframes. The keyframes and non-linear factors are subsequently used on the global bundle-adjustment layer.For the VIO layer, our method uses image features designed for fast and accurate tracking, while for the mapping layer we employ distinctive but lighting and viewpoint invariant keypoints that are suitable for loop closing. With this, our approach can leverage information from the IMU and shortterm visual tracking at high frame rates together with keypoint matching and loop-closing at low frame rates for globally consistent mapping ). The factors also help to keep the map gravity-aligned, bridge between frames that do not have enough visual information. Our approach also makes the optimization problem smaller, since we do not have to estimate velocities and biases.In summary, our contributions are:\u2022 We propose a novel two-layered visual-inertial mapping approach that integrates keypoint-based bundleadjustment with inertial and short-term visual tracking through non-linear factor recovery. \u2022 As the first layer of our mapping approach we propose a VIO system which outperforms the state-of-the-art methods in terms of trajectory accuracy on the majority of the evaluated sequences. This is achieved by carefully combining appropriate components (patch tracking, landmark representation, first-estimate Jacobians, marginalization scheme) as detailed in Sec. IV. \u2022 Unlike other state-of-the-art systems that use preintegrated IMU measurements also for mapping, we subsume high-frame rate visual-inertial information in non-linear factors extracted from the marginalization prior of the VIO layer. This results not only in a smaller optimization problem but also in better pose estimates in the resulting gravity aligned map. We encourage the reader to watch the demonstration video and inspect the open-source implementation of the system, which is available at:https://vision.in.tum.de/research/vslam/basaltVisual-inertial odometry: Early methods for visualinertial odometry are primarily filter-based , . In tightly integrated filters, the prediction step typically propagates the current camera state estimate using the IMU measurements. The state is recursively corrected based on the camera images. A significant drawback of filters is that the linearization point for the non-linear measurement and state transition models cannot be changed, once a measurement is integrated. Fixed-lag smoothers (a.k.a. optimization-based approaches) such as ,  relinearize at the current states in a local optimization window of recent frames. The visual-inertial state estimation is formulated as a full bundle adjustment (BA) over keyframes and IMU measurements. The problem is reduced to a computationally manageable size by marginalization of old frames up to the recent set in the optimization window. The continuous relinearization, windowed optimization and maintenance of the marginalization prior increase the accuracy of the methods. The above methods need to discard keypoints and observations that are observed in marginalized keyframes in order to maintain the sparse structure of the marginalization prior. Hsiung et al.  apply non-linear factor recovery to achieve a sparse marginalization prior without discarding information about observed keypoints. This way, the approach can further refine the keypoints and achieve higher accuracy, but in contrast to our work it is limited to local BA.Visual-inertial mapping: Only few works have tackled globally consistent mapping from visual and inertial measure-ments. Kasyanov et al.  add a pose-graph optimization layer with loop-closing on top of a keyframe-based visualinertial odometry method . The pose graph is built from the keyframes of the VIO and their relative pose estimates. In , the authors add inertial measurements to a keyframe-based SLAM system through IMU preintegration. The IMU measurements are preintegrated into a set of pseudo-measurements between keyframes. They notice that the accuracy of preintegrated measurements degrades over time and restrict the time between keyframes to 0.5 seconds in local BA and 3 seconds in global BA. A further shortcoming of the method is its requirement of estimating the camera velocity and IMU biases at each keyframe which is less well constrained through visual measurements than in our approach due to the strong temporal subsampling into keyframes. Schneider et al.  follow a similar approach in which preintegrated IMU measurements are inserted into the optimization. The approach in  proposes a combination of VIO and 4 degree-offreedom (DoF) pose optimization for visual-inertial mapping. They fix 2 DoF (roll and pitch) and optimize only for the others. We also constrain roll and pitch from visual-inertial measurements. However, we extract non-linear factors in a probabilistic formulation which account for uncertainties in those values and are traded off with other information in the global probabilistic optimization.In this paper, we write matrices as bold capital letters (e.g. R) and vectors as bold lowercase letters (e.g. \u03be). Rigidbody poses are represented as (R, p) \u2208 SO(3) \u00d7 R 3 or as transformation matrices T \u2208 SE(3) when needed. Incrementing a rotation R by an increment \u03be \u2208 R 3 is defined as R \u2295 \u03be = Exp(\u03be)R. The difference between two rotations R 1 and R 2 is calculated as2 ) such that (R \u2295 \u03be) R = \u03be. Here we use Exp :which is a composition of the hat operator (R 3 \u2192 so(3)) and the matrix exponential (so(3) \u2192 SO(3)) and maps rotation vectors to their corresponding rotation matrices, and its inverse Log : SO(3) \u2192 R 3 . For all other variables, such as translation, velocity and biases, we define \u2295 and as regular addition and subtraction.In the following we will use a state s that is defined as a tuple of several rotation and vector variables, and a function r(s) that depends on it and can also produce rotations and vectors as the result. An increment \u03be \u2208 R n is a stacked vector with all the increments of the variables in s. Then, the Jacobian of the function with respect to the increment is defined asHere, s \u2295 \u03be denotes that each component in s is incremented with the corresponding segment in \u03be using the appropriate definition of the \u2295 operator, and similarly for . The limit is done component-wise, such that the Jacobian is a matrix. For Euclidean quantities, this definition is just a normal derivative, with an extension for rotations, both as function value and as function argument. For more details and possible alternative formulations we refer the reader to , , . In non-linear least squares problems, we minimize functions of the formwhich is a squared norm of the sum of residuals with blockdiagonal weight matrix W. In this case, r(s) is purely vector-valued. Near the current state s we can use a linear approximation of the residual, which leads toThe optimum of this approximated energy can be attained using the Gauss-Newton incrementWith this, we can iteratively update the state s i+1 = s i \u2295 \u03be * until convergence.We formulate the incremental motion tracking of the camera-IMU setup over time as fixed-lag smoothing. First, we use patch-based optical flow to track a sparse set of points in the 2D image plane between consecutive frames. This information is then used in a bundle-adjustment framework which for every frame minimizes an error that consists of point reprojection and IMU propagation terms. To maintain a fixed parameter size of the optimization problem we marginalize out old states. In the remainder of this section we will discuss these stages in more detail.As a first step of our algorithm we detect a sparse set of keypoints in the frame using the FAST  corner detector. To track the motion of these points over a series of consecutive frames we use sparse optical flow based on KLT . To achieve fast, accurate and robust tracking we combine the inverse-compositional approach as described in  with a patch dissimilarity norm that is invariant to intensity scaling. Several authors suggested zero-normalized cross-correlation (ZNCC) for illumination-invariant optical flow , , but we use locally-scaled sum of squared differences (LSSD) defined in  which is computationally less expensive than alternatives.We formulate the patch tracking problem as estimating the transform T \u2208 SE(2) between two corresponding patches in two consecutive frames that minimizes the differences between the patches according to the selected norm. Essentially, we minimize a sum of squared residuals, where every residual is defined asHere, I t (x) is the intensity of image t at pixel location x.The set of image coordinates that defines the patch is denoted \u2126 and the mean intensity of the patch in image t is I t . A visualization of the patch and tracking results is shown in . To achieve robustness to large displacements in the image we use a pyramidal approach, where the patch is first tracked on the coarsest level and then on increasingly finer levels. For outlier filtering, instead of an absolute threshold on the error, we track the patches from the current frame to the target frame and back to check consistency. Points that do not return to the initial location with the second tracking are considered as outliers and discarded.To estimate the motion of the camera we combine error terms based on tracked feature locations from KLT tracking with IMU error terms based on preintegrated IMU measurements .We use the following coordinate frames throughout the paper: W is the world frame, I is the IMU frame and C i is the frame of camera i, where i is the index of the camera in a stereo setup. We estimate transformations T WI \u2208 SE(3) from IMU to world coordinate frame. The transformations T ICi from camera frame i to IMU frame and the projection functions \u03c0 i are assumed to be static and known from calibration. For the formulation of reprojection errors we denote the transformations from camera i to world by T WCi . Those do not constitute additional optimization variables and are calculated using T WI and T ICi in practice.At different points in time, we optimize a statewhere s k contains IMU poses for n older keyframes, s f contains IMU poses, velocities and biases of the m most recent frames, which possibly are also keyframes if they host landmarks, and s l contains landmarks. A graphical representation of the problem is shown in . Landmarks are stored relative to the keyframe where they were observed for the first time  and defined by a unit-length direction vector in the coordinate frame of the camera and an inverse distance to the landmark . In the proposed system only keyframes host landmarks, which distinguishes them from regular frames. 1) Representation of Unit Vectors in 3D: In order to avoid the necessity of additional constraints for the optimization and to keep the number of optimiziation variables small, we parametrize the bearing vector in 3D space using a minimal representation, which is two-dimensional. In  the authors provide an extensive review of possible parametrizations and suggest a new parametrization based on SO(3) rotations that yields simple derivatives with respect to 2D increments.In this work we use a parametrization based on stereographic projection that given 2D coordinates (u, v) generates a unit-length bearing vectorThis parametrization is efficient as it only uses simple operations such as multiplication and division (compared to trigonometric operations needed in ) and is defined for all u and v. A geometric interpretation is shown in . The only direction vector that cannot be represented with finite u, v is the negative Z-direction 0 0 \u22121 . However, this is not a drawback in practice, as cameras usually have a limited field of view and cannot see points behind them.2) Reprojection Error: The first cue we can use for motion estimation is the reprojection error. When point i that is hosted in frame h(i) is detected in target frame t at image coordinates z it , the residual is defined aswhere c(t) is the index of the camera used to take frame t. The pose T t denotes T WC c(t) at the time when frame t has been taken, and similarly for T h(i) . The first three entries of the homogeneous point coordinates q i (u, v, d) are computed from the minimal representation (u, v) as described in Sec. IV-B1, with an additional fourth entry d, the inverse distance. Since the projection function is independent of scale we do not have to normalize q i , which makes this formulation numerically stable even when d is close or equal to zero.3) IMU Error: The second cue for motion estimation is the IMU data. To deal with the high frequency of IMU measurements we preintegrate several consecutive IMU measurements into a pseudo-measurement. When adding an IMU factor between frame i and frame j, we compute pseudomeasurement \u2206s = (\u2206R, \u2206v, \u2206p) similar to . For this, we compute bias-corrected accelerations a t = a raw t \u2212b a i and rotational velocities \u03c9 t = \u03c9 raw t \u2212b g i using the raw accelerometer a raw t and gyroscope \u03c9 raw t measurements. We fix the corresponding biasesb a i andb g i for the entire preintegration time and use linear approximation to account for changes in these variables.For the timestamp t i of frame i, we assign the initial state delta \u2206s ti = (I, 0, 0). Then, for each IMU timestamp t satisfying t i < t \u2264 t j the following updates are calculated.This defines \u2206s t+1 as a function of \u2206s t , a t+1 , and \u03c9 t+1 ,with corresponding JacobianFurthermore, all previous iterations of f up to t + 1 define \u2206s t+1 as a function of the biases,Starting with zero-initialization, the corresponding Jacobianwhich results from the chain rule. Eventually, the Jacobians of g tj are denoted J g and J a . Small changes in biases can be represented as increments to the linearization pointwith components \u2206s = (\u2206R, \u2206\u1e7d, \u2206p). The residuals are then calculated aswhere g is the gravity vector and R and p denote the rotation and translation components of T WI , respectively. These residuals have to be weighted with an appropriate covariance matrix, which can be also calculated recursively. Starting from \u03a3 ti = 0, updates are calculated aswith diagonal matrices \u03a3 a and \u03a3 g that contain the hardwarespecific IMU noise parameters for accelerometer and gyroscope. For more detailed information about the underlying physical model of the IMU and preintegration theory we refer the reader to .The reprojection errors are summed over the set of points P and for each point i over the set obs(i) of frames where the point is observed, including its host frame. The set C contains pairs of frames which are connected by IMU factors.The energy E is optimized using the Gauss-Newton algorithm. To constrain the problem size we fix the number of keyframe poses and consecutive states that we optimize at every iteration. When a new frame is added, there are n poseonly keyframes in s k and the m newest frames including the newly added one in s f . After optimizing, we perform a partial marginalization of the state to prevent the problem size from growing.Two possible scenarios for marginalization are shown in . In the first one we marginalize out the oldest non-keyframe. In this case we drop the landmark factors that have this frame as a target to maintain the sparsity of the problem. In the second case we have a new keyframe, so we marginalize out velocity and biases for this frame and one old keyframe with corresponding landmarks.In both cases the marginalization is done on the linearized Markov blanket of the variables we want to remove, where the Markov blanket is a collection of incident states to those variables. The linearization H and b represent a distribution of the estimated state in the vector space of the increment \u03be.If we split the increment \u03be = [\u03be \u03b1 , \u03be \u03b2 ] into variables \u03be \u03b1 to stay in the system and variables \u03be \u03b2 to be marginalized, we can compute the parameters of the new distribution using the Schur complement,where we have split the original H and b intoH m \u03b1\u03b1 and b m \u03b1 now define an energy term that only depends on \u03be \u03b1 and can be added to the total energy at the next iteration.We use first-estimate Jacobians  to maintain the nullspace properties of the linearized marginalization prior. As soon as a variable becomes a part of the marginalization prior, its linearization point is fixed, and the Jacobian used to calculate H and b is evaluated at this linearization point, while the residuals are calculated at the current state estimate. Residuals already in the marginalization term have to be linearly approximated, thus not b m \u03b1 , but b m \u03b1 + H m \u03b1\u03b1 \u03b4 \u03b1 is added to the Gauss-Newton optimization once \u03be \u03b1 deviates by \u03b4 \u03b1 from the state used to calculate the residuals in b m \u03b1 .The fixed-lag smoothing method for visual-inertial odometry (  presented in the previous section accumulates drift in the estimate due to the fixed linearization points outside the optimization window. A typical approach to eliminate such drift is to detect loop closures and incorporate loop-closing constraints into the optimization. We propose a two-layered approach which runs our visual-inertial odometry on the lower layer and bundle-adjustment on the visual-inertial mapping layer, where we additionally use non-linear factors that summarize the keyframe pose information from the odometry layer. BA optimizes the camera poses of keyframes and positions of keypoints. We implicitly detect loop closures using keypoint matching and achieve globally consistent mapping. : Factor graphs. (a) After marginalizing a frame, the system consists of n older keyframes K 1 . . . K n and the m \u2212 1 most recent frames F 1 and F 2 (which could potentially also host landmarks and hence be keyframes). After a new frame has been added, the oldest velocity v and the oldest bias b are marginalized. If they do not belong to a keyframe (b), the whole frame including its pose T is marginalized. If they belong to a keyframe (c), another keyframe is selected for marginalization, including the landmarks hosted in it and its pose. In both cases, reprojection factors where the target frame is the marginalized frame are dropped. In the latter case, reprojection factors from the marginalized frame to F 2 are dropped to allow relinearization. Note that not all possible combinations of host and target frames for reprojection factors are shown. To get statistically independent observations we detect and match ORB  features (distinct from VIO points) between the keyframes in the global map optimization. This allows us to use the reprojection error function as defined in Eq. . Combining this reprojection error with the error terms from the recovered non-linear factors yields the objective function:where E nfr (s) collects the error terms by the recovered nonlinear factors. These factors and their recovery are detailed in the following. The state s that we optimize on this global optimization layer includes the keyframe poses and the positions of the new landmarks (parametrized as in Sec. IV-B1).We interface the global map optimization with the VIO layer at the keyframe poses. When a keyframe is marginalized out from the VIO we save the linearization of the Markov blanket ) and marginalize all other variables except of keyframe poses. From this marginalization prior, we re-cover a set of non-linear factors on the keyframe poses that approximate the distribution stored in it.Non-linear factor recovery (NFR ) approximates a dense distribution stored in the linearized Markov blanket of the original factor graph with a different set of non-linear factors that yield a sparse factor graph topology. While the initial aim of NFR is to keep the computational complexity of SLAM optimization bounded, we use it to transfer information accumulated during VIO to our globally consistent visualinertial map optimization.By linearization of the residual function of a non-linear least squares problem Eq. (2), we obtain a multivariate Gaussian distribution p(s) \u223c N (\u00b5 o , H \u22121 o ) in which the mean \u00b5 o equals the state estimate. We want to construct another distribution p a (s) \u223c N (\u00b5 a , H \u22121 a ) that well approximates the original distribution with a sparser factor graph topology.We follow NFR  and minimize the Kullback-Leibler divergence (KLD) between the recovered distribution and the original distribution. More formally, we minimizewhereand d is constant. For the ith non-linear factor that we want to recover, we need to define a residual function such that r i (s, z i ) = with \u223c N (0, H \u22121 i ). NFR estimates the pseudo measurements z i and information matrices H i for the factors. Choosing z i such that r i (\u00b5 o , z i ) = 0 induces \u00b5 a = \u00b5 o which makes the third term of (27) vanish. To estimate H i we definewhere J r stacks the Jacobians of the defined residual functions with respect to the state, and H r is a block diagonal matrix that consists of the H i for the corresponding residual functions. This allows us to write H a = J r H r J r , and consequently, we can recover the information matrices H i by minimizingFor full-rank and invertible J r , ,  showed that the following closed-form solution exists,where {} i denotes the corresponding diagonal block.When we need to marginalize out a keyframe as shown in , we save the current linearization and marginalize out everything except the keyframe poses. This gives us a factor that densely connects all keyframe poses in the optimization window. We use it to recover non-linear factors between the marginalized keyframe and all other keyframes as shown in . We define the following residual functions:where with xy we denote x and y components of the vector and with z we denote the recovered measurements from the estimated state at the time of linearization. In our caseWe recover pairwise relative-pose factors between the keyframe that we will remove and all other current VIO keyframes. For that keyframe we also recover roll-pitch, absolute position and yaw factors ). This gives us a full-rank invertible Jacobian J r which means that we can use Eq. (30) for recovering information matrices for the factors.Since yaw and absolute position are 4 unobservable states of the VIO, the only information we have there comes from the initial prior on the start pose. As we do not need this information for the global map we drop yaw and absolute position factors, and only take relative pose and roll-pitch factors for the map optimization. With these factors, the energy terms E G nfr become E G nfr (s) =where R is a set of all relative pose factors and P is the set of all roll-pitch factors.VI. EVALUATION To evaluate the presented approach we conduct evaluation on the EuRoC dataset  and compare it to other state-ofthe-art systems. We present the evaluation for both our VIO subsystem and our full visual-inertial mapping approach. Our VIO runs the optimization in a local window of frames and provides a pose for every tracked frame, while the mapping system performs global map optimization for keyframes that were selected by the VIO. To measure the accuracy of the evaluated systems, we use the root mean square (RMS) of the absolute trajectory error (ATE) after aligning the estimates with ground truth. a) System parameters: At the KLT tracking stage the image is divided into a regular grid with the cell size of 50 pixels. For each cell that has no point tracked from the previous frame, one feature point with the best FAST response is extracted (if it exceeds the threshold). With the resolution of the EuRoC dataset it results in 80-120 features tracked by the system at every point in time. At the VIO level we use a window of 7 old keyframes (poses) and 3 latest temporal states (poses, velocities and biases). The newest temporal state is selected as a keyframe if less than 70% of the KLT features are connected to the currently tracked points in the local map. b) Accuracy: The results of the evaluation are summarized in . When considering visual-inertial odometry methods our system shows the best performance on eight out of ten sequences while the closest competitor (VI DSO ) shows the best results on five.To evaluate the mapping part we compare it to the visualinertial version of ORB-SLAM , where the vision subsystem is very similar to the one proposed in our mapping layer (ORB keypoints). The main difference lies in the inertial part where ORB-SLAM uses preintegrated measurements between keyframes, while we use recovered non-linear factors that summarize IMU and visual tracking on the VIO layer.The proposed system clearly outperform ORB-SLAM on the \"machine hall\" sequences where the large scale of the environment results in large time intervals between keyframes. On the \"Vicon room\" sequences the difference is smaller, since the rapid motion of the MAV that carries the camera in a small room results in many keyframes with small time intervals between them.Qualitative results of reconstructed maps are shown in . With the proposed system we are able to reconstruct globally consistent gravity-aligned maps and recover keyframe poses even for segments where no matches between detected ORB features can be estimated. c) Factor Weighting: To evaluate the importance of the extracted factors and their proper weighting in the final mapping results we consider two alternative implementations. In the first one we do not use any factors and rely purely on the BA with ORB features. In the second one we extract the factors, but use identity weights (i.e. H ij = H i = I in Eq. (35)) for all of them, which is a typical approach for pose graph optimization , . The evaluation results presented in    In the lower part we evaluate mapping methods that operate on all keyframes and perform global map optimization. In both evaluations the proposed system shows the lowest error on the majority of the sequences and outperforms the competitors. Note: The V2 03 sequence is excluded from the comparison because it has more than 400 missing frames for one of the cameras.  recovered according to Sec. V results in better accuracy and robustness when compared to those alternatives. d) Timing: The main source of timing improvement for the mapping stage is the fact that for a global optimization requires a 2.5 smaller state (no velocity or biases) compared to the naive IMU integration. In absolute numbers we test our system on an Intel E5-1620 CPU (4 cores, 8 virtual cores). Our implementation is highly parallel and utilizes all available CPU resources. For the VIO the average time per frame on the EuRoC sequences is 7.83 ms (largest: 9.4 ms on MH 02; smallest: 5.5 ms in V1 03). On average 11.5% of the frames are selected as keyframes and proceed to the mapping stage.The timing of the mapping stage is provided in . In particular, for the MH 05 sequence (see , 2273 stereo frames, 114 seconds) the processing takes 19.2 seconds for VIO and 9.7 seconds for mapping for the entire sequence (around 4x faster than real-time playback).In this paper we present a novel approach for visualinertial mapping that combines the strengths of highly accurate visual-inertial odometry with globally consistent keyframebased bundle adjustment. We achieve this in a hierarchical framework that successively recovers non-linear factors from the VIO estimate that summarize the accumulated inertial and visual information between keyframes. VIO is formulated as fixed-lag smoothing which optimizes a set of active recent frames in a sliding window and keeps past information in marginalization priors. The accumulated VIO information between keyframes is extracted and retained for the visualinertial mapping when a keyframe falls outside the window and is marginalized.Compared to alternative approaches that use preintegrated IMU measurements between keyframes our system shows better trajectory estimates on a public benchmark. This formulation has the potential to reduce the computational cost of optimization by reducing the dimensionality of the state space and enable large-scale visual-inertial mapping. Integrating information from other sensor modalities or extending the system for multi-camera settings are interesting directions for future research.", "videoStruct": [{"timeStart": "00-00-01", "timeEnd": "00-00-07", "sentence": "hi everyone in this video will present the visual natural mapping was not linear factor recovery"}, {"timeStart": "00-00-08", "timeEnd": "00-00-14", "sentence": "this work was done by bloody angle Nikolas demo David Schubert Yorkshire and Daniel kramer's"}, {"timeStart": "00-00-16", "timeEnd": "00-00-19", "sentence": "comer based navigation and mapping is important for many applications"}, {"timeStart": "00-00-19", "timeEnd": "00-00-27", "sentence": "starting from the drones that can be used for inspections surveillance and delivery to household robots that can help us with our daily tasks"}, {"timeStart": "00-00-27", "timeEnd": "00-00-35", "sentence": "it can also be used for autonomous cars and technologies like they are and we are which requires spatial understanding and localization"}, {"timeStart": "00-00-37", "timeEnd": "00-00-40", "sentence": "combining camera images was imu measurements"}, {"timeStart": "00-00-40", "timeEnd": "00-00-46", "sentence": "is a popular approach to improve incremental motion tracking which is called visual in our short geometry"}, {"timeStart": "00-00-46", "timeEnd": "00-00-47", "sentence": "or short vdeo"}, {"timeStart": "00-00-48", "timeEnd": "00-00-51", "sentence": "I am you measurements compliment the image is in many ways"}, {"timeStart": "00-00-52", "timeEnd": "00-00-56", "sentence": "they may crawl and bitch and goes observable from the gravity directions"}, {"timeStart": "00-00-56", "timeEnd": "00-01-02", "sentence": "when they have few outliers and make optimization problem well defined"}, {"timeStart": "00-01-02", "timeEnd": "00-01-09", "sentence": "I am you predictions can also be used for better initialization and increased robustness and accuracy of the system"}, {"timeStart": "00-01-09", "timeEnd": "00-01-12", "sentence": "even when the visual data is not available"}, {"timeStart": "00-01-14", "timeEnd": "00-01-19", "sentence": "now approaches for visual inertial dama tree have been a research topic for many years"}, {"timeStart": "00-01-19", "timeEnd": "00-01-25", "sentence": "but getting globally consistent gravity align maps is still an open research challenge"}, {"timeStart": "00-01-25", "timeEnd": "00-01-31", "sentence": "the main issue is that for global mapping vision and I am you have contradicting requirements to achieve"}, {"timeStart": "00-01-38", "timeEnd": "00-01-47", "sentence": "for vision large baseline is required for better point translations so many approaches rely on a sparse set of key frames that have a large time intervals between each other"}, {"timeStart": "00-01-47", "timeEnd": "00-01-53", "sentence": "this allows to reduce the size of the optimization problem but increases time intervals between frames"}, {"timeStart": "00-01-53", "timeEnd": "00-02-00", "sentence": "for I am your small time intervals between frames are preferable as the position information degrades quickly was time"}, {"timeStart": "00-02-01", "timeEnd": "00-02-09", "sentence": "who proposed a new approach that combines imu and short-term visual information into non linear factors extracted from the v I O"}, {"timeStart": "00-02-09", "timeEnd": "00-02-11", "sentence": "which is the first layer of our system"}, {"timeStart": "00-02-11", "timeEnd": "00-02-17", "sentence": "this factors are then used to make a globally consistent gravity alignment in the v I p"}, {"timeStart": "00-02-17", "timeEnd": "00-02-19", "sentence": "which has a second layer of our system"}, {"timeStart": "00-02-19", "timeEnd": "00-02-29", "sentence": "compared to the Roper integrated imu factors that are used in many state of the art methods our approach results in smaller optimization problem and increased accuracy"}, {"timeStart": "00-02-29", "timeEnd": "00-02-34", "sentence": "on the top right you can see the trajectory produced by our Rio and the bottom"}, {"timeStart": "00-02-34", "timeEnd": "00-02-37", "sentence": "and the trajectory from our our way Mapper"}, {"timeStart": "00-02-38", "timeEnd": "00-02-42", "sentence": "let's get started with the first layer which is the visual inertial dama"}, {"timeStart": "00-02-43", "timeEnd": "00-02-48", "sentence": "the first uses sparse optical flow to track shirt on visual features"}, {"timeStart": "00-02-48", "timeEnd": "00-02-53", "sentence": "which rec watches from frame to frame as human attitude transformation between them"}, {"timeStart": "00-02-53", "timeEnd": "00-03-00", "sentence": "we used an inverse compositional approach for warping patches and locally skilled sum of squared differences to make"}, {"timeStart": "00-03-00", "timeEnd": "00-03-05", "sentence": "the first feature tracker that is invariant to changes in the exposure time"}, {"timeStart": "00-03-05", "timeEnd": "00-03-13", "sentence": "to remove outliers with Draco patches to the target frame and then backwards to see if they returned to the same location"}, {"timeStart": "00-03-13", "timeEnd": "00-03-17", "sentence": "additionally we used AP polar checks for stereo pairs"}, {"timeStart": "00-03-17", "timeEnd": "00-03-26", "sentence": "the tracks from optical flow and then used to formulate the rep rejection error where sweetie points are stored in the coordinate frame associated to one of the key frames"}, {"timeStart": "00-03-26", "timeEnd": "00-03-30", "sentence": "and Parliament raised was a unit direction vector and inverse distance"}, {"timeStart": "00-03-30", "timeEnd": "00-03-35", "sentence": "for the minimal permit is ation of the unit vector we used the stereographic projection"}, {"timeStart": "00-03-35", "timeEnd": "00-03-42", "sentence": "to combine short term vision information was the am you measurements be followed the approach that was proposed by looped him at all"}, {"timeStart": "00-03-42", "timeEnd": "00-03-47", "sentence": "we combine several consecutive measurements together into PR integrated pseudo measurement"}, {"timeStart": "00-03-47", "timeEnd": "00-03-50", "sentence": "and included into the optimization"}, {"timeStart": "00-03-50", "timeEnd": "00-03-54", "sentence": "this way we can reduce the size of the optimization problem"}, {"timeStart": "00-03-54", "timeEnd": "00-04-00", "sentence": "to keep the computation budget fixed we remove old frames and key frames from the optimization problem"}, {"timeStart": "00-04-00", "timeEnd": "00-04-03", "sentence": "by employing partial marginalization"}, {"timeStart": "00-04-03", "timeEnd": "00-04-06", "sentence": "this technique is also known as fixed like smoother"}, {"timeStart": "00-04-07", "timeEnd": "00-04-17", "sentence": "this all these techniques combined allow us to build the state of the art visual inertial odometer system that outperforms many open source competitors"}, {"timeStart": "00-04-17", "timeEnd": "00-04-19", "sentence": "as will be shown and they will Asian section"}, {"timeStart": "00-04-23", "timeEnd": "00-04-26", "sentence": "lets continue to the visual inertial mapping"}, {"timeStart": "00-04-27", "timeEnd": "00-04-33", "sentence": "it is possible to use the same approach was PR integrated imu measurements for the global mapping as well"}, {"timeStart": "00-04-33", "timeEnd": "00-04-35", "sentence": "but its results in some optimal solution"}, {"timeStart": "00-04-35", "timeEnd": "00-04-40", "sentence": "the measurement variants of them you factors grows was the integration time"}, {"timeStart": "00-04-40", "timeEnd": "00-04-46", "sentence": "which was in milliseconds for a dama tree but in the mapping setting it can reach several seconds between the key frames"}, {"timeStart": "00-04-46", "timeEnd": "00-04-54", "sentence": "another issue is that was all the knowledge about will also DJ and biases it is hard to predict the next post from the a new measurement"}, {"timeStart": "00-04-54", "timeEnd": "00-04-59", "sentence": "and since the map inset in ah in the map in setting this variable czar under constraint"}, {"timeStart": "00-04-59", "timeEnd": "00-05-03", "sentence": "they can simply be used by the optimization algorithm to accommodate errors"}, {"timeStart": "00-05-17", "timeEnd": "00-05-19", "sentence": "in this light in window"}, {"timeStart": "00-05-20", "timeEnd": "00-05-28", "sentence": "essentially this factor represents a probability distribution over the poses of the key frames that is stored in the dense linear raised form"}, {"timeStart": "00-05-28", "timeEnd": "00-05-33", "sentence": "in an information matrix h and information made to vector be"}, {"timeStart": "00-05-33", "timeEnd": "00-05-36", "sentence": "this probability distribution is relevant for the global mapping"}, {"timeStart": "00-05-36", "timeEnd": "00-05-43", "sentence": "but we cannot simply insert it into the global optimization because the puzzles are leaner ised around their absolute values"}, {"timeStart": "00-05-43", "timeEnd": "00-05-50", "sentence": "instead we can try to find a set of structured nonlinear factors that approximate this their sub you Chanel"}, {"timeStart": "00-05-50", "timeEnd": "00-05-52", "sentence": "around the curent estimates"}, {"timeStart": "00-05-54", "timeEnd": "00-05-58", "sentence": "we selected a set of factors that are present absolute position"}, {"timeStart": "00-05-58", "timeEnd": "00-06-02", "sentence": "your are all in beach angles and power wise relative poses"}, {"timeStart": "00-06-13", "timeEnd": "00-06-16", "sentence": "will be removed after marginalization"}, {"timeStart": "00-06-18", "timeEnd": "00-06-24", "sentence": "to quantify how well our approximate distribution represents the original one we use Kal do virgins"}, {"timeStart": "00-06-24", "timeEnd": "00-06-31", "sentence": "in general minimizing kale divergence between two distribution requires iterative methods and can be rather slow"}, {"timeStart": "00-06-31", "timeEnd": "00-06-34", "sentence": "but in our particular case the closed form solution exists"}, {"timeStart": "00-06-36", "timeEnd": "00-06-43", "sentence": "with although an approach called a non-linear factor recovery that guarantees that the reconstructed distribution has a minimum"}, {"timeStart": "00-06-43", "timeEnd": "00-06-46", "sentence": "possible kale divergence from the original one"}, {"timeStart": "00-06-46", "timeEnd": "00-06-49", "sentence": "this approach is presented in the paper by Amos around at all"}, {"timeStart": "00-06-49", "timeEnd": "00-06-56", "sentence": "which describes a closed form solution for the case when stature carbon matrix of the new measurements has a full rank"}, {"timeStart": "00-06-56", "timeEnd": "00-07-06", "sentence": "after recovering the distribution parameters with drop absolute position and your factors and reuse a relative poses and troll bch factors into global mapping"}, {"timeStart": "00-07-07", "timeEnd": "00-07-13", "sentence": "for computing the global map we initialize the key frames was the poses from the v a O"}, {"timeStart": "00-07-13", "timeEnd": "00-07-20", "sentence": "we detect and manage the key points to find loop closures and combine them with recovered factors in the joint optimization problem"}, {"timeStart": "00-07-37", "timeEnd": "00-07-46", "sentence": "quantity results show that our Rio system outperformance many state of the art approaches including direct video methods in terms of absolute trajectory error"}, {"timeStart": "00-07-46", "timeEnd": "00-07-54", "sentence": "yeah I remember the second layer of our system optimize is the map that consists of only key frames and further increases the accuracy"}, {"timeStart": "00-07-54", "timeEnd": "00-08-02", "sentence": "we also showed that the proposed approach outperforms the pure vision only bundle adjustment both graph optimization and"}, {"timeStart": "00-08-02", "timeEnd": "00-08-06", "sentence": "methods that rely on PR integrated imu measurements for the global mapping"}, {"timeStart": "00-08-08", "timeEnd": "00-08-15", "sentence": "our implementation is well paralyze ubl and runs faster than real time on the modern multi core CPU"}, {"timeStart": "00-08-15", "timeEnd": "00-08-24", "sentence": "in our experiments average processing speed was around eight milliseconds for the vdeo and fifty milliseconds for all stages of the mapping frame"}, {"timeStart": "00-08-27", "timeEnd": "00-08-30", "sentence": "to summarize we proposed a two layer mapping system"}, {"timeStart": "00-08-30", "timeEnd": "00-08-39", "sentence": "our approach subsumed high frame rate visual inertial information in non linear factors extracted from the marginalization prior"}, {"timeStart": "00-08-39", "timeEnd": "00-08-40", "sentence": "of the way all layer"}, {"timeStart": "00-08-40", "timeEnd": "00-08-45", "sentence": "and uses them in the mapping layer to compute globally consistent gravity of land maps"}, {"timeStart": "00-08-45", "timeEnd": "00-08-50", "sentence": "this results in smaller optimization problem and in better pose estimates"}, {"timeStart": "00-08-50", "timeEnd": "00-08-55", "sentence": "we released the visual inertial download tree coat mapping code and calibration"}, {"timeStart": "00-08-55", "timeEnd": "00-08-56", "sentence": "code of our system"}, {"timeStart": "00-08-56", "timeEnd": "00-09-01", "sentence": "open source at the link provided below thank you for your attention"}]}, {"title": "ICML 2020: Learning De-biased Representations with Biased Representations", "authors": "Hyojin Bahng", "abstract": "", "publicationOrg": "ICML", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/1910.02806.pdf", "pdfPath": "/data/cache/2/PDFs/ICML2020LearningDebiasedRepresentationswithBiasedRepresentations.pdf", "publicationUrl": "https://arxiv.org/pdf/1910.02806.pdf", "codeUrl": "https://github.com/clovaai/rebias.", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/lkjMxZDGubA", "videoPath": "/data/cache/2/videos/ICML 2020- Learning De-biased Representations with Biased Representations.mp4", "pdfText": "Most machine learning algorithms are trained and evaluated by randomly splitting a single source of data into training and test sets. Although this is a standard protocol, it is blind to a critical problem: the reliance on dataset bias . For instance, many frog images are taken in 1 Korea University 2 Clova AI Research, NAVER Corp.  Graduate School of AI, KAIST. Correspondence to: Seong Joon Oh <coallaoh@gmail.com>.Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). swamp scenes, but swamp itself is not a frog. Nonetheless, a model will exploit this bias (i.e., take \"shortcuts\") if it yields correct predictions for the majority of training examples. If the bias is sufficient to achieve high accuracy, there is little motivation for models to learn the complexity of the intended task, despite its full capacity to do so. Consequently, a model that relies on bias will achieve high in-distribution accuracy, yet fail to generalise when the bias shifts.We tackle this \"cross-bias generalisation\" problem where a model does not exploit its full capacity due to the \"sufficiency\" of bias cues for prediction of the target label in the training data. For example, language models make predictions based on the presence of certain words (e.g., \"not\" for \"contradiction\")  without much reasoning on the actual meaning of sentences, even if they are in principle capable of sophisticated reasoning. Similarly, convolutional neural networks (CNNs) achieve high accuracy on image classification by using local texture cues as shortcut, as opposed to more reliable global shape cues . 3D CNNs achieve high accuracy on video action recognition by relying on static cues as shortcut rather than capturing temporal actions .Existing methods attempt to remove a model's dependency on bias by de-biasing the training data through augmentation  or introducing a pre-defined bias that a model is trained to be independent of . Other approaches ) learn a biased model given source of bias as input, and de-bias through logit re-weighting or logit ensembling. These prior studies assume that biases can be easily defined or quantified (i.e., explicit bias label), but often real-world biases do not (e.g., texture or static bias above).To address this limitation, we propose a novel framework to train a de-biased representation by encouraging it to be statistically independent from representations that are biased by design. We use the Hilbert-Schmidt Independence Criterion  to formulate the independence. Our insight is that there are certain types of bias that can be easily captured by defining a bias-characterising model (e.g., CNNs of smaller receptive fields for texture bias; 2D CNNs for static bias in videos). Experiments show that our method arXiv:1910.02806v3  30 Jun 2020 effectively reduces a model's dependency on \"shortcuts\" in training data; accuracy is improved in test data where the bias is shifted or removed.We provide a rigorous definition of our over-arching goal: overcoming the bias in models trained on biased data. We systematically categorise the learning scenarios and crossbias generalisation strategies.We first define random variables, signal S and bias B as cues for the recognition of an input X as certain target variable Y . Signals S are the cues essential for the recognition of X as Y ; examples include the shape and skin patterns of frogs for frog image classification. Biases B's, on the other hand, are cues not essential for the recognition but correlated with the target Y ; many frog images are taken in swamp scenes, so swamp scenes can be considered as B. A key property of B is that intervening on B should not change Y ; moving a frog from swamp to a desert scene does not change the \"frogness\". We assume that the true predictive distribution p(Y |X) factorises as p(Y |S, B)p(S, B|X), signifying the sufficiency of p(S, B|X) for recognition.Under this framework, three learning scenarios are identified depending on the change of relationship p(S, B, Y ) across training and test distributions, p(S tr , B tr , Y tr ) and p(S te , B te , Y te ), respectively: in-distribution, crossdomain, and cross-bias generalisation. See .In-distribution. p(S tr , B tr , Y tr ) = p(S te , B te , Y te ). This is the standard learning setup utilised in many benchmarks by splitting data from a single source into training and test data at random.Cross-domain. p(S tr , B tr , Y tr ) = p(S te , B te , Y te ) and furthermore p(B tr ) = p(B te ). B in this case is often referred to as \"domain\". For example, training data consist of images with (Y tr =frog, B tr =wilderness) and (Y tr =bird, B tr =wilderness), while test data contain (Y te =frog, B te =indoors) and (Y te =bird, B te =indoors). This scenario is typically simulated by training and testing on different datasets .Cross-bias. p(B tr ) \u22a5 \u22a5 p(Y tr ) 1 and the dependency changes across training and test distributions: p(B tr , Y tr ) = p(B te , Y te ). We further assume that p(B tr ) = p(B te ), to clearly distinguish the scenario from the cross-domain generalisation. For example, training data only contain images of two types (Y tr =frog, B tr =swamp)1 \u22a5 \u22a5 and \u22a5 \u22a5 denote independence and dependence, respectively. and (Y tr =bird, B tr =sky), but test data contain unusual classbias combinations (Y te =frog, B te =sky) and (Y te =bird, B te =swamp). Our work addresses this scenario.Under cross-bias generalisation scenarios, the dependency p(B tr ) \u22a5 \u22a5 p(Y tr ) makes bias B a viable cue for recognition. The model trained on such data becomes susceptible to interventions on B, limiting its generalisabililty when the bias is changed or removed in the test data. There exist prior approaches to this problem, but with different types and amounts of assumptions on B. We briefly recap the approaches based on the assumptions they require. In the next part \u00a72.3, we will define our problem setting that requires an assumption distinct from the ones in prior approaches.When an algorithm to disentangle bias B and signal S exists. Being able to disentangle B and S lets one collapse the feature space corresponding to B in both training and test data. A model trained on such normalised data then becomes free of biases. As ideal as it is, building a model to disentangle B and S is often unrealistic (e.g., texture bias ). Thus, researchers have proposed other approaches to tackle cross-bias generalisation.When a data collection procedure or generative algorithm for p(X|B) exists. When additional examples can be supplied through p(X|B), the training dataset itself can be de-biased, i.e., B \u22a5 \u22a5 Y . Such a data augmentation strategy is indeed a valid solution adopted by many prior studies. Some approach has proposed to collect additional data to balance out the bias . Other approaches have proposed to synthesise data with a generative algorithm through image stylisation , object removal , or generation of diverse, semantically similar linguistic variations . However, collecting unusual inputs can be expensive , and building a generative model with pre-defined bias types  may suffer from bias misspecification or the lack of realism.When a ground truth or predictive algorithm for p(B|X) exists. Conversely, when one can tell the bias B for every input X, we can remove the dependency between the model predictions f (X) and the bias B. The knowledge on p(B|X) is provided in many realistic scenarios. For example, when the aim is to remove gender biases B in a job application process p(Y |X), applicants' genders p(B|X) are supplied as ground truths. Many existing approaches for fairness in machine learning have proposed independence-based regularisers to encourage training sample training sample training sample test sample f (X) \u22a5 \u22a5 B  or the conditional independence f (X) \u22a5 \u22a5 B | Y . Other approaches have proposed to remove predictability of p(B|X) based on f (X) through domain adversarial losses  or mutual information minimisation . When the ground truth of p(B|X) is not provided, another approach has proposed to quantify texture bias by utilising the neural gray-level co-occurrence matrix and encouraging independence through projection . Unfortunately, for certain bias types (e.g., texture bias), it is difficult to enumerate the possible bias classes and put labels on samples.Under the cross-bias generalisation scenario, some biases are not easily addressed by the above methods. Take texture bias as an example ( \u00a71, Geirhos et al. (2019)): (1) texture B and shape S cannot easily be disentangled, (2) collecting unusual images or building a generative model p(X|B) is expensive, (3) building the predictive model p(B|X) for texture requires enumeration (classifier) or embedding (regression) of all possible textures, which is not feasible.However, slightly modifying the third assumption results in a problem setting that allows interesting application scenarios. Instead of assuming explicit knowledge on p(B|X), we can approximate B by defining a set of models G that are biased towards B by design. For texture biases, for example, we define G to be the set of CNN architectures with small receptive fields. Then, any learned model g \u2208 G can by design make predictions g(x) based on the patterns that can only be captured with small receptive fields (i.e., textures), becoming more liable to overfit to texture.More precisely, we define G to be a bias-characterising model class for the bias-signal pair (B, S) if for every possible joint distribution p(B, X) there exists a g \u2208 G such that p(B|X) \u2248 g(X) (recall condition) and every g \u2208 G satisfies g(X) \u22a5 \u22a5 S | B (precision condition). Consider these conditions as conceptual tools to break down what is a good G?\" In practice, G may not necessarily include all biases and may also capture important signals (i.e., imperfect recall and precision). With this in mind, we formulate our framework as a regulariser to the original task so that f (X) does not ignore every signal captured by G. We do not require G to be perfect.There exist many scenarios when such G can be characterised, based on several empirical evidence for the type of bias. For instance, action recognition models rely heavily on static cues without learning temporal cues ; we can regularise the 3D CNNs towards better generalisation across static biases by defining G to be the set of 2D CNNs. VQA models rely overly on language biases rather than visual cues . G can be defined as the set of models that only look at the language modality . Entailment models are biased towards word overlap rather than understanding the underlying meaning of sentences . We can design G to be the set of bag-of-words classifiers . These scenarios exemplify situations when the added architectural capacity is not fully utilised because there exist simpler cues for solving the task.There are recent approaches that attempt to capture bias with bias-characterising models G and remove dependency on B via logit ensembling  or logit reweighting . In \u00a74, we empirically measure their performance on synthetic and realistic biases.We present a solution for the cross-bias generalisation when the bias-characterising model class G is known (see \u00a72.3); the method is referred to as ReBias. The solution consists of training a model f for the task p(Y |X) with a regularisation term encouraging the independence between the prediction f (X) and the set of all possible biased predictions {g(X) | g \u2208 G}. We will introduce the precise definition of the regularisation term and discuss why and how it leads to the unbiased model.If p(B|X) is fully known, we can directly encourage f (X) \u22a5 \u22a5 B. Since we only have access to the set of biased models G ( \u00a72.3), we seek to promote f (X) \u22a5 \u22a5 g(X) for every g \u2208 G. Simply put, we de-bias a representation f \u2208 F by designing a set of biased models G and letting f run away from G. This leads to the independence from bias cues B while leaving signal cues S as valid recognition cues; see \u00a72.3. We will specify ReBias learning objective after introducing our independence criterion, HSIC.Since we need to measure the degree of independence between continuous random variables f (X) and g(X) in high-dimensional spaces, it is infeasible to resort to histogram-based measures; we use HSIC . For two random variables U and V and kernels k and l, HSIC is defined as HSIC k,l (U,HS where C k,l is the cross-covariance operator in the Reproducing Kernel Hilbert Spaces (RKHS) of k and l , an RKHS analogue of covariance matrices. || \u2022 || HS is the Hilbert-Schmidt norm, a Hilbert-space analogue of the Frobenius norm. It is known that for two random variables U and V and radial basis function (RBF) kernels k and l, HSIC k,l (U, V ) = 0 if and only if U \u22a5 \u22a5 V . A finite-sample estimate of HSIC k,l (U, V ) has been used in practice for statistical testing , feature similarity measurement , and model regularisation . We employ an unbiased estimator HSIC k,lwheree., the diagonal entries of U are set to zero. V is defined similarly.Minimax optimisation for bias removal. We definewith an RBF kernel k for the degree of independence between representation f \u2208 F and the biased representations G. We write HSIC 1 (f, G) and HSIC 1 (f, g) as shorthands.The learning objective for f is then defined aswhere L(f, X, Y ) is the loss for the main task p(Y |X) and \u03bb > 0. We write L(f ) as shorthands. Having specified G to represent the bias B, we need to train g \u2208 G for the original task to intentionally overfit G to B. Thus, the inner optimisation involves both the independence criterion and the original task loss L(g). The final learning objective for ReBias is then\u03bb, \u03bb g > 0. We solve equation 3 by alternative updates. Our intention is that f is trained to be different from multiple possible biased predictions {g(X) | g \u2208 G}, thereby improving its de-biased performance.Independence describes relationships between random variables, but we use it for function pairs. Which functional relationship does statistical independence translate to? In this part, we argue with proofs and observations that the answer to the above question is the dissimilarity of invariance types learned by a pair of models.Linear case: Equivalence between independence and orthogonality. We study the set of function pairs (f, g) satisfying f (X) \u22a5 \u22a5 g(X) for suitable random variable X \u223c p(X). Assuming linearity of involved functions and the normality of X, we obtain the equivalence between statistical independence and functional orthogonality.Lemma 1. Assume that f and g are affine mappings f (x) = Ax + a and g(x) = Bx + b where A \u2208 R m\u00d7n and B \u2208 R l\u00d7n . Assume further that X is a normal distribution with mean \u00b5 and covariance matrix \u03a3. Then, f (X) \u22a5 \u22a5 g(X) if and only if ker(A) \u22a5 \u22a5 \u03a3 ker(B) \u22a5 . For a positive semidefinite matrix \u03a3, we define r, s \u03a3 = r, \u03a3s , and the set orthogonality \u22a5 \u03a3 likewise. The proof is in Appendix.In particular, when f and g have 1-dimensional outputs, the independence condition is translated to the orthogonality of their weight vectors and decision boundaries; f and g are models with orthogonal invariance types.Non-linear case: HSIC as a metric learning objective.We lack theories to fully characterise general, possibly nonlinear, function pairs (f, g) achieving f (X) \u22a5 \u22a5 g(X); it is an interesting open question. For now, we make a set of observations in this general case, using the finite-sample independence criterion HSIC 0 (f, g) := (m\u22121) \u22122 tr( f g T ) = 0, where f is the mean-subtracted kernel matrixand likewise for g. Unlike in the loss formulation ( \u00a73.1), we use the biased HSIC statistic for simplicity.Note that tr( f g T ) is an inner product between flattened matrices f and g. We consider the inner-product-minimising solution for f on an input pair x 0 = x 1 given a fixed g. The problem can be written as. Then, the above problem boils down to min f (x0),f (x1) f 01 , signifying the relative variance of f on (x 0 , x 1 ). Following a similar argument, we obtain the converse statement: if g is relatively variant on a pair of inputs, invariance of f on the pair minimises the objective.We conclude that min f HSIC 0 (f, g) against a fixed g is a metric-learning objective for the embedding f , where ground truth pairwise matches and mismatches are relative mismatches and matches for g, respectively. As a result, f and g learn different sorts of invariances.Effect of HSIC regularisation on toy data. We have established that HSIC regularisation encourages the difference in model invariances. To see how it helps to de-bias a model, we have prepared synthetic two-dimensional training data following the cross-domain generalisation case in : X = (B, S) \u2208 R 2 and Y \u2208 {red, yellow, green}. Since the training data is perfectly biased, a multi-layer perceptron (MLP) trained on the data only shows 55% accuracy on de-biased test data (see decision boundary figure in Appendix). To overcome the bias, we have trained another MLP with equation 3 where the bias-characterising class G is defined as the set of MLPs that take only the bias dimension as input. This model exhibits de-biased decision boundaries (Appendix) with improved accuracy of 89% on the de-biased test data.In the previous section, ReBias has been introduced and theoretically justified. In this section, we present experimental results of ReBias. We first introduce the setup, including the biases tackled in the experiments, difficulties inherent to the cross-bias evaluation, and the implementation details ( \u00a74.1). Results on Biased MNIST ( \u00a74.2), ImageNet ( \u00a74.3) and action recognition ( \u00a74.4) are shown afterwards. While our experiments are focused on vision tasks, we stress that the underlying concept and methodology are not exclusive to them. For example, it will be an interesting future research direction to apply ReBias to visual question answering and natural language understanding problems ( \u00a72.3), tasks that also suffer from dataset biases.Which biases do we tackle? Our work tackles the types of biases that are used as shortcut cues for recognition in the training data. In the experiments, we tackle the \"texture\" bias in image classification and the \"static\" bias in video action recognition. Even if a CNN image classifier has wide receptive fields, empirical evidence indicates that they heavily rely on local texture cues for recognition, instead of the global shape cues . Similarly, a 3D CNN action recognition model possesses the capacity to model temporal cues, yet it heavily relies on static cues like scenes or objects rather than the temporal motion for recognition . While it is difficult to precisely define and quantify all texture or scene types, it is easy to intentionally design a model G biased towards such cues. In other words, we model the entire bias domain (e.g., texture) through a chosen inductive bias of the G network architecture. For texture bias in image recognition, we design G as a CNN with smaller receptive fields; for static bias in action recognition, we design G as a 2D CNN.Evaluating cross-bias generalisation is difficult. To measure the performance of a model across real-world biases, one requires an unbiased dataset or one where the types and degrees of biases can be controlled. Unfortunately, data in real world arise with biases. To de-bias a frog and bird image dataset with swamp and sky (see \u00a72.1), either rare data samples must be collected or one must generate such data; they are expensive procedures .We thus evaluate our method along two axes: (1) synthetic biases (Biased MNIST) and (2) realistic biases (ImageNet classification and action recognition task). Biased MNIST contains colour biases which we control in training and test data for an in-depth analysis of ReBias. For ImageNet classification, on the other hand, we use clustering-based proxy ground truths for texture bias to measure the crossbias generalisability. For action recognition, we utilize the unbiased data that are publicly available (Mimetics), albeit in small quantity. We use the Mimetics dataset  for the unbiased test set accuracies, while using the biased Kinetics  dataset for training. The set of experiments complement each other in terms of experimental control and realism.Implementation of ReBias. We describe the specific design choices in ReBias implementation (equation 3). The source code is in the supplementary materials.For texture biases, we define the biased model architecture families G as CNNs with small receptive fields (RFs). The biased models in G will by design learn to predict the target class of an image only through the local texture cues. On the other hand, we define a larger search space F with larger RFs for our unbiased representations.In our work, all networks f and g are fully convolutional networks followed by a global average pooling (GAP) layer and a linear classifier. f (x) and g(x) denote the outputs of GAP layer (feature maps), on which we compute the independence measures using HSIC ( \u00a73.1).For Biased MNIST, F is a fully convolutional network with four convolutional layers with 7 \u00d7 7 kernels. Each convolutional layer uses batch normalisation  and ReLU. G has the same architecture as F , except that the kernel sizes are 1 \u00d7 1. On ImageNet, we use the ResNet18  architecture for F with RF=435. G is defined as BagNet18 , which replaces many 3\u00d73 kernels with 1\u00d71, thereby being limited to RF=43. For action recognition, we use 3D-ResNet18 and 2D-ResNet18 for F and G whose RF along temporal dimension are 19 and 1, respectively.We conduct experiments using the same batch size, learning rate, and epochs for fair comparison. We choose \u03bb = \u03bb g = 1. For the Biased MNIST experiments, we set the kernel radius to one, while the median of distances is chosen for ImageNet and action recognition experiments. More implementation details are provided in Appendix.Comparison methods. There are prior methodologies that can be applied in our cross-bias generalisation task ( \u00a72.2). We empirically compare ReBias against them. The prior methods include RUBi  and LearnedMixin+H ) that reduce the dependency of the model F on biases captured by G via logit re-weighting and logit ensembling, respectively. While the prior works additionally alter the training data for G, we only compare the objective functions themselves in our experiment. We additionally compare two methods that tackle texture bias: HEX  and StylisedImageNet . HEX attempts to reduce the dependency of a model on \"superficial statistics\". It measures texture via neural grey-level co-occurrence matrices (NGLCM) and projects out the NGLCM feature from the model. StylisedImageNet reduces the models reliance on texture by augmenting the training data with texturised images.We first verify our model on a dataset where we have full control over the type and amount of bias during training and evaluation. We describe the dataset and present the experimental results.We construct a new dataset called Biased MNIST designed to measure the extent to which models generalise to bias shift. We modify  by introducing the colour bias that highly correlate with the label Y during training. With B alone, a CNN can achieve high accuracy without having to learn inherent signals for digit recognition S, such as shape, providing little motivation for the model to learn beyond these superficial cues. We inject the colour bias by adding a colour on training image backgrounds ( ). We pre-select 10 distinct colours for each digit y \u2208 {0, \u2022 \u2022 \u2022 , 9}. Then, for each image of digit y, we assign the pre-defined colour b(y) with probability \u03c1 \u2208 [0, 1] and any other colour with probability (1\u2212\u03c1). \u03c1 then controls the bias-target correlation in the training data: \u03c1 = 1.0 leads to complete bias and \u03c1 = 0.1 leads to an unbiased dataset. We consider \u03c1 \u2208 {0.99, 0.995, 0.997, 0.999} to simulate significant amounts of bias during training. We evaluate the models generalisability to bias shift by evaluating under the following criterion:Biased. p(S te , B te , Y te ) = p(S tr , B tr , Y tr ), the indistribution case in \u00a72.1. Whatever bias the training set contains, it is replicated in the test set (same \u03c1). This measures the ability of de-biased models to maintain high indistribution performances while generalising to the crossbias test set.Unbiased. B te \u22a5 \u22a5 Y te , the cross-bias generalization in \u00a72.1. We assign biases on test images independently of the labels. Bias is no longer predictive of Y and a model needs to utilise actual signals S to yield correct predictions.  Results on the Biased MNIST are shown in .ReBias lets a model overcome bias. We observe that vanilla F achieves 100% accuracy under the \"biased\" metric (the same bias between training and test data) in the Biased MNIST for all \u03c1. This is how most machine learning tasks are evaluated, yet this does not show the extent to which the model depends on bias for prediction. When the bias cues are randomly assigned to the label at evaluation, vanilla F accuracy collapses to 10.4% under the \"unbiased\" metric on the Biased MNIST when the train correlation is large, i.e., \u03c1 = 0.999. The intentionally biased models G result in 10.0% on the Biased MNIST, the random chance performance, for all \u03c1. This exemplifies the case where a seemingly high-performing model has in fact overfitted to bias and does not generalise to new situations.On the other hand, ReBias achieves robust generalisation across all settings by learning to be different from the representations G. Especially, ReBias unbiased accuracies than the vanilla model under the highly correlated settings: 10.4 \u2192 22.7% and 33.4 \u2192 64.2% boosts for \u03c1 =0.999 and 0.997, respectively.Comparison against other methods. As HEX predefines bias as patterns captured by NGLCM, we observe that it does not improve generalisability to colour bias (18.0%) while also hurting the in-distribution accuracy (74.1%) compared to vanilla F . LearnedMixin achieves performance gain in unbiased accuracies (57.2%) yet suffers a severe performance drop for unbiased accuracies (15.2%).RUBi achieves robust generalisation across biased and unbiased accuracies (99.7% and 60.2% respectively). We show in the following experiments that LearnedMixin and RUBi achieve sub-optimal performances in realistic texture and static biases.Analysis of per-bias performances. In , we provide more fine-grained results by visualising the accuracies per bias-class pair (B, Y ) = (b, y). The diagonal average corresponds to the biased accuracy and the overall average corresponds to the unbiased accuracy. We observe that the vanilla model has higher accuracies on diagonals and lower on off-diagonals, showing the heavy reliance on colour (bias) cues. HEX and RUBi demonstrate sporadic improvements in certain off-diagonals, but the overall improvements are limited. LearnedMixin shows further enhancements, yet with near-zero accuracies on diagonal entries (also seen in ). ReBias uniformly improves the off-diagonals, while not sacrificing the diagonals.Learning Curves. In , we plot the evolution of unbiased accuracy and HSIC values as ReBias is trained.ReBias is trained with \u03c1 = 0.997 and tested with \u03c1 = 0.1 (unbiased). While the classification loss alone, i.e., vanilla F , leads to an unbiased accuracy of 33.4%, the unbiased accuracy increases dramatically (> 60%) as the HSIC between F and G is minimised during training. We observe that there exists a strong correlation between the HSIC values and unbiased accuracies. In ImageNet experiments, we further validate the applicability of ReBias on the texture bias in realistic images (i.e., objects in natural scenes). The texture bias often lets a model achieve good in-distribution performances by exploiting the local texture shortcuts (e.g., determining a swan class by not seeing its shape but the background water texture).We construct 9-Class ImageNet, a subset of ImageNet (Russakovsky et al., 2015) containing 9 super-classes , since using the original ImageNet is not scalable. We additionally balance the ratios of sub-class images for each super-class to focus on the effect of texture bias.Since it is difficult to evaluate the cross-bias generalisability on realistic data ( \u00a74.1), we settle for surrogate measures:Biased. p(S te , B te , Y te ) = p(S tr , B tr , Y tr ). Accuracy is measured on the in-distribution validation set. Though widely-used, this metric is blind to a model's generalisability to unseen bias-target combinations.Unbiased. B te \u22a5 \u22a5 Y te . As a proxy to the perfectly debiased test data, which is difficult to collect ( \u00a74.1), we use texture clusters IDs c \u2208 {1, \u2022 \u2022 \u2022 , K} as the ground truth labels for texture bias obtained by the k-means clustering. For full details of texture clustering algorithm, see Appendix. For an unbiased accuracy measurement, we compute the accuracies for every set of images corresponding to a texture-class combination (c, y). The combination-wise accuracy A c,y is computed by Corr(c, y)/Pop(c, y), where Corr(c, y) is the number of correctly predicted samples in (c, y) and Pop(c, y) is the total number of samples in (c, y), called the population at (c, y). The unbiased accuracy is then the mean accuracy over all A c,y where the population Pop(c, y) > 10. This measure gives more weights on samples of unusual texture-class combinations (smaller Pop(c, y)) that are less represented in the usual biased accuracies. Under this unbiased metric, a biased model basing its recognition on textures is likely to show sub-optimal results on unusual combinations, leading to a drop in the unbiased accuracy. Since the k-means clustering is non-convex, we report the average unbiased accuracy of three clustering results with different initial points.ImageNet-A. ImageNet-A (Hendrycks et al., 2019) contains the failure cases of ImageNet-trained ResNet50 among web images. The images consist of many failure modes of networks when \"frequently appearing background elements\"  become erroneous cues for recognition (e.g. a bee image feeding on hummingbird feeder is recognised as a hummingbird). An improved performance on ImageNet-A is an indirect signal that the model learns beyond the bias shortcuts.ImageNet-C. ImageNet-C ) is proposed to evaluate robustness to 15 corruption types including \"noise\", \"blur\", \"weather\", and \"digital\" with five severities. Improved performances on ImageNet-C indicate that the model robustly generalises to a wide range of distortions, despite their absence during training. 90.8 88.8 24.9 54.2 Biased 67.7 65.9 18.8 31.7StylisedIN  88.4 86.6 24.6 61.1LearnedMixin  64.1 62.7 15.0 27.5RUBi  90.5 88.6 27.7 53.7 ReBias (ours) 91.9 90.5 29.6 57.5 . ImageNet results. We show results corresponding to F =ResNet18 and G =BagNet18. IN-A and IN-C indicates ImageNet-A and ImageNet-C, respectively. We repeat each experiment three times.We measure performances of ResNet18 trained under ReBias to be different from BagNet18. We use the metrics in the previous part. Results are shown in .Vanilla models are biased. ResNet18 shows good performances on the biased accuracy (90.8%) but dropped performances on the texture-unbiased accuracy (88.8%).BagNet18 performs worse than the vanilla ResNet as they are heavily biased towards texture by design (i.e., small receptive field sizes). The drop signifies the biases of vanilla models towards texture cues; by basing their predictions on texture cues they obtain generally better accuracies on texture-class pairs (c, y) that are more represented. The drop also shows the limitation of current evaluation schemes where the cross-bias generalisation is not measured.ReBias leads to less biased models. When ReBias is applied on ResNet18 to make it learn cues beyond those captured by BagNet18, we observe a general boost in the biased, unbiased, ImageNet-A, and ImageNet-C accuracies ( ). The unbiased accuracy of ResNet18 improves from 88.8% to 90.5%, thus robustly generalising to less represented texture-class combinations at test time. Our method also shows improvements on the challenging ImageNet-A subset (e.g. from 24.9% to 29.6%), which further shows an improved generalisation. While StylisedImageNet attempts to mitigate texture bias by stylisation, it does not increase the generalisability for both the unbiased and ImageNet-A accuracy (86.6% and 24.6% respectively). Similar to the Biased MNIST results, Learned-Mixin suffers a collapse in the in-distribution accuracy (from 90.8% to 67.9%) and does not improve generalisability to less represented texture-class combinations or the challenging ImageNet-A. RUBi only shows improvement on ImageNet-A (from 24.9% to 27.7%).In ImageNet-C experiments, StylisedImageNet shows the best ImageNet-C performance (61.1%) and ReBias achieves the second best accuracy (57.5%).Despite ReBias does not use any data augmentation, it improves the generalisability to ImageNet-C while Learned-Mixin and RUBi fail to generalise (from 54.2% to 27.5% and 53.7%, respectively).To see further effectiveness of ReBias on reducing static biases in a video understanding task, we conduct the action recognition experiments with 3D CNNs. 3D CNNs have proven their state-of-the-art performances on action recognition benchmarks such as Kinetics , but recent studies  have shown that such action datasets have strong static biases towards the scene or objects in videos. As a result, 3D CNNs make predictions dominantly based on static cues, despite their ability to capture temporal signals, and they achieve high accuracies even with temporal cues removed (e.g., shuffling frames or masking-out human actor in videos) . This bias problem occasionally leads to performance drop when static cues shift across training and test settings (e.g., predicting \"swimming\" class when a person plays football near a swimming pool).We use the Kinetics dataset  for training, which is known to have bias towards static cues. To evaluate the cross-bias generalisability, we use the Mimetics dataset ) that consists of videos of a mime artist performing actions without any context. The classes of Mimetics are fully covered by the Kinetics classes and we use it as the unbiased validation set. Since the training and testing of the full action datasets are not scalable, we sub-sample 10-classes from both datasets. Detailed dataset descriptions are in Appendix.We evaluate the performances of 3D-ResNet18 trained to be different from the biased model 2D-ResNet18. Main results are shown in .Vanilla model is biased. The vanilla 3D-ResNet18 model F shows a reasonable performance on the biased Kinetics with 54.5% accuracy, but significantly loses the accuracy on the unbiased Mimetics with 18.9% accuracy. While 3D-ResNet18 is originally designed for capturing temporal signals within videos, it relies a lot on static cues, resulting in a similar performance with the 18.4% accuracy by 2D-ResNet18 on Mimetics.ReBias reduces the static bias. Applying ReBias on 3D-ResNet18 encourages it to utilise the temporal modelling capacity by forcing it to reason differently from 2D-ResNet18. ReBias improves the accuracies on Biased Unbiased Model description (Kinetics) (Mimetics) Vanilla  54.5 18.9 Biased  50.7 18.4LearnedMixin  12.3 11.4RUBi  22.4 13.4 ReBias (ours) 55.8 22.4 . Action recognition results. We show results corresponding to F =3D-ResNet18 and G =2D-ResNet18 with baseline comparisons. Top-1 accuracies are reported. Each result is the average of three runs.both Kinetics and Mimetics datasets beyond the vanilla model F : 54.5 \u2192 55.8% and 18.9 \u2192 22.4%, respectively. We also compare against the two baseline methods, LearnedMixin  and RUBi , as in the previous sections. ReBias shows better performances than the two baseline methods for reducing the static bias for action recognition. We believe that the difficulty of the action recognition task on Kinetics hampers the normal operation of the logit-modification step in the baseline methods, severely hindering the convergence with respect to the cross-entropy loss. The training of ReBias, on the other hand, remains stable as the independence loss acts only as a regularisation term.We have identified a practical problem faced by many machine learning algorithms that the learned models exploit bias shortcuts to recognise the target: the cross-bias generalisation problem ( \u00a72). Models tend to under-utilise its capacity to extract non-bias signals (e.g., global shapes for object recognition, or temporal actions for action recognition) when bias shortcuts provide sufficient cues for recognition in the training data (e.g., texture for object recognition, or static contexts for action recognition) . We have addressed this problem with the ReBias method. Given an identified set of models G that encodes the bias to be removed, ReBias encourages a model f to be statistically independent of G ( \u00a73). We have provided theoretical justifications ( \u00a73.2) and have validated the superiority of ReBias in removing biases from models through experiments on Biased MNIST, ImageNet classification, and the Mimetics action recognition benchmark ( \u00a74). the batch sizes  for (Biased MNIST, ImageNet, action recognition) experiments. For Biased MNIST, the learning rate is initially set to 0.001 and is decayed by factor 0.1 every 20 epochs. For ImageNet and action recognition, learning rates are initially set to 0.001 and 0.1, respectively, and are decayed by cosine annealing. For action recognition, we use f (x) and g(x) as the output logits for the sake of stable training. For each dataset, we train every method  for the same number of epochs. We train the models with (80, 120, 120) epochs for (Biased MNIST, ImageNet, action recognition) experiments. All experiments are implemented using PyTorch .In addition, we observe that a better optimiser than ADAM, e.g. AdamP , helps performance improvements in Biased MNIST and ImageNet experiments.  shows the results from the reference paper . For future researches, we recommend using AdamP for better optimisation stability. . AdamP experiments. Biased MNIST and 9-Class ImageNet benchmarks with AdamP    30.5 (+7.5) 70.9 (+7.9) 80.9 (+6.0) 89.6 (+2.6) 68.0 (+6.0) 95.2 (+1.4) 94.5 (+1.8) 32.9 (+1.7)Architecture details for action recognition. The 3D-ResNet  architecture is widely used in various video understanding tasks. We choose 3D-ResNet18 and 2D-ResNet18 as F and G of our method, respectively. The architectural details are in .Training details for comparison methods. For training LearnedMixin, we pre-train and fix G before training F , as done in the original paper. We pre-train G for 5 epochs for Biased MNIST, and 30 epochs for ImageNet and action recognition. For training RUBi, we update F and G simultaneously without pre-training G, as done in the original paper. For training HEX, we substitute G with the neural grey-level co-occurrence matrix (NGLCM) to represent the \"superficial statistics\". For StylisedImageNet, we augment 9-class ImageNet with its stylised version (i.e., twice the original dataset size), while maintaining the training setup as identical.", "videoStruct": [{"timeStart": "00-00-05", "timeEnd": "00-00-07", "sentence": "hi I'm hurting bang"}, {"timeStart": "00-00-07", "timeEnd": "00-00-11", "sentence": "and I'm gonna talk about learning to biased representations with biased representation"}, {"timeStart": "00-00-11", "timeEnd": "00-00-17", "sentence": "and this is joint work on her tongue out Chung sang Julian and sang Gino at coal via"}, {"timeStart": "00-00-17", "timeEnd": "00-00-19", "sentence": "and jay gold shoe at Christ"}, {"timeStart": "00-00-20", "timeEnd": "00-00-23", "sentence": "so I'm gonna talk about dataset bias in machine learning"}, {"timeStart": "00-00-24", "timeEnd": "00-00-27", "sentence": "let's say I'll training data has most car images on the road"}, {"timeStart": "00-00-27", "timeEnd": "00-00-29", "sentence": "and most frog images in the swamp"}, {"timeStart": "00-00-30", "timeEnd": "00-00-35", "sentence": "on your network will see these patterns and use them to maximize accuracy"}, {"timeStart": "00-00-37", "timeEnd": "00-00-39", "sentence": "however at deployment"}, {"timeStart": "00-00-39", "timeEnd": "00-00-41", "sentence": "when it sees a car in swamp or frog on road"}, {"timeStart": "00-00-42", "timeEnd": "00-00-46", "sentence": "relying on these patterns or biases in the data set"}, {"timeStart": "00-00-46", "timeEnd": "00-00-48", "sentence": "is not helpful for generalization"}, {"timeStart": "00-00-50", "timeEnd": "00-00-53", "sentence": "biases such as road and swamp"}, {"timeStart": "00-00-53", "timeEnd": "00-00-56", "sentence": "easy shortcuts to recognise car and frog"}, {"timeStart": "00-00-57", "timeEnd": "00-00-59", "sentence": "in a standard scenario"}, {"timeStart": "00-00-59", "timeEnd": "00-01-03", "sentence": "most approaches assume that training and test data have identical distributions"}, {"timeStart": "00-01-03", "timeEnd": "00-01-07", "sentence": "therefore relying on biased haircuts are sufficient to achieve high accuracy"}, {"timeStart": "00-01-10", "timeEnd": "00-01-13", "sentence": "this assumption does not hold in the real world"}, {"timeStart": "00-01-13", "timeEnd": "00-01-17", "sentence": "we target a scenario where the training and test data have different biases"}, {"timeStart": "00-01-17", "timeEnd": "00-01-22", "sentence": "therefore overcoming bias is critical for good generalization"}, {"timeStart": "00-01-23", "timeEnd": "00-01-28", "sentence": "the problem is we often do not know every single bias that exist in a dataset"}, {"timeStart": "00-01-29", "timeEnd": "00-01-32", "sentence": "that is impossible to build a perfectly on biased data"}, {"timeStart": "00-01-34", "timeEnd": "00-01-38", "sentence": "our approach that across bias generalization problem is called for by us"}, {"timeStart": "00-01-41", "timeEnd": "00-01-44", "sentence": "when we only give our model the original task Claus"}, {"timeStart": "00-01-44", "timeEnd": "00-01-46", "sentence": "we cannot prevent it from being a biased model"}, {"timeStart": "00-01-47", "timeEnd": "00-01-49", "sentence": "to medicate this"}, {"timeStart": "00-01-49", "timeEnd": "00-01-53", "sentence": "we characterized by us by a set of intentionally biased models"}, {"timeStart": "00-01-54", "timeEnd": "00-01-58", "sentence": "and then four star model to learn a different representation"}, {"timeStart": "00-01-58", "timeEnd": "00-02-02", "sentence": "as a result our model becomes less biased"}, {"timeStart": "00-02-03", "timeEnd": "00-02-05", "sentence": "this raises two questions"}, {"timeStart": "00-02-05", "timeEnd": "00-02-08", "sentence": "first how do we characterize bias models"}, {"timeStart": "00-02-09", "timeEnd": "00-02-12", "sentence": "second how do we cold be the friend"}, {"timeStart": "00-02-13", "timeEnd": "00-02-17", "sentence": "for the first question we will take texture by us as an example"}, {"timeStart": "00-02-17", "timeEnd": "00-02-21", "sentence": "image that train to cnn's are biased towards texture"}, {"timeStart": "00-02-21", "timeEnd": "00-02-25", "sentence": "doesn't maintains a very high accuracy when given texture cues"}, {"timeStart": "00-02-26", "timeEnd": "00-02-29", "sentence": "but collapses when texture cues are absent"}, {"timeStart": "00-02-30", "timeEnd": "00-02-36", "sentence": "in this case we can build a model intentionally biased tours texture by reducing the receptive fields"}, {"timeStart": "00-02-37", "timeEnd": "00-02-42", "sentence": "and ordinary CNN has a largest of the field so that it can capture both local and global cues"}, {"timeStart": "00-02-43", "timeEnd": "00-02-46", "sentence": "by deliberately reducing the receptive fields"}, {"timeStart": "00-02-46", "timeEnd": "00-02-49", "sentence": "a model can only capture local cues like texture"}, {"timeStart": "00-02-51", "timeEnd": "00-02-57", "sentence": "therefore we can build a model intentionally biased towards texture by reducing the receptive fields"}, {"timeStart": "00-02-58", "timeEnd": "00-03-01", "sentence": "the second question is how can we in cold be different"}, {"timeStart": "00-03-02", "timeEnd": "00-03-05", "sentence": "we do this through statistical independence"}, {"timeStart": "00-03-06", "timeEnd": "00-03-09", "sentence": "measured with hill brush mitt independence criteria"}, {"timeStart": "00-03-11", "timeEnd": "00-03-17", "sentence": "it is the hill brush Mich normal though Krause co variance operator and reproducing kernel Hilbert spaces"}, {"timeStart": "00-03-18", "timeEnd": "00-03-22", "sentence": "and it is zero if and only if two variables are independent"}, {"timeStart": "00-03-23", "timeEnd": "00-03-29", "sentence": "therefore we minimize h s I see between the two models to supervise be different"}, {"timeStart": "00-03-30", "timeEnd": "00-03-33", "sentence": "now we explained our objective function in detail"}, {"timeStart": "00-03-33", "timeEnd": "00-03-37", "sentence": "given our model and a bias model g"}, {"timeStart": "00-03-37", "timeEnd": "00-03-41", "sentence": "we do not their function spaces as capital f and g respectively"}, {"timeStart": "00-03-42", "timeEnd": "00-03-47", "sentence": "conceptually i've completely contains g because jeez architectural e constrained"}, {"timeStart": "00-03-48", "timeEnd": "00-03-54", "sentence": "the first trained up by us two model by minimizing independence between the two representations f n g"}, {"timeStart": "00-03-55", "timeEnd": "00-04-00", "sentence": "in other words she tries to catch up f while minimizing the original task calls"}, {"timeStart": "00-04-02", "timeEnd": "00-04-07", "sentence": "then we train our model by maximizing independence between the two representations"}, {"timeStart": "00-04-07", "timeEnd": "00-04-12", "sentence": "so aph tries to be different from g while minimizing the original task loss"}, {"timeStart": "00-04-13", "timeEnd": "00-04-16", "sentence": "and we repeat these steps alternatively"}, {"timeStart": "00-04-19", "timeEnd": "00-04-21", "sentence": "eventually"}, {"timeStart": "00-04-21", "timeEnd": "00-04-24", "sentence": "she can not catch a pass due to its architectural constraints"}, {"timeStart": "00-04-24", "timeEnd": "00-04-27", "sentence": "and by being outside of the bias function space g"}, {"timeStart": "00-04-27", "timeEnd": "00-04-29", "sentence": "FS to biased"}, {"timeStart": "00-04-30", "timeEnd": "00-04-34", "sentence": "to summarize we train f n g jointly in a min a max game"}, {"timeStart": "00-04-34", "timeEnd": "00-04-37", "sentence": "whereas tries to run away from g"}, {"timeStart": "00-04-37", "timeEnd": "00-04-39", "sentence": "while she tries to catch up f"}, {"timeStart": "00-04-39", "timeEnd": "00-04-45", "sentence": "our intention is that our model is forced to be different from multiple possible bias predictions"}, {"timeStart": "00-04-45", "timeEnd": "00-04-48", "sentence": "and therefore improve its device performance"}, {"timeStart": "00-04-50", "timeEnd": "00-04-52", "sentence": "now we share our experiment results"}, {"timeStart": "00-04-53", "timeEnd": "00-04-58", "sentence": "the first validate our model by building a dataset called biased enemies"}, {"timeStart": "00-04-58", "timeEnd": "00-05-01", "sentence": "during training we inject color bias"}, {"timeStart": "00-05-01", "timeEnd": "00-05-03", "sentence": "where each color highly correlates with each digit"}, {"timeStart": "00-05-04", "timeEnd": "00-05-10", "sentence": "therefore learning color is a shortcut to achieve high accuracy and navy trained models would be color biased"}, {"timeStart": "00-05-11", "timeEnd": "00-05-16", "sentence": "for evaluation we measure both biased and a biased accuracy"}, {"timeStart": "00-05-16", "timeEnd": "00-05-18", "sentence": "for the biased metric"}, {"timeStart": "00-05-18", "timeEnd": "00-05-20", "sentence": "bias is identical to the training set"}, {"timeStart": "00-05-20", "timeEnd": "00-05-25", "sentence": "and this measures the ability of the bias models to maintain a high in distribution performance"}, {"timeStart": "00-05-25", "timeEnd": "00-05-27", "sentence": "all generalizing to the unbiased setting"}, {"timeStart": "00-05-28", "timeEnd": "00-05-30", "sentence": "are d on biased metric"}, {"timeStart": "00-05-30", "timeEnd": "00-05-35", "sentence": "colors are randomly assigned to the label so that models relying on bias with perform poorly"}, {"timeStart": "00-05-36", "timeEnd": "00-05-42", "sentence": "for the models we set our vanilla model as an ordinary CNN that has large receptor field"}, {"timeStart": "00-05-42", "timeEnd": "00-05-48", "sentence": "and photo intentionally biased model we deliberately reduced its this receptive field to one by one"}, {"timeStart": "00-05-48", "timeEnd": "00-05-53", "sentence": "it can only perceive per pixel values so its more liable to over fit to color"}, {"timeStart": "00-05-54", "timeEnd": "00-05-56", "sentence": "and these are the results"}, {"timeStart": "00-05-57", "timeEnd": "00-06-01", "sentence": "vanilla virtues one hundred percent accuracy under the bias setting"}, {"timeStart": "00-06-01", "timeEnd": "00-06-08", "sentence": "this is how most machine learning methods are evaluated but does not show how much the model relies on bias her prediction"}, {"timeStart": "00-06-09", "timeEnd": "00-06-12", "sentence": "when you randomly assign colors to the label"}, {"timeStart": "00-06-12", "timeEnd": "00-06-14", "sentence": "the accuracy plummets to thirty three percent"}, {"timeStart": "00-06-15", "timeEnd": "00-06-20", "sentence": "for the intentionally biased model the accuracy plummets to ten per cent which is random chance"}, {"timeStart": "00-06-21", "timeEnd": "00-06-29", "sentence": "this shows the case where up seemingly high performing models has in fact over fitted to by us and does not generalize well to new situations"}, {"timeStart": "00-06-29", "timeEnd": "00-06-36", "sentence": "on the other hand our method achieves robust generalization by learning to be different from the biased model"}, {"timeStart": "00-06-36", "timeEnd": "00-06-41", "sentence": "the accuracy boosts to sixty four percent and it is superior to existing to bias ng methods"}, {"timeStart": "00-06-42", "timeEnd": "00-06-46", "sentence": "be further validate our model under realistic texture bias of omission it"}, {"timeStart": "00-06-47", "timeEnd": "00-06-52", "sentence": "as mentioned before you must not trained cnn's our bias towards texture"}, {"timeStart": "00-06-53", "timeEnd": "00-06-57", "sentence": "so consequently models fail when texture cues are absent"}, {"timeStart": "00-06-57", "timeEnd": "00-07-01", "sentence": "and does not generalize well too unseen texture signal combinations"}, {"timeStart": "00-07-03", "timeEnd": "00-07-09", "sentence": "in this experiment we forced train our models in the subset of emission it containing nine super causes"}, {"timeStart": "00-07-10", "timeEnd": "00-07-16", "sentence": "and then for evaluation we measured the biased metric by testing on the original image data set"}, {"timeStart": "00-07-16", "timeEnd": "00-07-19", "sentence": "here are bias is identical to the training set"}, {"timeStart": "00-07-19", "timeEnd": "00-07-26", "sentence": "and is the standard method of evaluating models yet it is blind to the model's ability to generalize beyond via shortcuts"}, {"timeStart": "00-07-26", "timeEnd": "00-07-30", "sentence": "for the am biased metric we can not get a perfectly unbiased data"}, {"timeStart": "00-07-30", "timeEnd": "00-07-35", "sentence": "so as a proxy we give more weight on unusual texture class combinations"}, {"timeStart": "00-07-35", "timeEnd": "00-07-38", "sentence": "we do this by getting texture clusters"}, {"timeStart": "00-07-39", "timeEnd": "00-07-43", "sentence": "and then compute accuracy for every texture class combination"}, {"timeStart": "00-07-43", "timeEnd": "00-07-50", "sentence": "so we divide number of correctly predicted samples with the total number of samples in each texture class company"}, {"timeStart": "00-07-50", "timeEnd": "00-07-53", "sentence": "so that we up wait unusual combinations"}, {"timeStart": "00-07-54", "timeEnd": "00-07-59", "sentence": "be further evaluate our model on imagine a day which has challenging examples"}, {"timeStart": "00-07-59", "timeEnd": "00-08-03", "sentence": "and on imagine us see which measures robustness to common corruptions"}, {"timeStart": "00-08-04", "timeEnd": "00-08-10", "sentence": "for the models we set our vanilla model as resident eighteen which has larger septic field"}, {"timeStart": "00-08-10", "timeEnd": "00-08-16", "sentence": "and we said art intentionally bias to model by a substituting some of the three by three colonels to one by one"}, {"timeStart": "00-08-16", "timeEnd": "00-08-21", "sentence": "therefore it has a small receptor field and it's liable to perfect to local texture"}, {"timeStart": "00-08-22", "timeEnd": "00-08-23", "sentence": "and these are the results"}, {"timeStart": "00-08-23", "timeEnd": "00-08-27", "sentence": "the vanilla model shows good performances in the biased accuracy"}, {"timeStart": "00-08-28", "timeEnd": "00-08-31", "sentence": "but the performance drops for the unbiased accuracy"}, {"timeStart": "00-08-32", "timeEnd": "00-08-37", "sentence": "the biased model performs worse as they are heavily biased towards texture by design"}, {"timeStart": "00-08-38", "timeEnd": "00-08-47", "sentence": "and revise this apply to the vanilla model to make you learn queues beyond those captured by the biased model we observe a general boost in all four accuracies"}, {"timeStart": "00-08-49", "timeEnd": "00-08-53", "sentence": "we finally validate our model on the action recognition task"}, {"timeStart": "00-08-53", "timeEnd": "00-08-58", "sentence": "action data sets have been reported to have a strong bias towards static cues"}, {"timeStart": "00-08-58", "timeEnd": "00-09-01", "sentence": "so models relying on static shortcuts rather than learn temporal signals"}, {"timeStart": "00-09-01", "timeEnd": "00-09-09", "sentence": "for instance for this opening fridge action all the models seizes the indoor static indoors of a fridge"}, {"timeStart": "00-09-09", "timeEnd": "00-09-11", "sentence": "rather than learned up temporal action"}, {"timeStart": "00-09-12", "timeEnd": "00-09-17", "sentence": "there are many real worlds narrows where the static who is absent"}, {"timeStart": "00-09-17", "timeEnd": "00-09-19", "sentence": "or even correlated that different class"}, {"timeStart": "00-09-19", "timeEnd": "00-09-23", "sentence": "in these cases the model would fail in generalization"}, {"timeStart": "00-09-24", "timeEnd": "00-09-26", "sentence": "and this experiment"}, {"timeStart": "00-09-26", "timeEnd": "00-09-31", "sentence": "the first train our models on the kinetics data set which has a strong bias towards static cues"}, {"timeStart": "00-09-31", "timeEnd": "00-09-36", "sentence": "and for evaluation we get the biased metric by testing on the original kinetics test set"}, {"timeStart": "00-09-36", "timeEnd": "00-09-44", "sentence": "and we get down biased metric by testing on them are metrics data set which has videos of a mime artist doing the same actions without context"}, {"timeStart": "00-09-46", "timeEnd": "00-09-50", "sentence": "for the models we set our vanilla model as a three d rest Nate"}, {"timeStart": "00-09-50", "timeEnd": "00-09-53", "sentence": "which can capture both temporal and static cues"}, {"timeStart": "00-09-53", "timeEnd": "00-09-59", "sentence": "and we set are intentionally biased model as a two d resident which can only capture static cues"}, {"timeStart": "00-10-00", "timeEnd": "00-10-02", "sentence": "and these are the results"}, {"timeStart": "00-10-02", "timeEnd": "00-10-05", "sentence": "by applying restaurant on the three d rest Nate"}, {"timeStart": "00-10-05", "timeEnd": "00-10-11", "sentence": "it encourages to utilizes temporal modeling capacity by forcing it to reason differently from the two d resonant"}, {"timeStart": "00-10-11", "timeEnd": "00-10-16", "sentence": "therefore through bias improves the accuracy of both kinetics new medics data set"}, {"timeStart": "00-10-18", "timeEnd": "00-10-19", "sentence": "in conclusion"}, {"timeStart": "00-10-19", "timeEnd": "00-10-26", "sentence": "cross bias journal is ation is a realistic problem in deployment scenarios where models explored by stork us to recognize the target"}, {"timeStart": "00-10-27", "timeEnd": "00-10-32", "sentence": "and when bias can be characterized by a models my bias is a good candidate"}, {"timeStart": "00-10-32", "timeEnd": "00-10-38", "sentence": "revise effectively removes bias and biased amnesty admission er classification and action recognition"}, {"timeStart": "00-10-39", "timeEnd": "00-10-41", "sentence": "thank you for watching"}]}, {"title": "Understanding Regularisation Methods for Continual Learning @ICML CL Workshop", "authors": "Frederik Benzing", "abstract": "", "publicationOrg": "ICML", "year": "2020", "pdfUrl": "https://arxiv.org/pdf/2006.06357.pdf", "pdfPath": "/data/cache/2/PDFs/UnderstandingRegularisationMethodsforContinualLearningICMLCLWorkshop.pdf", "publicationUrl": "https://arxiv.org/pdf/2006.06357.pdf", "codeUrl": "https://github.com/freedbee/continual_regularisation", "datasetUrl": "", "videoUrl": "https://www.youtube.com/embed/GHV302OC2Ns", "videoPath": "/data/cache/2/videos/Understanding Regularisation Methods for Continual Learning @ICML CL Workshop.mp4", "pdfText": "Despite much progress, there remain many gaps between artificial and biological intelligence. Among these, the ability of animals to acquire new knowledge throughout their life has inspired a growing body of literature. In continual learning, a number of tasks are presented sequentially and the algorithm should learn new tasks without overwriting previous knowledge and without revisiting old data. An important class of algorithms proposed to solve this problem are regularisation approaches . They identify important weights after each task and protect them from large changes by modifying the loss function. At the heart of these algorithms lies the question how to estimate parameter importance, as this directly determines which parameters will be kept close to their old value and which parameters can be modified freely to solve new tasks. In this article, our aim is to understand why different importance measures proposed in the literature are effective. Notably, this question remains open for two well-known regularisation methods, Synaptic Intelligence (SI)  and Memory Aware Synapses (MAS) . The latter approach has a purely heuristic motivation, while the former makes simplifying assumptions, violated in a deep learning setting, to theoretically justify its importance measure.Our main contribution is to provide mathematical as well as empirical evidence that both these methods, SI and MAS, despite their different motivations, approximate a rescaled version of the Fisher Information. The Fisher Information has been used for continual learning  along with a clear theoretical, Bayesian justification . Thus, on top of unifying different regularisationapproaches proposed so far, our results give a more sound theoretical explanation for the effectiveness of two more continual learning algorithms.To establish our claims, we first investigate MAS and give a simple mathematical derivation to show that MAS approximates what we call the Absolute Fisher. We carefully validate the assumptions of our derivations as well as the resulting claims empirically on standard continual learning benchmarks. As a second -maybe surprising -step we show that the importance approximation of SI is biased and that this bias contributes most to the continual learning performance of SI. Finally, we show that through this bias SI is -like MAS -closely related to the Absolute Fisher.Before reviewing the continual learning literature we point out that one of our algorithms is linked to the recently proposed Loss-Change-Allocation (LCA) . This method gains insights into the training process of neural networks  by assessing how much the changes of individual parameters contribute to learning. To this end, LCA approximates the same path integral as Synaptic Intelligence . For the approximation, the gradient has to be evaluated on the entire training set for each parameter update. Our method 'Unbiased Synaptic Intelligence' gives an unbiased estimate of the same quantity, but only using mini-batches, thus being computationally cheaper.The problem of catastrophic forgetting in neural networks has been known and studied for many decades . In the context of deep learning, it recently started receiving more attention again . An attempt to overcome this problem was made with the algorithm Elastic Weight Consolidation (EWC) . We will review EWC and two other regularisation approaches -Synaptic Intelligence  and Memory Aware Synapses  -as well as their relatives  more closely in the next section. The idea of regularisation was also applied in the context of Bayesian neural networks in , who use the weights' posteriors after having learned a task as the prior for learning the next task.On top of regularisation approaches, Parisi et al.  identify two more classes of continual learning approaches.One of these classes are replay methods, which rely on data from old distributions to protect knowledge while learning new tasks. For example, iCarl  uses stored datapoints from old datasets and computes nearest neighbours in feature space for classification. GEM  and follow up work  project gradients from new tasks onto directions that do not increase the loss on stored examples. However, empirical results from  suggest that one of the most effective ways to use small samples of old data is to simply mix them with the mini-batches of new data during training. To avoid storing old examples, which strictly speaking violates continual learning desiderata, generative replay methods train generative models to simulate old data distributions . In this context, biologically inspired replay systems which mimic the hippocampus have also been put forward .The third class of continual learning algorithms are architectural approaches, which use network expansion to learn new tasks without corrupting previous knowledge. PathNet  uses evolution to identify good subnetworks for each individual tasks, while  extends the network through neural architecture search.  first train on a task, then compress the resulting net and iterate this process. Mixture models, with growing numbers of components, are investigated in .Finally,  point out that different continual learning scenarios and assumptions with varying difficulty were used across the literature. They also clearly define and distinguish these scenarios and test how well different algorithms perform in them.In continual learning we are given K datasets D 1 , . . . , D K sequentially. When training a neural net with N parameters \u03b8 \u2208 R N on dataset D k , we have no access to the previously seen datasets D 1:k\u22121 but should retain some memory of them. Regularisation based approaches introduced in  do so by modifying the loss function L k related to dataset D k . Let us denote the parameters obtained after finishing training on task k by \u03b8  and let \u2126 (k) be some positive semi-definite N \u00d7 N matrix. When training on task k, regularisation methods use the losswhere c > 0 is a hyperparameter. The first term in the loss function is the standard loss on task k.The second term makes sure that the parameters do not move away too far from the the previous parameters.It can already be seen that the matrix \u2126 (k) can quickly become prohibitively large as it has N 2 entries, where N is the number of parameters. In practice, \u2126 is therefore often approximated by a diagonal matrix diag(\u03c9 1 , . . . , \u03c9 N ). The loss function defined above then simplifies t\u00f5and \u03c9 i is often referred to as the importance of parameter \u03b8 i . Usually, we have \u03c9where \u03c9 i is the importance for the most recently learned task k.How good a regularisation based approach is, depends on how meaningful its importance estimates \u03c9 i are. Different such estimates have been proposed and the aim of this work is to understand why different approaches work and how similar they are.Broadly speaking (and to the best of our knowledge), three methods to measure parameter importance have been proposed, but see also below for adaptations of these methods.  uses the diagonal of the Fisher Information as importance measure. It is evaluated at the end of training a given task. To define the Fisher Information Matrix F , let us assume we are given a dataset X and that, for a datapoint X the network predicts a probability distribution (or equivalently, a vector of probabilities) q X with entries q X (y) for y \u2208 L, where L is the set of potential labels. Let us further assume that we use a categorical cross entropy loss CEL(q X , y) and write g(X, y) = \u2202CEL(q X ,y) \u2202\u03b8 for its gradient.Then the Fisher Information F is given asConcretely, taking only the diagonal of F meansThe Empirical Fisher Information matrix is an approximation of the Fisher Information, which rather than taking the expectation over y \u223c q X takes the (deterministic) label y given by the labels of the dataset. We will also define the Predicted Fisher Information as taking the maximum-likelihood predicted label, i.e. the argmax of q X (y). Our empirical findings suggest that these different version of the Fisher Information can be used almost exchangeably, see appendix.The Fisher Information is often used as a measure of parameter sensitivity and under the assumption that the learned label distribution q X is the real label distribution it equals the Hessian of the loss (with respect to the real, not the empirical, label distribution) . Its effectiveness has a clean theoretical interpretation .Memory Aware Synapses  heuristically argues that the sensitivity of the final layer with respect to a parameter should be used as the parameter's importance. Similar to the Fisher Information, this sensitivity is evaluated after training a given task. Denoting, as before, the final layer of learned probabilities by q X , this meanswhere the norm is Euclidean.  Synaptic Intelligence  approximates the contribution of each parameter to the decrease in loss and uses this contribution as importance. To formalise the 'contribution of a parameter', let us denote the parameters at time t by \u03b8(t) and the loss by L(t). If the parameters follow a smooth trajectory in parameter space, then we can write the decrease in loss between time 0 and T asThe i-th summand in (4) can be interpreted as the contribution of parameter i to the decrease in loss. While we cannot evaluate the integral precisely, we can use a first order approximation to obtain the importances. To do so, we write \u2206 i (t) = (\u03b8 i (t + 1) \u2212 \u03b8 i (t)) for an approximation of \u03b8 i (t)dt and getIn addition, SI rescales its importances as follows 2The authors of SI provide a theoretical argument for why this importance measure is equal to the Hessian of the loss under some assumptions. Not all of these assumptions seem realistic and we will show theoretically and empirically that the assumption of using full-batch-gradient-descent, which is violated in a deep-learning setting, has an important effect.Related regularisation approaches. Strictly speaking, we presented a version of EWC described in  and tested in . Riemannian Walk  is a combination of EWC and a 'KL-rescaled' version of SI. Similar to EWC  uses the Fisher Information for regularisation, but rather than a diagonal approximation, finds a block diagonal approximation allowing to take layer-wise parameter interactions into account. While improving performance, this comes at the cost of a memory requirement of order N \u2022 K (where N is the number of parameters and K the number of tasks) which is enough memory to train and store separate networks for each task.  .In this section we describe our theoretical motivations and design experiments to test our hypotheses, focussing on Memory Aware Synapses (MAS) in Section 4.1 and on Synaptic Intelligence (SI) in Section 4.2. The empirical results of our experiments will be presented in Section 5. An overview of different algorithms described here can be found in .We start by taking a closer look at the definition of the importances of MAS. We apply linearity of derivatives, the chain rule and write y 0 = arg max q X to obtainHere we made the assumption that the sum is dominated by its maximum-likelihood label y 0 , which should be the case if the network classifies its images confidently, i.e. q(y 0 ) q(y) for y = y 0 . Using the same heuristic for the Fisher Information we obtainComparing this with (6) and recalling notation g(X, y 0 ) = \u2202 log q X (y 0 ))/\u2202\u03b8, the similarity between MAS and the Fisher Information becomes apparent. The only difference is that MAS has an additional : Overview of different algorithms and their importance measures for one task. Algorithms on the left calculate importance 'online' along the parameter trajectory during training. Algorithms on the right calculate importance at the end of training a task by going through (part of) the training set again. Correspondingly, the sum is over timesteps t (left) or datapoints X (right). N is the number of images over which is summed. Note that all the algorithms on the left rescale their final importances as in equation . In all algorithms \u2206(t) = \u03b8(t + 1) \u2212 \u03b8(t) refers to the parameter update at time t, which depends on both the current task's loss and the regularisation loss. Moreover, (g t + \u03c3 t ) refers to the stochastic gradient estimate of the current task's loss (where g t is the full gradient and \u03c3 t the noise) given to the optimizer (together with the regularisation loss) to calculate the parameter update. In contrast, (g t + \u03c3 t ) refers to an independent stochastic gradient estimate of the current task loss. For a datapoint X, q X denotes the predicted label distribution and g(X, y) refers to the gradient of datapoint X if it had label y.Namefactor of 2q X (y 0 ) and g(X, y 0 ) is not squared. We therefore hypothesise that MAS is almost linearly dependent of the Absolute Fisher (AF), which is analogous to the Fisher Information, but takes absolute values of gradients rather than their squares, c.f. .Our hypothesis is based on two assumptions: (1) The sum over the labels is dominated by its maximum likelihood label y 0 , cf. equation .  We assume that the factor 2q X (y 0 ) is roughly constant across most images X. To check (1), we explicitly evaluate the term of the sum based on y 0 (and call this \u03c9(MASX), see ), and compare it to the entire sum (MASX vs MAS). Intuitively,is justified because at the end of training most images X will have q X (y 0 ) \u2248 1. Note that even images with q X (y 0 ) = 0.5 deviate only by a factor of 2, and still lead to a large correlation between MAS and AF. An empirical validation of (2) is checking if MASX and AF are linearly dependent. Both assumptions (1)&(2) are applicable in a wide range of settings, see Figures 2 (A) and 1b for strong empirical confirmations of them on standard benchmarks.To calculate \u03c9(SI), we need to calculate the product p = \u2202L(t) \u2202\u03b8 \u2022 \u2206(t) at each step t. Since evaluating the full gradient  \u2202\u03b8 is prohibitively expensive, SI  uses a mini-batch. The resulting estimate is biased since the same mini-batch is used for the parameter update \u2206(t) and the estimate of  \u2202\u03b8 . We now give the calculations detailing the argument above. For ease of exposition, let us assume that the network is optimized using vanilla SGD with learning rate 1. Given a mini-batch, denote its gradient estimate by g + \u03c3, where g =  \u2202\u03b8 denotes the real gradient and \u03c3 the mini-batch noise. With SGD the parameter update equals \u2206(t) = g + \u03c3. Thus, our product p should be p = g \u2022 (g + \u03c3). However, using g + \u03c3, which was used for the parameter update, to estimate  \u2202\u03b8i results in p biased = (g + \u03c3) 2 . Thus, the gradient noise introduces a bias of E[\u03c3Unbiased Synaptic Intelligence. It is easy to design an unbiased estimate of p by using two independent mini-batches to calculate the parameter update and to estimate g. This way we get \u2206(t) = g + \u03c3 and an estimate g + \u03c3 for g from an independent mini-batch with independent noise \u03c3 . We obtain p unbiased = (g + \u03c3 ) \u2022 (g + \u03c3) which in expectation is equal to p = g \u2022 (g + \u03c3). Based on this we define an unbiased importance measur\u1ebdBias-Only version of SI. To isolate the bias, we can simply take the difference between biased and unbiased estimate. Concretely, this gives a measure which only measures the bias of SI and is independent of the path integral,Observe that this estimate multiplies the parameter-update \u2206(t) with nothing but stochastic gradient noise. From the perspective of SI, this should not be meaningful and have poor continual learning performance.Which part dominates SI? The approximations presented in equation imply that L(0) \u2212 L(T ) \u2248 i\u03c9 i (SI). Thus, comparing i\u03c9 i for SI, SIU, SIB to L(0) \u2212 L(T ) measures how good the approximations are and shows whether SI is dominated by its bias, see . Further, comparing the continual learning performance of SI, SIU, SIB shows whether SI's continual learning performance relies on the approximation of the path integral or its bias, see , Exp No 2.In this section we will indicate the relation between the importance measure \u03c9(SI) and the Fisher Information. Recall that \u03c9(SI) is a sum over terms  \u2202\u03b8 \u2022 \u2206(t), where \u2206(t) = \u03b8(t + 1) \u2212 \u03b8(t) is the parameter update at time t. Recall as well that both terms in this product,  \u2202\u03b8 as well as \u2206(t), are computed/approximated using the same mini-batch and thus the have the same noise.For a precise understanding, we need to take into account how the optimizer uses stochastic gradients to update the parameters. In our experiments (as well as in the original SI experiments) we used the Adam optimizer . Given a stochastic gradient g t + \u03c3 t , it keeps a running average of the gradient m t = (1 \u2212 \u03b2 1 )(g t + \u03c3 t ) + \u03b2 1 m t\u22121 as well as a running average of the squared gradientsIgnoring some normalisation terms and the learning rate, the Adam parameter update is given by \u2206(t) = m t /(, where \u03b2 1 = 0.9 and \u03b2 2 = 0.999. Thus,Assuming that the noise \u03c3 t is considerably bigger than the gradient g t we obtainThe precise assumption made here is (1 \u2212 \u03b2 1 )\u03c3 2 t \u03b2 1 m t\u22121 g t (we ignore the term \u03c3 t m t\u22121 since E[\u03c3 t m t\u22121 ] = 0 and since we average SI over many time steps). This assumption is supported strongly by , where the difference between green (SI) and blue (SIU) line is caused precisely by (1 \u2212 \u03b2 1 )\u03c3 2 , see B.1 for full details. We also refer to  and Figures D.1, D.2 and F.5, F.4 for more evidence for the assumption that the noise is considerably bigger than the gradient. Next, we consider v t . It is a slowly moving average of (g t + \u03c3 t ) 2 and will therefore be approximately E[(g t + \u03c3 t ) 2 ]. It is thus reasonable to expect that \u221a v t is typically roughly proportional to |g t + \u03c3 t |.This clearly does not hold for all possible distributions of g t + \u03c3 t , but is unlikely to be violated by non adversarially chosen, realistic distributions. With  andOur resulting approximation\u03c9(SI) \u221d \u223c t |g t + \u03c3 t | is very similar to the (Empirical) Absolute Fisher. The only differences are (1) the gradient in\u03c9(SI) is evaluated in mini-batches rather than on single images; (2) the sum is taken 'online' along the parameter trajectory rather than at the final point of the trajectory. (1) does not have a large effect when \u03c3 t g t (as detailed in Appendix B.2 ) which is the case, c.f. our findings about gradient noise. The influence of (2) is hard to quantify theoretically and depends on how much the gradients change along the parameter trajectory. To capture this difference, we define the Online Absolute Fisher as\u03c9(OnAF) = t |g + \u03c3 t | and empirically test how similar \u03c9(OnAF) and\u03c9(AF) are.Our heuristics suggest that\u03c9(SI) and\u03c9(OnAF) are very similar. If in addition, we empirically see that\u03c9(OnAF) and\u03c9(AF) are similar, this indicates that SI approximates AF, a rescaled version of the Fisher Information.  empirically confirm both parts of this hypothesis.We emphasise that our derivation above assumes that the parameter update \u2206(t) is given by the gradients of the current task and ignores the regularisation term. The latter will change update direction and momentum of the optimizer. Thus, strong regularisation will make the relation between SI and OnAF noisy, but not abolish it.  and Sec F.3 for confirmations hereof.In this section we empirically test our three main hypotheses: Our experiments closely follow . We use the Permuted MNIST  benchmark with 10 tasks, a fully connected ReLU architecture and a single output-head ('domain incremental' setting, c.f. ) and the Split CIFAR 10/100 benchmark with the keras default CNN and a multi-head output ('task incremental'). Following , but unlike , we re-initialise model variables (fully connected layers, etc.) after each task and usually find better performance in our hyperparameter searchers. We defer further experimental details to the Appendix A. Code is available on github 4 .MAS and Absolute Fisher. Our first objective is to test whether MAS and Absolute Fisher are related. To this end we calculate \u03c9(MAS), \u03c9(MASX), \u03c9(AF) and show scatter plots of these values in  (A). We find that they are almost identical.  shows equally strong correlations on the other tasks and datasets, presenting strong evidence for our heuristics. We also evaluated continual learning algorithms based on \u03c9(MAS), \u03c9(MASX), \u03c9(AF) and found very similar results, Why does SI work? To understand how large the bias of SI is, we track the sum i\u03c9 i of importances for SI and its unbiased version SIU. As a control we include the decrease of the loss L(0) \u2212 L(T ) which the path-integral (c.f. equation ) approximates, as well as an first order approximation of the path-integral without relying on stochastic gradient estimates (computing the full training set gradient at each step). The results are portrayed in . They indicate that (1) the bias is 5-6 times larger than the unbiased part; (2) using an unbiased gradient estimate and using the entire training set gradient gives almost identical values of i \u03c9 i supporting the validity of the unbiased estimator; (3) even the unbiased first order approximation of the path integral overestimates the decrease in loss. This is consistent with previous empirical studies showing that the loss has positive curvature . See Figures F.5, F.4 for similar results on CIFAR and other MNIST tasks.We have seen that the SI's bias is considerably larger than its unbiased component. This does not fully explain which of these parts is more essential for continual learning. We therefore run continual learning algorithms based on SI, its unbiased version (SIU) as well as a version which isolates the bias (SIB). The results are shown in , Exp No. 2. SIU consistently has worse performance than SI and SIB has the same or slightly better performance than SI. From SI's perspective this is rather surprising and is convincing evidence that SI's continual learning capability is due to its bias.Is SI related to Absolute Fisher? Our main assumption in relating SI to Online Absolute Fisher (OnAF) was that the contribution of the noise (bias) to SI is bigger than its unbiased part, c.f. equation . We have already seen strong evidence that this is the case ). Therefore it may be expected that OnAF and SI do have significant correlations: On all tasks of MNIST and the first task of CIFAR, SI and OnAF have (Pearson) correlations > 0.93; on tasks 2-6 of CIFAR we find weaker, but significant correlations around 0.  We also find that OnAF and AF are surprisingly similar, thus closely relating SI and AF ) supporting our hypotheses. We also evaluate the corresponding continual learning algorithms SI, OnAF and AF, see , Exp 3. The results are very similar, indicating that SI, like MAS, works because it approximates a rescaled version of the Fisher Information.Is Absolute or Real Fisher better for CL? We have seen evidence that both MAS and SI rely on estimating a rescaled Fisher Information. This raises the questions whether a rescaled version of the Fisher or the Fisher itself are more effective for continual learning. To answer this, we compare MAS, AF and EWC, since these methods all evaluate the importance at the end of training. The results are shown in , Exp 4. All algorithms perform similarly, only the difference between MAS and EWC on CIFAR is significant (p = 0.04, t-test, not corrected for multiple comparisons). Which algorithm performs best may well depend on hyperparameters and the precise training set-up.We have focused on regularisation approaches for continual learning and more specifically, how these methods assess parameter importance. We investigated three different importance measures proposed in the literature, namely Elastic Weight Consolidation, Synaptic Intelligence and Memory Aware Synapses. For standard (non-bayesian) neural networks these are (to the best of our knowledge) the only approaches to regularisation based continual learning. Other regularisation-related work directly relies on one of these algorithms or combinations of them . Originally, all three methods were motivated rather differently and suggested seemingly orthogonal importance measures. However, we gave heuristic mathematical derivations indicating that all methods nevertheless -and somewhat accidentally -are very closely related to the Fisher Information. We thoroughly checked the assumptions underlying our heuristics in realistic, standard continual learning set-ups and found convincing empirical support for our claims.  On the one hand, our work unifies regularisation approaches proposed so far and highlights unexpected similarities. Our mathematical derivations do not only explain some surprising results, but also imply that our findings are more robust than merely observing empirical correlations.On the other hand, our findings can have immediate practical consequences, as they help explain which algorithms are likely to work in which setting. There are several situations, in which we can make grounded predictions on the relative performance of MAS, SI and EWC. To be concrete, we exhibit how our insights may explain empirical observations presented in , which is an extensive comparative study of continual learning algorithms. (1)  finds that SI is sensitive to task-ordering when dataset sizes are imbalanced -namely, when small datasets are learned first SI overfits more. When training for a fixed number of epochs, small datasets will have small SI importances (since its biased sum contains few terms, independent of the path-integral) so that the net is subsequently less regularised and prone to overfit. (2)  notes that EWC suffers from having many weights with small importances and performs worse than MAS. Note that relatively, MAS' (Absolute Fisher) rescaling makes small importances much bigger, explaining why it alleviates the small-importance problem.(3)  reports bad performance of SI in various settings. This may be due to learning rate decay, which gives more weight to early terms in SI's running sum. This makes the sum less related to the Fisher at the end of training, causing worse performance of SI. We note that (1) and (3) could be fixed by replacing SI by a normalised version of OnAF. We leave testing these hypoteheses to future work and hope that our insights help developing, testing and improving regularisation methods.For P-MNIST we use a fully connected net with ReLu activations and two hidden layers of 2000 units each. For Split CIFAR10/100 we use the default Keras CNN for CIFAR 10 with 4 convolutional layers and two fully connected layers, dropout  and maxpooling, see 1. We use Glorot-uniform  initialization for both architectures.We use the Adam Optimizer  with tensorflow  default settings (lr = 0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999, = 10 \u22128 ) and batchsize 256 for both tasks like . We reset the optimizer variables (momentum and such) after each task.Recall that we applied the operation max(0, \u2022) (i.e. a ReLU activation) to the importance measure of each individual task (equation of main paper), before adding it to the overall importance. In the original SI implementation, this seems to be replaced by applying the same operation to the overall importances (after adding potentially negative values from a given task). No description of either of these operations is given in the SI-paper. In light of our findings, our version seems more justified.Somewhat naturally, the gradient \u2202L \u2202\u03b8 usually refers to the cross-entropy loss of the current task and not the total loss including regularisation. For CIFAR we evaluated this gradient without dropout (but the parameter update was of course evaluated with dropout).  For OnAF, similarly to SI, we used the gradient evaluated without dropout on the cross-entropy loss of the current task for our importance measure.When estimating Fisher Information / MAS, we do not iterate over the whole training set, due to the considerable computational cost. We sample uniformly random subsets of size 1000 (P-MNIST) and 500 (CIFAR). In preliminary experiments with twice as large subsets we found no improvement in average accuracy. When comparing two different measures (e.g. MAS and AF) we use the same samples to calculate both measures to avoid unnecessary noise in our comparisons. .1: CIFAR 10/100 architecture. Following  we use the keras default architecture for CIFAR 10. Below, 'Filt.' refers to the number of filters of a convolutional layer, or respectively the number of neurons in a fully connected layer. 'Drop.' refers to the dropout rate. 'Non-Lin.' refers to the type of non-linearity used.  For all methods and benchmarks, we performed grid searches for the hyperparameter c and over the choice whether or not to re-initialise model variables after each task.The grid of c included values a \u2022 10 i where a \u2208 {1, 2, 5} and i was chosen in a suitable range (if a hyperparameter close to a boundary of our grid performed best, we extended the grid).For CIFAR, we measured validation set performance based on at least three repetitions for good hyperparameters. We then picked the best HP and ran 10 repetitions on the test set. For MNIST, we measured HP-quality on the test set based on at least 3 runs for good HPs.Additionally, SI and consequently SIU, SIB as well as OnAF have rescaled importance (c.f. equation from main paper). The damping term \u03be in this rescaling was set to 0.1 for MNIST and to 0.001 for CIFAR following  without further HP search.All results shown are based on the same hyperparameters obtained -individually for each methodas described above. They can be found in .2. We note that the difference between the HPs for MAS and MASX might seem to contradict our claims that the two measures are almost identical (they should require the same c in this case), but this is most likely due to similar performance for different HPs and random fluctuations. For example on CIFAR, MAS had almost identical validation performance for c = 200 (best for MAS) and c = 1000 (best for MASX) (74.2 \u00b1 0.7 vs 74.1 \u00b1 0.5).Also for the other methods, we observed that usually there were two or more HP-configurations which performed very similarly. The precise 'best' values as found by the HP search and reported in .2 are therefore subject to random fluctuations in the grid search.Here we give two calculations omitted Secontion 4.2.2 of the main paper.We claimed that the difference between SI and SIU (green and blue line) seen in  (and also in , F.5) is due to the term (1 \u2212 \u03b2 1 )\u03c3 2 t . To see this, recall that for SI, we approximate \u2202L(t) \u2202\u03b8 by g t + \u03c3 t , which is the same gradient estimate given to Adam. So we get SI:For SIU, we use an independent mini-batch estimate g t + \u03c3 t for \u2202L(t) \u2202\u03b8 and therefore obtain SIU:Taking the difference between these two and ignoring all terms which have expectation zero (note that E[\u03c3 t ] = E[\u03c3 t ] = 0 and that \u03c3 t , \u03c3 t are independent of m t\u22121 and g t ) givesSI \u2212 SIU:as claimed.Note also that in expectation SIU equals (1 \u2212 \u03b2 1 )\u221a vt+ so that a large difference between SI and SIU really meansThe last approximation here is valid because \u03b2 1 m t\u22121(1 \u2212 \u03b2 1 )g t which holds since (1) \u03b2 1 (1 \u2212 \u03b2 1 ) and  we have E[m t\u22121 ] \u2248 g t and m t\u22121 additionally has a noise component, which g t does not have, so thatFor this subsection, let us slightly change notation and denote the images by X 1 , . . . , X D and the gradients (with respect to their labels and the cross entropy loss) by g + \u03c3 1 , . . . g + \u03c3 D . Here, again g is the overall training set gradient and \u03c3 i is the noise (i.e. D i=1 \u03c3 i = 0). Then the Empirical Fisher is given byWe want to compare this to evaluating the squared gradient over a batch. Let i 1 , . . . , i b denote uniformly random, independent indices from {1, . . . , D}, so that X i1 , . . . , X i b is a random minibatch of size b. Let g + \u03c3 be the gradient on this mini-batch. We then have, taking expectations over the random indices,This conclusion may seem surprising, but please recall that it is only based on one assumption, namely that the gradient noise is considerably bigger than the gradient itself -an assumption for which we have collected ample evidence.Concretely, we assume E[(g + \u03c3) 2 ] (b \u2212 1)g 2 . Note that unlike in other sections of this manuscript, here \u03c3 refers to the gradient noise of individual images (rather than mini-batch noise). We have seen that even with a batch-size of 256 (i.e. when we reduce then noise by a factor of 256) the noise is still much bigger than the gradient (recall one more time , F.5, D.1, D.2), so that our assumption is true with room to spare.We also mention preliminary and rather unsurprising experiments, in which we evaluated the Empirical Fisher in mini-batches on P-MNIST (at the end of training a tasked) and observed the same performance as when evaluating the Empirical Fisher (and also 'real' Fisher, for that matter) on single images.Based on our findings, we tried several adoptions of SI as described below.Firstly, rather than taking the running sum of the product of gradient and update for SI, we experimented with an exponential moving average (EMA). We tried this based on our evidence that SI approximates some form of the Fisher Information. The EMA puts more weight on recently observed samples of this value, which should be more related to the actual Fisher Information and discards information which stems too far from the past. We performed HP searches over different decay factors for the running average on P-MNIST and found small improvements. Additionally, we found that the EMA is less stable if training time is increased (to e.g. 100 or 200 epochs per task). In this case, the magnitude of the EMA after for example the first task varies greatly from run to run (since the parameters may or may not be in a very flat region of the loss landscape), while the running sum (as in SI) is more stable (as we observed empirically).Secondly, observe that the magnitude of the per-task-importance of SI depends largely on how long we train, rather than on the change in loss. This can be seen for examples in .5, where the importances of the CIFAR 100 tasks are considerably smaller than the importance of the CIFAR 10 task. This is partly because each class in CIFAR 100 has 10 times less training samples, corresponding to 10 times less training iterations when keeping the number of epochs fixed (at 60 in our case). This suggests rescaling the per-task-importance by the number of training iterations of that task. We simply divided the importance of each task by the number of training iterations of that task (and performed a new HP search). We found no change in performance with this rescaling.Thirdly, recall that we argued that SI is closely related to the running sum of absolute gradient values, OnAF. We also implemented an importance measure based on the running sum of squared gradients (OnF -Online Fisher), as this variant more closely matches the Real Fisher Information. We observed a slight decrease in performance.We believe that our findings may indicate that regularisation based continual learning is fairly robust to rescaling its importance measures, at least on the datasets/settings we tested.Here, we quantitatively assess the noise magnitude outside the continual learning context. Recall that  from the main paper, as well as  and F.5 already show that the noise dominates the SI importance measure, which indicates that the noise is considerably larger than the gradient itself.To obtain an assessment independent of the SI continual learning importance measure, we trained our network on MNIST as described before, i.e. a ReLu network with 2 hidden layers of 2000 units each, trained for 20 epochs with batch size 256 and default Adam settings. At each training iteration, on top of calculating the stochastic mini-batch gradient used for optimization, we also computed the full gradient on the entire training set and computed the noise -which refers to the squared 2 distance between the stochastic mini-batch gradient and the full gradient -as well as the ratio between noise and gradient, measured as the ratio of squared 2 norms. The results are shown in 1. In addition, we computed the fraction of iterations in which the ratio between noise and squared gradient norm is above a certain threshold, see 2.It is not fully clear which version of Fisher Information is used for EWC and its variants, since 'Empirical' and 'Real' Fisher Information are both often referred to simply as 'Fisher Information' in 2: y-value shows fraction of training iterations in which the ratio between mini-batch noise and full training set gradient was at least x-value. Data obtained as described in main text / 1, in particular the batch-size was 256. 'Ratio' refers to the ratio of squared 2 norms of the respective values.the literature. Neither  nor  make code available, or provide details in the description of their algorithms.We compare different variants here and note that  also hints at such a comparison. We compare 'empirical', 'predicted' as well as 'real' Fisher, as described in the main paper. Note that, at least with a na\u00efve implementation, the real Fisher takes 10 times longer to compute than empirical and predicted Fisher, where 10 is the number of potential output labels. More efficient ways to calculate the Fisher Information  are not directly supported by standard deep learning libraries (at the time of writing, to the best of our knowledge).We report continual learning performance based on the different Fisher Informations in .1 and find very little difference.Additionally, we trained SI on P-MNIST and Split CIFAR and evaluated the different kinds of Fisher at the end of each task using the same samples of the training set (10 tasks for P-MNIST and 6 tasks for CIFAR) and calculated the Pearson Correlations.For real and empicial Fisher, on P-MNIST the correlation was between 0.49 and 0.91; on Split CIFAR it was between 0.77 and 0.99.For real and predicted Fisher, on P-MNIST the correlation was between 0.75 and 0.92; on Split CIFAR it was between 0.77 and 1.00. For SI, we note that the observation that most of the SI importance is due to its bias is consistent across datasets and tasks, see  and F.5. Intriguingly, on CIFAR we find that the unbiased approximation of SI slightly underestimates the decrease in loss in the last task, suggesting that strong regularisation pushes the parameters in places, where the cross-entropy of the current task has negative curvature.We show analogues of  consisting of only scatter/ only intensity plots in .6. Note that for both architectures there are a few million weights. The scatter plots are overcrowded and show the whole range of dependencies between two measure that can possibly occur. Intensity plots show the dependencies that the majority of weights adheres to. It seems plausible that the correlation between AF and OnAF we observed is due to training on tasks for a long time -the model might converge quickly so that the sum of OnAF is dominated by summands which approximate AF close to the final point in training. To test this, we trained our networks for varying numbers of epochs on the first task of P-MNIST and Split CIFAR10/100 and measured the correlation between OnAF and AF. The results in Tables F.1, F.2 show that the correlation between AF, OnAF does not rely on long training duration. On MNIST, we even see decreasing correlation with longer training time, this may be due to either an actual decrease in correlation or due to higher variance when estimating AF using 10000 samples (or both). Here, we show correlations of SIU, SIB with OnAF, AF as controls, see .1. We generally find that SIU is less correlated with AF/OnAF than SI, SIB as predicted by our heuristic derivations. This is in line with our hypothesis that performance of these algorithms is largely explained by their relation to (On)AF.The only exception is the first task of CIFAR (i.e. CIFAR10), where correlation with OnAF is 0.994 \u00b1 0.001 for SI; 0.991 \u00b1 0.001 for SIU; 0.979 \u00b1 0.003 for SIB. The similarity between SIU and OnAF in this situation is explained by the weights with largest OnAF importance: If we remove the 5% of weights which have highest OnAF importance, correlation between OnAF, SIU drops to 0.599 \u00b1 0.027, but for SI resp. SIB the same procedure yields 0.977 \u00b1 0.002 resp. 0.965 \u00b1 0.003. This indicates that for the first task of CIFAR there is a small fraction of weights with large OnAF importances, which are dominated by the gradient rather than the noise. The remainder of weights is in accordance with our previous observations and dominated by noise, recall also  We perform two experiments to check whether the weaker correspondence between SI, OnAF on CIFAR tasks 2-6 as compared to CIFAR task1 is really due to regularisation. Firstly, we set the hyperparameter c governing the strength of regularisation to 0. We show results in 2, finding that correspondence is as strong on tasks 2-6 as it was on the first task. Secondly, we precisely follow the setting of the original SI method  and do not reinitialize the network weights after each task. Note that in this setting, the optimal hyper parameter as found by our HP search is c = 0.5 and validation performance is slightly worse (72.4 \u00b1 1.0 without re-initialisation vs 74.9 \u00b1 0.6 with reinitialisation and c = 0.5). In addition to smaller c, the lack of reinitialisation means that the parameters will never get too far away from the optimum of the old task during optimization, meaning that the regularisation-gradients are smaller. Indeed, we find that in this case there is a strong correspondence between OnAF and SI as shown in 2.Finally, we point out that our comparisons are usually obtained after applying a max(0, \u2022) (c.f. equation  of main paper) to the SI (and SIU, SIB) importances as the algorithms diverge without this operation. We present results without this max operation in F.3, further showing that (1) correlation between OnAF and SI is weakened by strong regularisation and (2) the correlation is due to the bias of SI. See also .7.     ", "videoStruct": [{"timeStart": "00-00-01", "timeEnd": "00-00-04", "sentence": "and thank you very much for you to visit"}, {"timeStart": "00-00-04", "timeEnd": "00-00-07", "sentence": "I'll give you a brief overview of a paper which"}, {"timeStart": "00-00-07", "timeEnd": "00-00-11", "sentence": "focuses on understanding whether physician with it still continued"}, {"timeStart": "00-00-11", "timeEnd": "00-00-13", "sentence": "as little bit of background"}, {"timeStart": "00-00-13", "timeEnd": "00-00-18", "sentence": "it's important to know that regulation methods are one of the three main approaches to continue learn"}, {"timeStart": "00-00-18", "timeEnd": "00-00-20", "sentence": "and"}, {"timeStart": "00-00-20", "timeEnd": "00-00-22", "sentence": "it had the only option basically"}, {"timeStart": "00-00-22", "timeEnd": "00-00-27", "sentence": "if architecture is fixed and if you don't allow replace and replace any way a kind of a cheat"}, {"timeStart": "00-00-28", "timeEnd": "00-00-34", "sentence": "there have been three main I rhythms proposed in this domain you w see m a s and s I"}, {"timeStart": "00-00-34", "timeEnd": "00-00-41", "sentence": "there has been quite a bit of follow up work at its basically old centered around these three ideas combining them for extending them"}, {"timeStart": "00-00-41", "timeEnd": "00-00-44", "sentence": "so we focused on these three methods"}, {"timeStart": "00-00-44", "timeEnd": "00-00-49", "sentence": "and quite surprisingly we found that all of them are quite closely related"}, {"timeStart": "00-00-49", "timeEnd": "00-00-53", "sentence": "so even though they look very different from the outside"}, {"timeStart": "00-00-53", "timeEnd": "00-00-56", "sentence": "all approximated version of the Fisher information"}, {"timeStart": "00-00-56", "timeEnd": "00-00-59", "sentence": "and effort quite closely related"}, {"timeStart": "00-00-59", "timeEnd": "00-01-08", "sentence": "if you are familiar with the openness eyes might also be interesting for you to know that as I smiles and the bias is actually a reason why is I work so well"}, {"timeStart": "00-01-09", "timeEnd": "00-01-12", "sentence": "and now you might wonder how I came up with these claims"}, {"timeStart": "00-01-12", "timeEnd": "00-01-15", "sentence": "we do mathematical derivations"}, {"timeStart": "00-01-15", "timeEnd": "00-01-17", "sentence": "to so that these items are close to it"}, {"timeStart": "00-01-17", "timeEnd": "00-01-22", "sentence": "but we also carefully and period aladin one standard engine x in settings"}, {"timeStart": "00-01-22", "timeEnd": "00-01-29", "sentence": "on one hand this is a nice result because it unifies two regulation approaches with have been proposed so far"}, {"timeStart": "00-01-29", "timeEnd": "00-01-31", "sentence": "and it also explains why some of them work"}, {"timeStart": "00-01-31", "timeEnd": "00-01-33", "sentence": "which was not really known before"}, {"timeStart": "00-01-33", "timeEnd": "00-01-37", "sentence": "and on a practical level understanding is algorithm"}, {"timeStart": "00-01-37", "timeEnd": "00-01-42", "sentence": "she is also a some technical insights which are important if you actually want to work with these items"}, {"timeStart": "00-01-42", "timeEnd": "00-01-46", "sentence": "so if you want to know more details or prob nite cleanse little feather"}, {"timeStart": "00-01-46", "timeEnd": "00-01-48", "sentence": "please do come to my poster session"}]}]