<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/xjw/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-18">18 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,151.19,178.57,60.88,8.96"><forename type="first">Huanrui</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020)</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,221.04,178.57,68.93,8.96"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020)</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,298.94,178.57,69.48,8.96"><forename type="first">Hongliang</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020)</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,377.38,178.57,79.44,8.96"><forename type="first">Nathan</forename><surname>Inkawhich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,142.86,190.19,73.70,8.96"><forename type="first">Andrew</forename><surname>Gardner</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Radiance Technologies 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,225.53,190.19,70.57,8.96"><forename type="first">Andrew</forename><surname>Touchet</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Radiance Technologies 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,305.07,190.19,61.34,8.96"><forename type="first">Wesley</forename><surname>Wilkes</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Radiance Technologies 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,375.38,190.19,53.40,8.96"><forename type="first">Heath</forename><surname>Berry</surname></persName>
							<email>heath.berry@radiancetech.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Radiance Technologies 1</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0" coords="1,437.75,190.19,27.41,8.96"><forename type="first">Hai</forename><surname>Li</surname></persName>
							<email>hai.li@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-18">18 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2009.14720v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2-SNAPSHOT" ident="GROBID" when="2021-01-15T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research finds CNN models for image classification demonstrate overlapped adversarial vulnerabilities: adversarial attacks can mislead CNN models with small perturbations, which can effectively transfer between different models trained on the same dataset. Adversarial training, as a general robustness improvement technique, eliminates the vulnerability in a single model by forcing it to learn robust features. The process is hard, often requires models with large capacity, and suffers from significant loss on clean data accuracy. Alternatively, ensemble methods are proposed to induce sub-models with diverse outputs against a transfer adversarial example, making the ensemble robust against transfer attacks even if each sub-model is individually non-robust. Only small clean accuracy drop is observed in the process. However, previous ensemble training methods are not efficacious in inducing such diversity and thus ineffective on reaching robust ensemble. We propose DVERGE, which isolates the adversarial vulnerability in each sub-model by distilling non-robust features, and diversifies the adversarial vulnerability to induce diverse outputs against a transfer attack. The novel diversity metric and training procedure enables DVERGE to achieve higher robustness against transfer attacks comparing to previous ensemble methods, and enables the improved robustness when more sub-models are added to the ensemble. The code of this work is available at https://github.com/zjysteven/DVERGE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent discoveries of adversarial attacks cast doubt on the inherent robustness of convolutional neural networks (CNNs) <ref type="bibr" coords="1,211.43,566.15,10.91,8.64" target="#b0">[1,</ref><ref type="bibr" coords="1,225.24,566.15,7.52,8.64" target="#b1">2,</ref><ref type="bibr" coords="1,235.66,566.15,7.27,8.64" target="#b2">3]</ref>. These attacks, commonly referred to as adversarial examples, comprise precisely crafted input perturbations that are often imperceptible to humans yet consistently induce misclassification in CNN models. Moreover, previous research has demonstrated widespread transferability of adversarial examples, wherein adversarial examples generated against an arbitrary model can reliably mislead other unspecified deep learning models trained with the same dataset <ref type="bibr" coords="1,494.46,609.78,10.78,8.64" target="#b3">[4,</ref><ref type="bibr" coords="1,108.00,620.69,7.42,8.64" target="#b4">5,</ref><ref type="bibr" coords="1,117.92,620.69,7.12,8.64" target="#b5">6]</ref>. Ilyas et al. <ref type="bibr" coords="1,174.70,620.69,11.49,8.64" target="#b4">[5]</ref> conjecture the existence of robust and non-robust features within standard image classification datasets. Whereas humans may understand an image via "human-meaningful" robust features, which usually are insensitive to small additive noise, deep learning models are more prone to learning non-robust features. Non-robust features are highly correlated with output labels and help improve clean accuracy but are not visually meaningful and are sensitive to noise. Such dependency on non-robust features leads to adversarial vulnerability that is exploited by adversarial examples to mislead CNN models. Moreover, Ilyas et al. empirically show that CNN models independently <ref type="figure" coords="2,117.90,205.32,32.41,8.64">Figure 1</ref>: Decision regions in the ∞ ball around the same testing image learned by ensembles of 3 ResNet-20 models trained on CIFAR-10 dataset. Same color indicates the same predicted label. The vertical axis is along the adversarial direction of a surrogate benign ensemble, and the horizontal axis is along a random Rademacher vector. The same axes are used for each subplot. Adversarial vulnerability can be inferred from the closest decision boundary and corresponding class. The baseline ensemble is achieved via standard training on clean data while the bottom ensemble is trained with DVERGE. More plots of this nature can be seen in Appendix C.1.</p><p>trained on the same dataset tend to capture similar non-robust features, demonstrating overlapping vulnerability <ref type="bibr" coords="2,162.74,305.49,10.72,8.64" target="#b4">[5]</ref>. This property can be observed from the example in the upper row of <ref type="figure" coords="2,468.04,305.49,33.39,8.64">Figure 1</ref>, where an ensemble is trained on clean data and each of its sub-models are vulnerable along the same axis of a transfer attack. This similarity is key to the high transferability of adversarial attacks <ref type="bibr" coords="2,481.65,327.31,10.75,8.64" target="#b4">[5,</ref><ref type="bibr" coords="2,494.90,327.31,7.17,8.64" target="#b6">7]</ref>.</p><p>Extensive research has been conducted to improve the robustness of CNN models against adversarial attacks, most notably adversarial training <ref type="bibr" coords="2,282.37,354.60,10.72,8.64" target="#b2">[3]</ref>. Adversarial training minimizes the loss of a CNN model on online-generated adversarial examples against itself at each training step. This process forces the model to prefer robust to non-robust features and thereby largely eliminates the model's vulnerability. Nevertheless, learning robust features is hard, so adversarial training often leads to a significant increase in the generalization error on clean testing data <ref type="bibr" coords="2,376.93,398.24,10.58,8.64" target="#b7">[8]</ref>.</p><p>Similar to traditional ensemble methods like bagging <ref type="bibr" coords="2,319.41,414.63,11.54,8.64" target="#b8">[9]</ref> and boosting <ref type="bibr" coords="2,386.52,414.63,15.18,8.64" target="#b9">[10]</ref>, which train an ensemble of weak learners with diverse predictions to improve overall accuracy, a recent line of research proposes to train an ensemble of individually non-robust sub-models that produce diverse outputs against transferred adversarial examples <ref type="bibr" coords="2,273.89,447.36,15.88,8.64" target="#b10">[11,</ref><ref type="bibr" coords="2,292.40,447.36,12.50,8.64" target="#b11">12,</ref><ref type="bibr" coords="2,307.52,447.36,11.92,8.64" target="#b12">13]</ref>. Intuitively, the approach can defend against black-box transfer attacks as an attack can succeed only when multiple sub-models converge towards the same wrong prediction <ref type="bibr" coords="2,220.51,469.17,15.42,8.64" target="#b12">[13]</ref>. Such an ensemble could also hypothetically achieve high clean accuracy since the training process doesn't exclude non-robust features. Various ensemble training methods have been explored, such as diversifying output logits' distributions <ref type="bibr" coords="2,413.74,490.99,15.72,8.64" target="#b10">[11,</ref><ref type="bibr" coords="2,431.95,490.99,13.25,8.64" target="#b11">12]</ref> or minimizing the cosine similarity between the input gradient direction of each sub-model <ref type="bibr" coords="2,425.02,501.90,15.42,8.64" target="#b12">[13]</ref>. Yet empirical results show that these diversity metrics are not very effective at inducing output diversity among sub-models, and thus the corresponding ensemble can hardly attain the desired robustness <ref type="bibr" coords="2,468.34,523.72,15.27,8.64" target="#b13">[14]</ref>.</p><p>We note that black-box transfer attacks are prevalent in real-world applications where model parameters are not exposed to end users <ref type="bibr" coords="2,264.30,551.02,10.90,8.64" target="#b5">[6,</ref><ref type="bibr" coords="2,277.96,551.02,11.91,8.64" target="#b12">13]</ref>. Moreover, high clean accuracy is always desirable. We therefore seek an effective training method that mitigates attack transferability while maintaining high clean accuracy. Based on a close investigation of the cause of adversarial vulnerability in sub-models, we propose to distill the features learned by each sub-model corresponding to its vulnerability to adversarial examples and use the overlap between the distilled features to measure the diversity between sub-models. As adversarial examples exploit the vulnerability of sub-models, a small overlap between sub-models indicates that a successful adversarial example on one sub-model is unlikely to fool the other sub-model. Consequently, our method impedes attack transferability between sub-models and leads to diverse outputs against a transferred adversarial example. Based on this diversity metric, we propose Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles (DVERGE), which uses a round-robin training procedure to distill and diversify the features corresponding to each sub-model's vulnerability. The proposed ensemble training method makes the following contributions:</p><p>• DVERGE can successfully isolate and diversify the vulnerability in each sub-model such that within-ensemble attack transferability is nearly eliminated;</p><p>• DVERGE can significantly improve the overall robustness of the ensemble against black-box transfer attacks without significantly impacting the clean accuracy; • The diversity induced by DVERGE consistently improves robustness as the number of ensemble sub-models increases under equivalent evaluation conditions.</p><p>As shown in the bottom row of <ref type="figure" coords="3,232.40,132.05,31.90,8.64">Figure 1</ref>, diverse vulnerabilities allowed to persist in each sub-model for high clean accuracy by DVERGE combine to yield an ensemble robust to transfer attacks. Our method can also be augmented with the adversarial training objective to yield an ensemble with both satisfying white-box robustness and higher clean accuracy compared to exclusively adversarial training. To the best of our knowledge, this work is the first to utilize distilled features for training diverse ensembles and quantitatively relate it to the robustness against adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Adversarial attack and defense. The pervasiveness of adversarial examples highlights the vulnerability of modern CNN systems to malicious inputs. An adversarial attack usually applies an additive perturbation δ subject to some constraint S to an original input x to form the adversarial example x adv = x + δ. The goal of the attack is to find δ so that x adv can maximize the loss L θ of some CNN model with parameters θ w.r.t. x's true label y. The attacker's objective can be formulated as</p><formula xml:id="formula_0" coords="3,165.36,292.03,152.80,10.59">x adv = x + argmax δ∈S L θ (x + δ, y).</formula><p>The constraint S typically ensures adversarial examples are visually indistinguishable from original inputs, which is often defined as ||δ|| p ≤ for some perturbation strength and p -norm, e.g. p = 0 <ref type="bibr" coords="3,341.29,314.17,15.42,8.64" target="#b14">[15]</ref>, p = 2 <ref type="bibr" coords="3,390.04,314.17,10.72,8.64" target="#b1">[2]</ref>, or p = ∞ <ref type="bibr" coords="3,449.95,314.17,10.91,8.64" target="#b0">[1,</ref><ref type="bibr" coords="3,463.55,314.17,7.27,8.64" target="#b2">3]</ref>. In this work, we focus on the attack bounded by the ∞ norm, which has become increasingly common in recent attack and defense research studies. Madry et al. <ref type="bibr" coords="3,330.11,335.98,11.60,8.64" target="#b2">[3]</ref> show that the attacker's objective can be effectively optimized in a multi-step projected gradient descent (PGD) manner, where in each step of the gradient update the achieved adversarial example is projected back into the constraint set S to make sure it complies with the ∞ norm constraint. The attack can be further strengthened via application of a random starting point <ref type="bibr" coords="3,260.76,379.62,11.62,8.64" target="#b2">[3]</ref> or consideration of the gradient's momentum information during the optimization <ref type="bibr" coords="3,204.03,390.53,15.77,8.64" target="#b15">[16,</ref><ref type="bibr" coords="3,222.29,390.53,11.83,8.64" target="#b16">17]</ref>.</p><p>Various empirical methods have been investigated for improving model robustness. Among theses methods, adversarial training <ref type="bibr" coords="3,247.40,417.83,11.75,8.64" target="#b2">[3]</ref> has gained prominence for its reliability and effectiveness. Adversarial training generates adversarial examples while concurrently training CNN model(s) to minimize the loss on these adversarial examples. The objective of adversarial training is formulated as a min-max optimization:</p><formula xml:id="formula_1" coords="3,223.25,445.33,152.45,15.61">min θ E (x,y)∼D [max δ∈S L θ (x + δ, y)]</formula><p>, where the inner maximization is often conducted with PGD attacks for greater robustness <ref type="bibr" coords="3,353.85,461.46,10.72,8.64" target="#b2">[3]</ref>. Although recent research shows that PGD adversarial training encourages a model to capture robust features within datasets <ref type="bibr" coords="3,475.16,472.37,10.60,8.64" target="#b4">[5]</ref>, the process is difficult and costly. The learning of robust feature detrimentally and significantly affects the accuracy of the model on clean data <ref type="bibr" coords="3,270.53,494.19,10.69,8.64" target="#b7">[8]</ref>, and the model architecture needs to be much larger in order to compensate for the added complexity of the objective <ref type="bibr" coords="3,357.57,505.10,10.58,8.64" target="#b2">[3]</ref>.</p><p>Ensemble for improved accuracy and uncertainty measurement. Traditionally, ensemble learning methods have been extensively studied to improve the performance of the model and tackle out-of-distribution uncertainty and generalization. With some early success on performance improvement with neural network ensemble <ref type="bibr" coords="3,250.63,560.79,15.13,8.64" target="#b17">[18]</ref>, bagging <ref type="bibr" coords="3,306.01,560.79,11.49,8.64" target="#b8">[9]</ref> and boosting <ref type="bibr" coords="3,372.75,560.79,15.13,8.64" target="#b9">[10]</ref>, Kuncheva et al. <ref type="bibr" coords="3,457.14,560.79,16.47,8.64" target="#b18">[19]</ref> make a thorough evaluation on the relationship between sub-model diversity and ensemble accuracy, and find that as a higher diversity generally improves ensemble accuracy, specially designed diversity metric and training algorithm are needed to induce a stronger relationship between the two. Later on it is also observed that deep neural network ensembles can be used for uncertainty estimation <ref type="bibr" coords="3,486.04,604.42,15.36,8.64" target="#b19">[20]</ref>, where the average predicted probability estimated from the ensemble outputs can lead to a wellcalibrated uncertainty estimation. Recent advances in the field include diversifying the sub-models with varitional information bottleneck and diversity-inducing adversarial loss to further improve the ensemble accuracy <ref type="bibr" coords="3,187.61,648.06,15.41,8.64" target="#b20">[21]</ref>, and resolving the scalability issue of deep ensembles or Bayesian neural networks by reducing the memory and computation cost with shared-weight across sub-models and rank-1 parameterization <ref type="bibr" coords="3,227.28,669.88,15.89,8.64" target="#b21">[22,</ref><ref type="bibr" coords="3,246.43,669.88,11.92,8.64" target="#b22">23]</ref>. As the contribution of our work is mainly addressing the adversarial robustness issues, our training algorithm is orthogonal to these previous works. Yet we believe it would be a promising direction to combine our algorithm with other newly-proposed ensemble training methods, leading to a robust ensemble with higher accuracy and less memory and computation cost.</p><p>Ensemble of diverse sub-models for robustness. Given the success of ensemble methods, a recent line of work investigates improving the robustness of an ensemble of small sub-models (especially against transfer adversarial attacks). Such robust ensembles can be obtained not only by combining individually robust sub-models but also by eliminating adversarial vulnerabilities shared by different sub-models, be they robust or non-robust, so that attacks cannot transfer between the sub-models within the ensemble. Several works attempt to promote diversity in internal representations or outputs across sub-models to serve as a mechanism to limit adversarial transferability and improve ensemble robustness. Pang et al. <ref type="bibr" coords="4,199.41,151.84,16.58,8.64" target="#b11">[12]</ref> propose the ADP regularizer, which forces different sub-models to have high diversity in the non-maximal predictions. Kariyappa et al. <ref type="bibr" coords="4,371.70,162.75,16.73,8.64" target="#b12">[13]</ref> reduce the overlap between "Adversarial Subspaces" <ref type="bibr" coords="4,209.67,173.66,16.73,8.64" target="#b23">[24]</ref> of different sub-models by maximizing the cosine distance between each sub-model's gradient w.r.t. the input. Although the ideas behind these methods are intuitive for improving sub-model diversity, these diversity metrics do not in practice align well with diversifying the adversarial vulnerability shared by different sub-models. Thus training the ensemble with these diversity metrics does not lead to satisfying robustness against transferability between sub-models, and consequently the resulted ensemble is still highly non-robust <ref type="bibr" coords="4,365.99,228.20,15.18,8.64" target="#b13">[14]</ref>. An ensemble diversity metric that can effectively lead to low attack transferability and high overall robustness is still lacking.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vulnerability diversity metric</head><p>A recent study <ref type="bibr" coords="4,171.63,310.12,11.75,8.64" target="#b4">[5]</ref> reveals that non-robust features captured by deep learning models are highly sensitive to additive noise, which is the main cause of adversarial vulnerability in CNN models. Based on this observation, we propose to isolate the vulnerability of CNN models based on their distilled non-robust features. Let us take a CNN model f i trained on dataset D as an example. We consider a target input-label pair (x, y) ∈ D and another randomly-chosen independent source pair (x s , y s ) ∈ D. Corresponding to the source image x s , the distilled feature of the input image x by the l-th layer of f i can be approximated with the feature distillation objective as <ref type="bibr" coords="4,414.39,375.58,10.79,8.64" target="#b4">[5]</ref>:</p><formula xml:id="formula_2" coords="4,182.06,390.61,321.94,20.52">x f l i (x, x s ) = argmin z f l i (z) − f l i (x) 2 2 , s.t. ||z − x s || ∞ ≤ ,<label>(1)</label></formula><p>where f l i (•) denotes the output before the activation (e.g. ReLU) of the l-th hidden layer. This constrained optimization objective can then be optimized with PGD <ref type="bibr" coords="4,391.66,430.31,10.72,8.64" target="#b2">[3]</ref>. The distilled feature is expected to be visually similar to x s rather than x but classified as the target class y since the same feature will be extracted from x f l i and x by f i . Such misalignment between the visual similarity and the classification result shows that x f l i reflects the adversarial vulnerability of f i when classifying x. Therefore, we define the vulnerability diversity between two models f i and f j as:</p><formula xml:id="formula_3" coords="4,154.71,496.36,349.29,22.31">d(f i , f j ) := 1 2 E (x,y),(xs,ys),l L fi x f l j (x, x s ), y + L fj x f l i (x, x s ), y .<label>(2)</label></formula><p>Here L f (x, y) denotes the cross-entropy loss of model f for an input-label pair (x, y). The expectation is taken over the independent uniformly random choices of (x, y) ∈ D, (x s , y s ) ∈ D, and layer l of models f i and f j . Since the distilled feature has the same dimension as input images, this formulation can be evaluated on models with arbitrary architectures trained on the same dataset. As</p><formula xml:id="formula_4" coords="4,108.00,567.70,340.67,13.88">x f l i (x, x s ) is visually uncorrelated with y, the cross entropy loss L fj (x f l i (x, x s ), y)</formula><p>is small only if f j 's vulnerability on x's non-robust features overlaps with that of f i , and vice versa. So the formulation in Equation (2) effectively measures the vulnerability overlap between the two models. Note that the feature distillation process in Equation (1) can be considered a special case of generating an adversarial example from source image x s with target label y. The diversity defined in Equation (2) therefore corresponds to the attack success rate when transferring adversarial examples between the two models in the same way as training cross-entropy loss corresponds to training accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vulnerability diversification objective</head><p>As adversarial attacks are less likely to transfer between models with high vulnerability diversity, we propose to apply the metric defined in Equation <ref type="formula" coords="4,314.06,691.70,11.63,8.64" target="#formula_3">2</ref>as an objective during the ensemble training to induce diverse sub-models and block transfer attacks. Equation <ref type="formula" coords="4,374.83,702.61,11.72,8.64">3</ref>shows a straightforward way to incorporate the diversity metric into the training objective, where for each sub-model f i , the diversity between itself and all other sub-models f j in the ensemble is maximized when minimizing the original cross-entropy loss:</p><formula xml:id="formula_5" coords="5,225.64,98.03,160.72,25.04">min fi E (x,y) [L fi (x, y)] − α j =i d(f i , f j ).</formula><p>(3)</p><p>As the formulation of d(f i , f j ) has no upper bound, directly maximizing it may ultimately lead to divergence. Thus, we revise the training objective as</p><formula xml:id="formula_6" coords="5,171.28,153.09,332.72,33.76">min fi E (x,y)   L fi (x, y) + α j =i E (xs,ys),l L fi x f l j (x, x s ), y s   ,<label>(4)</label></formula><p>which has a stronger bound than Equation <ref type="formula" coords="5,274.76,193.47,10.37,8.64">3</ref>. The new objective not only encourages the increase of vulnerability diversity but also facilitates the correct classification of the distilled image as y s . As such, the objective is well-posed and can be effectively optimized.</p><p>Furthermore, it should be noted that minimizing</p><formula xml:id="formula_7" coords="5,108.00,231.36,396.00,29.45">L fi (x f l j (x, x s ), y s ) can effectively contribute to the minimization of L fi (x s , y s ) as the distilled image x f l j (x, x s )</formula><p>is close to the clean image x s . Previous adversarial training research <ref type="bibr" coords="5,221.26,262.08,10.69,8.64" target="#b2">[3,</ref><ref type="bibr" coords="5,234.44,262.08,13.22,8.64" target="#b24">25]</ref> also show that it is not necessary to include the clean data loss in the objective. So we further simplify Equation <ref type="formula" coords="5,295.26,272.99,11.62,8.64" target="#formula_6">4</ref>to</p><formula xml:id="formula_8" coords="5,219.04,284.63,284.96,25.04">min fi E (x,y),(xs,ys),l j =i L fi (x f l j (x, x s ), y s ),<label>(5)</label></formula><p>which is adopted for training individual sub-models in DVERGE. The objective in Equation <ref type="formula" coords="5,476.12,315.18,11.59,8.64" target="#formula_8">5</ref>can be understood as training sub-model f i with the adversarial examples generated for other sub-models. However, DVERGE is fundamentally different from adversarial training. Adversarial training process constantly trains a model on white-box attacks against itself and forces the model to capture the robust feature of the dataset. In DVERGE, Equation (5) can be minimized if f i utilizes a different set of features from other sub-models, including non-robust features. As non-robust features are distributed more commonly in dataset than robust features <ref type="bibr" coords="5,343.76,380.64,10.63,8.64" target="#b4">[5]</ref>, capturing and integrating some nonrobust features allows DVERGE to reach higher clean accuracy compared to adversarial training. Our training process should also be distinguished from that of Tramer et al. <ref type="bibr" coords="5,398.28,402.45,15.41,8.64" target="#b25">[26]</ref>, which trains a single model with adversarial examples transferred from an ensemble of static pretrained sub-models for improving robustness. In DVERGE, all the sub-models in the ensemble are being optimized with Equation (5) in a round-robin fashion. The procedure dynamically maximizes the diversity of every pair of sub-models, rather than forcing only a single model away from static pretrained sub-models. The entire training process of DVERGE is elaborated in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DVERGE training routine</head><p>Algorithm 1 shows the pseudo-code for training an ensemble of N sub-models. We first randomly initialize and pre-train all the sub-models based on the clean dataset so that their feature spaces will be useful and we do not waste time diversifying irrelevant features. Though we show in Appendix C.6 that training DVERGE from scratch can also lead to better results than other methods. Then for each batch of training data during the diverse training phase, we randomly sample another batch of source data and use them to distill the non-robust feature following the objective of Equation (1). A PGD optimization scheme is applied in the feature distillation process. Round-robin training is then employed wherein a single stochastic gradient descent step is performed on each sub-model with the distilled images from all other sub-models and their source labels, as stated in the objective in Equation <ref type="bibr" coords="5,145.93,598.94,10.42,8.64" target="#b4">(5)</ref>. This training process is performed on all B batches of training data and repeated for E epochs. The layer l for the feature distillation is randomly chosen in each epoch to avoid overfitting to the features of a particular layer. An ablation study along with discussion on this choice is given in Appendix C.5. This training routine can effectively increase the vulnerability diversity between each pair of sub-models within the ensemble and block within-ensemble transfer attacks. Consequently, the overall black-box robustness of the ensemble improves.</p><p>DVERGE induces a similar training complexity as adversarial training does. Both of these methods need extra back propagations to either distill non-robust features or find adversarial examples. However, DVERGE uses only intermediate features rather than final outputs for distillation so it is marginally faster than adversarial training. Detailed comparison of the training complexity of DVERGE vs. previous methods can be found in Appendix A.</p><p>Algorithm 1 DVERGE training routine for a N -sub-model ensemble.</p><p>1: # initialization and pretraining 2:</p><formula xml:id="formula_9" coords="6,111.78,99.84,85.50,18.09">for i = 1, . . . , N do 3:</formula><p>Randomly initialize sub-model fi 4:</p><p>Pretrain fi with clean dataset 5: # round-robin feature diversification 6: for e = 1, . . . , E do 7:</p><p>Uniformly randomly choose layer l for feature distillation 8:</p><p>for b = 1, . . . , B do 9:</p><p>(X, Y ) ← get batched input-label pairs 10:</p><p>(Xs, Ys) ← uniformly sample batched source input-label pairs 11:</p><p># get distilled batch for each model 12:</p><p>for i = 1, . . . , N do 13:</p><formula xml:id="formula_10" coords="6,164.29,211.24,37.74,12.74">X i := x f l i</formula><p>(X, Xs) ← non-robust feature distillation with Equation <ref type="formula" coords="6,409.17,211.52,10.45,7.77" target="#formula_2">1</ref>14: # calculate loss and perform SGD update for all sub-models 15:</p><p>for i = 1, . . . , N do 16:</p><formula xml:id="formula_11" coords="6,108.00,246.77,182.28,57.42">∇ f i ← ∇[ j =i L f i (fi(X j ), Ys)] 17: fi ← fi − lr • ∇ f i 4 Experimental results</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We compare DVERGE with various counterparts, including Baseline which trains an ensemble in a standard way and two previous robust ensemble training methods: ADP <ref type="bibr" coords="6,402.17,350.65,16.73,8.64" target="#b11">[12]</ref> and GAL <ref type="bibr" coords="6,460.45,350.65,15.42,8.64" target="#b12">[13]</ref>. For a fair comparison, we use ResNet-20 <ref type="bibr" coords="6,249.63,361.56,16.55,8.64" target="#b26">[27]</ref> as sub-models and average the output probabilities after the soft-max layer of each sub-model to yield the final predictions of ensembles. All the evaluations are performed on the CIFAR-10 dataset <ref type="bibr" coords="6,250.19,383.38,15.13,8.64" target="#b27">[28]</ref>. Training configuration details can be found in Appendix A. For DVERGE, we use PGD with momentum <ref type="bibr" coords="6,289.33,394.29,16.59,8.64" target="#b28">[29]</ref> to perform the feature distillation in Equation <ref type="formula" coords="6,491.65,394.29,10.57,8.64" target="#formula_2">1</ref>. We conduct 10 steps of gradient descent during feature distillation with a step size of /10. The used for each ensemble size to achieve the results in this section was empirically chosen for the highest diversity and lowest transferability, such that = 0.07, 0.05, 0.05 for ensembles with 3, 5, and 8 sub-models, respectively. Analysis on the effect of is given in Appendix B. Codes for reproducing the experiments are available at https://github.com/zjysteven/DVERGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Diversity and transferability within the ensemble</head><p>The objective of DVERGE is to guide sub-models to capture diverse non-robust features and minimize the vulnerability overlap between sub-models, thereby reducing the attack transferability within the ensemble. To validate our method, we measure diversity and transferability by randomly picking 1,000 test samples on which all sub-models initially give correct predictions. We compute the pair-wise diversity as the expected cross-entropy loss formulated in Equation <ref type="formula" coords="6,422.97,538.92,10.79,8.64" target="#formula_3">2</ref>, which is further averaged across all pairs of sub-models to obtain the diversity measurement of the whole ensemble. When measuring the transferability, we generate untargeted adversarial examples using 50-step PGD with a step size of /5 and five random starts. The transferability is measured by the attack success rate which counts any misclassification as a success. Similar to diversity, the averaged pair-wise attack success rate is used to indicate the level of transferability within the ensemble.</p><p>First, let's take a look at how the diversity and transferability within the ensemble changes during the training process of DVERGE. <ref type="figure" coords="6,249.33,620.76,34.62,8.64">Figure 2</ref> shows the result of an ensemble of three sub-models. The diversity is evaluated using the same of 0.07 as during the training, and the transferability is measured using the standard of 0.03 (≈ 8/255) for adversarial attacks on CIFAR-10 <ref type="bibr" coords="6,446.59,642.58,10.44,8.64" target="#b2">[3]</ref>. The figure clearly shows that the diversity increases while the transferability decreases as the training proceeds. This trend empirically validates that minimizing the DVERGE objective can effectively lead to a higher diversity and a lower adversarial transferability within an ensemble.  intersection of the i-th row and j-th column represents the transfer success rate of the adversarial examples generated from the i-th sub-model and tested on the j-th sub-model. When i = j, the number becomes the white-box attack success rate. Larger off-diagonal numbers indicate greater transferability across sub-models. Compared with other ensemble methods, DVERGE suppresses the transferability to a much lower level; among all adversarial examples that successfully break one sub-model, only 3-6% of them could lead to misclassification on other sub-models. Although ADP and GAL also strive to improve the diversity for better robustness, they cannot effectively block adversarial transfer. ADP exhibits transference of 60% to 70% of attacks between sub-models. When it comes to GAL, two out of the three sub-models are still extremely vulnerable to each other, where more than 80% of adversarial examples can successfully transfer between the first and second sub-models. Our evaluation demonstrates that stopping attack transfers is not trivial and applying an appropriate diversification metric is crucial. Therefore, we advocate the use of DVERGE as a more effective means for mitigating the attack transferability within an ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness of the ensemble</head><p>We evaluate the robustness of ensembles under two threat models: black-box transfer adversary, where the attackers cannot access the model parameters and rely on surrogate models to generate transferable adversarial examples, and white-box adversary, where the attackers have the full access of everything of the model. Under the black-box scenario, we use hold-out baseline ensembles with  The results under this setting can be seen in Appendix C.3. We use three attack methodologies:</p><p>(1) PGD with momentum <ref type="bibr" coords="8,214.52,264.58,16.73,8.64" target="#b28">[29]</ref> with three random starts. (2) M-DI 2 -FGSM <ref type="bibr" coords="8,416.11,264.58,15.41,8.64" target="#b29">[30]</ref>, which randomly resizes and pads the image in each step of attack generation. (3) SGM <ref type="bibr" coords="8,384.61,275.49,15.14,8.64" target="#b30">[31]</ref>, which adds weight to the gradient through the skip connections of ResNets. The latter two attacks are essentially two stronger black-box transfer attacks that can better expose the attack transferability between models. For more details, we refer the reader to the attacks' respective papers. We run each attack for 100 iterations with the step size of /5. Other than using the cross-entropy loss, we also generate adversarial examples with CW loss <ref type="bibr" coords="8,164.55,330.03,11.65,8.64" target="#b1">[2]</ref> since it can also help with the transfer. As a result, in total, each sample will have 3 (surrogate models)×5 (PGD with 3 random starts plus 2 other attacks)×2 (loss functions) = 30 adversarial counterparts. The black-box accuracy is reported in a all-or-nothing fashion: We say the model is accurate on one sample only if all of its 30 adversarial versions are correctly classified by the model. We adopt such a powerful adversary and a strict criteria to give a tighter upper bound of the robustness against black-box transfer attacks. Under the white-box scenario, we use 50-step PGD with five random starts and the step size of /5 to attack ensembles. Our results in Appendix D prove that we have applied sufficient steps for attacks to converge.</p><p>Evaluated on 1,000 randomly selected test samples, <ref type="figure" coords="8,321.94,422.78,34.38,8.64" target="#fig_2">Figure 4</ref> shows the black-box and white-box robustness of ensembles with various number of sub-models across a wide range of attack budget .</p><p>Here we show the averaged results over three independent runs. We refer the reader to Appendix C.3 for the plots with error bars and numerical results. DVERGE, even with the least sub-models, outperforms each case of the other methods with higher accuracy in both black-box and whitebox settings and achieves comparable clean accuracy. In addition, robustness improvement can be easily obtained by adding more sub-models into the ensemble when using our method, while such a satisfying trend is less obvious in other methods. GAL, as the second best performing approach among the four methods, actually shares the same high-level concept as the proposed DVERGE algorithm. They both aim at diversifying the vulnerabilities shared by the sub-models. The difference lies in the fact that GAL considers using the adversarial gradient directions to evaluate the vulnerability of CNN models whereas DVERGE identifies the vulnerability in a model by distilling the learnt non-robust features. Results from both <ref type="figure" coords="8,268.37,553.69,34.26,8.64" target="#fig_1">Figure 3</ref> and <ref type="figure" coords="8,322.58,553.69,34.26,8.64" target="#fig_2">Figure 4</ref> suggest that our approach is a more effective realization of the intuition of identifying and diversifying adversarial vulnerability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">DVERGE with adversarial training</head><p>Although DVERGE achieves the highest robustness among ensemble methods, its robustness against white-box attacks and transfer attacks with a large perturbation strength is still quite low. This result is expected because the objective of DVERGE is to diversify the adversarial vulnerability rather than completely eliminate it. In other words, vulnerability inevitably exists within sub-models and can be captured by attacks with larger . One straightforward way to improve the robustness of ensembles is to augment DVERGE with adversarial training <ref type="bibr" coords="8,300.60,664.40,10.71,8.64" target="#b2">[3]</ref>. We describe implementation details regarding adversarial training in Appendix A and the amended objective in Appendix C.4. <ref type="figure" coords="8,108.00,691.70,33.77,8.64" target="#fig_3">Figure 5</ref> presents the black-box and white-box accuracy for adversarial training (AdvT), DVERGE only (DVERGE) and the combination of the two (DVERGE+AdvT) using the same evaluation setting as in Section 4.3. Ensembles with 5 sub-models are used here, and the results are averaged over three independent runs. More results under different ensemble sizes and plots with error bars can be found in Appendix C.4. The DVERGE+AdvT objective favors the capture of more robust features by the ensemble. Compared to AdvT, DVERGE+AdvT encourages the ensemble to learn diverse non-robust features alongside robust features, leading to a higher clean accuracy and higher robustness against transfer attacks. In the meantime, no matter which objective is applied, the overall learning capacity of the ensemble remains the same. That is, learning more robust features will leave less capacity in the ensemble to capture diverse non-robust features, and vice versa. Forcing the inclusion of robust features causes DVERGE+AdvT to sacrifice the accuracy on clean examples comparing to performing DVERGE only. Learning diverse non-robust features harms DVERGE+AdvT's robustness against white-box attacks with larger perturbations compared to AdvT alone. These results can be seen as an evidence for the recent findings in <ref type="bibr" coords="9,273.70,184.57,10.91,8.64" target="#b4">[5,</ref><ref type="bibr" coords="9,287.36,184.57,8.36,8.64" target="#b7">8]</ref> regarding the tradeoff between clean accuracy and robustness. DVERGE+AdvT can effectively explore such tradeoff by changing the ratio between the two objectives, which is further illustrated in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work we propose DVERGE, a CNN ensemble training method that isolates and diversifies the adversarial vulnerability in each sub-model to improve the overall robustness against transfer attacks without significantly reducing clean accuracy. We show that adversarial diversity in a CNN model can be successfully characterized by distilled non-robust features, from which we can measure the vulnerability diversity between two models. The diversity metric is further developed into the vulnerability diversification objective used for DVERGE ensemble training. We empirically show that training with DVERGE objective can effectively increase the vulnerability diversity between sub-models, thereby blocking attack transferability within the ensemble. In this way DVERGE reduces the success rate of transfer attacks between sub-models from more than 60% achieved by previous ensemble training methods to less than 6%, which enables ensembles trained with DVERGE to achieve significantly higher robustness against both black-box transfer attacks and white-box attacks compared to previous ensemble training methods. The robustness can be further improved with additional sub-models in the ensemble. We further demonstrate that DVERGE can be augmented with an adversarial training objective, which enables the ensemble to achieve higher clean accuracy and higher transfer attack robustness compared to adversarial training. In conclusion, the vulnerability diversity induced by DVERGE training objective can effectively contribute to enhancing the robustness of CNN ensembles while maintaining desirable clean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>DVERGE hypothetically addresses some black-box adversarial vulnerabilities pervasive across machine learning applications while increasing compute requirements to training models. As such methods presented herein suggest potential impacts on the reliability, security, and carbon-footprint of deep-neural-network-based systems. The reliability and robustness of machine learning systems are not just a concern for practitioners but also policy makers <ref type="bibr" coords="9,354.08,528.01,15.27,8.64" target="#b31">[32]</ref>.</p><p>A net increase in carbon production would be considered a negative impact by many researchers in climate-related fields. This problem is common to many techniques that modify model training to achieve robustness, including DVERGE. While yet to be examined in the case of DVERGE, the possibility to mitigate or reduce excessive training burdens through informed hyperparameter selection exists. Sometimes, though modified training may increase the required computation per model parameter update, the modified method may nevertheless require fewer steps or epochs to achieve desirable results. Recent work provides actionable recommendations, such as performing cost-benefit analysis, to determine if efficient downstream adoption is desirable <ref type="bibr" coords="9,425.99,620.76,15.27,8.64" target="#b32">[33]</ref>.</p><p>In both industrial and military applications, practical solutions to vulnerabilities, such as relying on human-AI teaming <ref type="bibr" coords="9,186.55,648.06,15.32,8.64" target="#b33">[34]</ref>, are effective but do not address the underlying source of vulnerability and may limit the adoption of machine learning elsewhere. Addressing vulnerabilities at the training stage, then, is a desirable capability for positive-impact applications. By orthogonally improving only black-box robustness, though, we leave machine learning systems vulnerable to other types of attacks. Previous work has shown that white-box knowledge can still be leaked in black-box scenarios <ref type="bibr" coords="9,108.00,702.61,15.65,8.64" target="#b34">[35,</ref><ref type="bibr" coords="9,126.11,702.61,11.74,8.64" target="#b35">36]</ref>. As such, DVERGE is reliant on adversarial training to defend against white-box attacks and on traditional computer security to maintain system integrity. The ultimate interpretation of impact due to improved model reliability and security is not clear-cut, however, as it is highly dependent on the application space. This uncertainty is symptomatic of the fact that machine learning is often fundamental by nature and that there is no machine learning technique for improving robustness that can be applied only to positive-impact applications, whatever one's subjective interpretation of "positive" may be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training and implementation details</head><p>We train the baseline ensembles for 200 epochs using SGD with momentum 0.9 and weight decay 0.0001. The initial learning rate is 0.1, and we decay it by 10× at the 100-th and the 150-th epochs. Any models pre-trained on the clean dataset can serve as the starting point for the training of DVERGE. In our implementation, DVERGE starts from the trained baseline ensembles. We follow the aforementioned learning rate schedule, but using a carefully-tuned one is likely to bring extra performance gain. We reproduce ADP <ref type="bibr" coords="12,260.30,155.62,16.47,8.64" target="#b11">[12]</ref> and GAL <ref type="bibr" coords="12,317.99,155.62,16.46,8.64" target="#b12">[13]</ref> according to either the released code or the paper with recommended hyperparameters and setups. Specifically, they both use Adam optimizer <ref type="bibr" coords="12,108.00,177.44,16.73,8.64" target="#b36">[37]</ref> with an initial learning rate of 0.001. Also note that GAL requires the ReLU function to be replaced with leaky ReLU to avoid gradient vanishing. The other configurations stay the same as those of baseline ensembles.</p><p>Ensembles with adversarial training follow the baseline's training setup. We use 10-step PGD with = 8/255 and a step size α = 2/255 <ref type="bibr" coords="12,261.07,226.55,10.72,8.64" target="#b2">[3]</ref>. More specifically, adversarial examples w.r.t. the whole ensemble are generated in each step of the training process and are used to update model parameters. When combining DVERGE with adversarial training, however, adversarial examples are generated on each sub-model instead of the whole ensemble. We empirically find these choices help each case achieve its best robustness.</p><p>We use 0.5 as the input transformation probability for M-DI 2 -FGSM <ref type="bibr" coords="12,384.27,286.58,16.56,8.64" target="#b29">[30]</ref> and 0.2 as the γ for SGM <ref type="bibr" coords="12,108.00,297.49,16.60,8.64" target="#b30">[31]</ref> when generating these two attacks as recommended by their respective papers.</p><p>All models are implemented and trained with PyTorch <ref type="bibr" coords="12,330.85,313.87,16.73,8.64" target="#b37">[38]</ref> on a single NVIDIA TITAN XP GPU. Evaluation is performed based on AdverTorch <ref type="bibr" coords="12,290.96,324.78,15.13,8.64" target="#b38">[39]</ref>. As shown in <ref type="table" coords="12,364.14,324.78,27.58,8.64" target="#tab_0">Table 1</ref>, the training of DVERGE is marginally faster than that of adversarial training (AdvT). As they both need extra back propagations to either distill non-robust features or find adversarial examples, DVERGE uses only intermediate features for distillation while adversarial training requires the information back propagated from the final output. As for previous methods, though ADP requires the least time budget, it does not improve the robustness much as shown in <ref type="figure" coords="12,237.82,379.33,31.53,8.64" target="#fig_2">Figure 4</ref>. And the significantly improved robustness would be worth the extra training cost of DVERGE over GAL. In addition, according to <ref type="figure" coords="12,399.58,390.24,32.65,8.64">Figure 2</ref>, DVERGE could reduce the transferability within the ensemble at the very early stage of the training, so later training epochs have the potential to be simplified to a fine-tuning process without diversity loss, which will require much less training time. Further mitigating the computational overhead of DVERGE would be one of our future goals. B Analysis on the training of DVERGE  One important hyperparameter of DVERGE is the used for feature distillation. This section provides some initial exploration on the effect of using different . We start by looking at how affects the optimization of the feature distillation in Equation (1) and the resulted diversity loss in Equation <ref type="bibr" coords="13,491.72,97.30,10.52,8.64" target="#b4">(5)</ref>.</p><formula xml:id="formula_12" coords="12,270.98,664.61,167.50,16.37">f l i (x f l i ) − f l i (x) 2 E (x,y),(xs,ys),l L f j (x f l i ,</formula><p>The results evaluated with 1,000 CIFAR-10 testing images on two pre-trained ResNet-20 models are shown in <ref type="table" coords="13,161.79,119.11,28.49,8.64" target="#tab_1">Table 2</ref>. We find that a larger enables more accurate feature distillation as a smaller distance between the internal representation of the distilled image x f l i and the target image x is achieved. As a result, the distilled image from model f i can lead to a higher diversity loss on another model f j , which intuitively encourages the training routine of DVERGE to enforce a greater diversity and therefore a lower transferability between the two models. We empirically confirm this intuition in <ref type="figure" coords="13,175.08,176.78,33.70,8.64" target="#fig_4">Figure 6</ref>, where we vary the training and measure the transferability between sub-models. For instance, when ensembles have three sub-models, increasing from 0.03 to 0.07 decreases the transferability from 8%-10% to 3%-6%. Interestingly, however, for ensembles with five or eight sub-models, although we do observe a drop in the transferability between most of the sub-model pairs by using a larger , some pairs of the sub-models remain highly vulnerable to one another. In particular, observe that when training an ensemble of 5 sub-models with an of 0.07, 79% of adversarial examples from the second sub-model can fool the fourth one and 48% of examples transfer in the reverse direction. We leave a thorough and rigorous analysis of this phenomenon to future work. Finally, we look at the clean accuracy and robustness achieved by DVERGE ensembles trained with different . In <ref type="table" coords="14,189.95,86.39,29.30,8.64" target="#tab_3">Table 3</ref>, we observe that training with a larger leads to a higher black-box transfer robustness but a lower clean accuracy. The trend between white-box robustness and is not monotonic though, which we suspect is related to the observation that the attack transferability worsens between some pairs of sub-models under a larger training , as shown in <ref type="figure" coords="14,449.72,119.11,33.66,8.64" target="#fig_4">Figure 6</ref>. As the robustness against white-box attacks is not the main focus of DVERGE, we will explore the relationship, both qualitatively and quantitatively, between the transferability among sub-models and the achieved white-box robustness of the whole ensemble in the future. C Additional results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Decision region visualization</head><p>We visualize the decision regions learned by DVERGE ensembles around more testing images from the CIFAR-10 dataset in <ref type="figure" coords="14,207.44,351.24,32.24,8.64" target="#fig_5">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Transferability within the ensemble under different testing</head><p>In addition to <ref type="figure" coords="14,166.33,397.40,33.22,8.64" target="#fig_1">Figure 3</ref>, we provide more results of the transferability between sub-models under different attack in <ref type="figure" coords="14,191.64,408.31,33.51,8.64" target="#fig_6">Figure 8</ref>. In all cases, DVERGE achieves the lowest level of transferability among all ensemble methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Numerical results for robustness</head><p>We report numerical results that correspond to <ref type="figure" coords="15,295.83,702.61,33.71,8.64" target="#fig_2">Figure 4</ref> and <ref type="figure" coords="15,349.01,702.61,33.71,8.64" target="#fig_3">Figure 5</ref> in <ref type="table" coords="15,395.52,702.61,29.56,8.64" target="#tab_4">Table 4</ref> (black-box transfer accuracy) and <ref type="table" coords="15,165.77,713.51,29.27,8.64" target="#tab_5">Table 5</ref> (white-box accuracy), respectively. Note that ADP presents higher white-box  accuracy than black-box transfer accuracy in some cases, e.g., 4.9% &gt; 0.4% for ADP/8 when is 0.02, which implies ADP might result in obfuscated gradients <ref type="bibr" coords="16,356.78,675.31,15.27,8.64" target="#b39">[40]</ref>.</p><p>In addition, we provide the robustness plots with error bars (indicating standard deviation) computed over three independent runs in <ref type="figure" coords="16,239.35,702.61,33.98,8.64" target="#fig_7">Figure 9</ref>. DVERGE presents a little higher variation in results, which we suspect is due to the random distillation layer selected in the last training epoch. Refer to Appendix C.5 for discussion on the layer effects. However, DVERGE still yields noticeable improvements over other methods across the attack spectrum. A more challenging scenario for adversarial defenses is assuming the attacker is fully aware of the exact defense that the system relies on. In such case, the adversary can train an independent copy of the defended network as the surrogate model. We report black-box transfer accuracy of each method under this setting in <ref type="table" coords="17,191.41,536.97,28.80,8.64" target="#tab_6">Table 6</ref>. The same group of attacks as in Section 4.3 are used and ensembles with 3, 5, and 8 sub-models form the collection of surrogate models. According to <ref type="table" coords="17,431.23,547.88,27.50,8.64" target="#tab_6">Table 6</ref>, DVERGE still presents the strongest robustness against such a powerful black-box transfer adversary among all ensemble methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Discussion on DVERGE with adversarial training</head><p>Formally, the combined training objective of DVERGE and adversarial training is</p><formula xml:id="formula_13" coords="17,157.55,637.70,346.45,60.95">min fi E (x,y),(xs,ys),l       λ • j =i L fi (x f l j (x, x s ), y s ) DVERGE loss + max δ∈S L fi (x s + δ, y s ) AdvT loss       ,<label>(6)</label></formula><p>where λ is a hyperparameter that balances between the two terms. We set λ = 1.0 to achieve the results in <ref type="figure" coords="17,145.92,713.51,31.58,8.64" target="#fig_3">Figure 5</ref>. Results for other ensemble sizes under λ = 1.0 are shown in <ref type="figure" coords="17,423.26,713.51,36.05,8.64">Figure 10</ref>, where the standard deviation over three independent runs is also included. The observations stay the same as in Section 4.4. To better reflect the trade-off between clean accuracy and robustness, we also report the results for an ensemble of eight sub-models with λ = 0.5. In this case, DVERGE loss is weighted less and the training process will favor adversarial training. In turn, the ensemble spends more of its capacity to capture robust features instead of diverse non-robust features. Consequently, compared with λ = 1.0, we observe a decrease in clean accuracy and an increase in both black-box and white-box robustness when is large. In addition, the ensemble size is actually another weight factor in Equation <ref type="formula" coords="18,157.12,334.84,11.73,8.64" target="#formula_13">6</ref>as increasing the number of sub-models will naturally lead to larger DVERGE loss such that it outweighs the AdvT loss. As a result, larger (smaller) ensemble sizes for DVERGE+AdvT results in better (worse) clean performance yet worse (better) black-box and white-box robustness under a large . This assertion can be confirmed by the results in the bottom three rows of <ref type="table" coords="18,474.05,367.57,29.95,8.64" target="#tab_4">Table 4</ref> and <ref type="table" coords="18,124.88,378.48,28.11,8.64" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Ablation study on layer selection for distillation</head><p>As aforementioned, at each epoch of the DVERGE training, we randomly sample a layer from 20 candidate layers in the ResNet-20s to perform feature distillation based on the intuition that this could help avoid overfitting to any specific layer's feature sets. However, it is natural to wonder what the difference is between using each individual layer. While a rigorous investigation on layers' effect will be an important future direction for this work, here we compare the random layer selection with a straightforward alternative which is using a fixed layer for distillation throughout the training. <ref type="table" coords="18,107.69,505.70,28.77,8.64">Table 7</ref>: Black-box robustness ( =0.03) / white-box robustness ( =0.01) of each layer choice. The results for random layer selection are directly taken from <ref type="table" coords="18,337.26,516.61,29.36,8.64" target="#tab_4">Table 4</ref>  Specifically, we train another four ensembles by distilling only from the output layer of either ResBlock 1, ResBlock 2, ResBlock 3, or the whole network (correspondingly, the 7-th, 13-th, 19-th, and 20-th layer of the ResNet20). Then we report the black-box and white-box robustness results in <ref type="table" coords="18,118.89,604.42,30.40,8.64">Table 7</ref> and present the pair-wise transferability results (evaluated under =0.01) in <ref type="figure" coords="18,463.52,604.42,37.99,8.64" target="#fig_8">Figure 11</ref>. It can be seen that while training with deep layers (ResBlock 3 and the output layer) could indeed lead to lower transferability between sub-models, training with shallow layers (ResBlock 1 and 2) can introduce a noticeably higher white-box robustness for each sub-model itself, as shown by the lower diagonal numbers in the first and second heatmap of <ref type="figure" coords="18,345.30,648.06,37.11,8.64" target="#fig_8">Figure 11</ref>. The fact that some degree of white-box robustness can be achieved with shallow layers is surprising and needs further study, but this observation explains why they lead to stronger overall robustness for the ensemble than deep layers in <ref type="table" coords="18,144.57,680.79,28.16,8.64">Table 7</ref>. Meanwhile, training with random layers can balance the effect from both shallow and deep layer, achieving both individual robustness improvement and low transferability at the same time. These two factors both contribute to the overall black-box robustness of the ensemble. As shown in <ref type="table" coords="18,146.28,713.51,27.95,8.64">Table 7</ref>, random layer selection provides superior black-box robustness than other choices, <ref type="figure" coords="19,127.80,424.81,36.99,8.64">Figure 10</ref>: Results for DVERGE combined with adversarial training. From top to bottom, sub-plots on each row report the performance with 3, 5, and 8 sub-models respectively. while the achieved white-box robustness is a little lower than the best alternative (ResBlock 1) due to the lack of individual robustness in each sub-model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Ablation study on pre-training</head><p>As mentioned in Section 3.3, we train DVERGE from a pre-trained ensemble based on the intuition that well-learnt features of the pre-trained models are more informative for distillation and diversification. However, we show in <ref type="figure" coords="19,227.12,691.70,38.84,8.64" target="#fig_9">Figure 12</ref> that although slightly worse than using pre-trained models, training from scratch still offers improved robustness over others, implying that pre-training is not strictly necessary for DVERGE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Convergence check</head><p>As suggested in <ref type="bibr" coords="20,175.12,272.93,15.89,8.64" target="#b13">[14,</ref><ref type="bibr" coords="20,193.74,272.93,11.91,8.64" target="#b40">41]</ref>, we report accuracy vs. the number of attack iterations in <ref type="table" coords="20,446.43,272.93,28.89,8.64" target="#tab_8">Table 8</ref>. Note, we use only one random start here for white-box attacks for efficiency. One can observe that using more steps decreases the accuracy by no more than 0.6% for black-box attacks and no more than 1.2% for white-box attacks. Thus, we confirm sufficient steps have been applied and all attacks have converged during the evaluation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,108.00,691.70,396.17,8.64;6,108.00,702.61,396.17,8.64;6,108.00,713.13,396.00,9.03"><head>Figure 3 Figure 2 :</head><label>32</label><figDesc>Figure 3presents the pair-wise transferability of an ensemble with three sub-models tested under the same as aforementioned. Results for the ensembles composed of more sub-models and for other testing are reported in Appendix B and Appendix C.2, respectively. The number at the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,127.80,294.78,356.39,8.64;7,127.80,305.69,127.02,8.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pair-wise transferability (in the form of attack success rate) among sub-models for different ensemble methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,127.80,709.30,356.40,8.64;7,127.80,720.20,148.31,8.64"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Robustness results for different ensemble methods. The number after the slash stands for the number of sub-models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,171.36,208.13,269.29,8.64"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results for DVERGE combined with adversarial training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,127.80,699.33,358.05,8.64;13,127.80,710.24,328.49,8.64"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Transferability within DVERGE ensembles trained with different . The evaluation setting follows that ofFigure 3, where the attack perturbation strength is 0.03.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="15,127.80,599.76,356.40,8.64;15,127.80,610.67,356.40,8.64;15,127.80,621.58,356.40,8.64;15,127.80,632.49,188.72,8.64"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Additional decision region plots of ensembles with 3 ResNet-20 sub-models trained on CIFAR-10. Each pair of two rows is generated with one testing image. The first row is for the baseline ensemble, and the second row is for the DVERGE ensemble. The axes are chosen in the same way as inFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,192.75,351.38,226.51,8.64"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Transferability results under different testing .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="17,127.80,459.12,356.57,8.64;17,127.80,470.03,356.39,8.64;17,127.80,480.94,274.00,8.64"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Robustness results with error bars for different ensemble methods. The number after the slash stands for the number of sub-models. From top to bottom, sub-plots on each row report the performance with 3, 5, and 8 sub-models respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="19,165.03,555.45,281.94,8.64"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Pair-wise transferability results for different layer selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="20,127.80,205.74,358.14,8.64;20,127.44,216.65,356.76,8.64"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Results of DVERGE with or without pre-training comparing to other methods. All results are evaluated with 3 sub-models. Error bars are evaluated with 3 repeated runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,107.69,466.27,396.30,50.28"><head>Table 1 :</head><label>1</label><figDesc>Training time comparison on a single TITAN XP GPU. All times are evaluated for training a ResNet-20 ensemble with 3 sub-models for 200 epochs.</figDesc><table coords="12,145.55,491.41,320.90,25.14"><row><cell>Method</cell><cell cols="5">Baseline ADP [12] GAL [13] AdvT [3] DVERGE</cell></row><row><cell>Training time (h)</cell><cell>1.0</cell><cell>2.0</cell><cell>7.5</cell><cell>11.5</cell><cell>10.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,107.69,596.18,396.31,84.80"><head>Table 2 :</head><label>2</label><figDesc>The effect of on optimizing the feature distillation objective in Equation (1) and the resulting diversity loss measured with Equation<ref type="bibr" coords="12,296.99,607.09,10.42,8.64" target="#b4">(5)</ref>. f i and f j are two ResNet-20 models trained in a standard way on CIFAR-10.x f l i is short for x f l i (x, x s ).The step size used for feature distillation is chosen as /#steps.</figDesc><table coords="12,183.17,650.53,244.98,30.46"><row><cell>#steps</cell><cell>feature distillation objective</cell><cell>diversity loss</cell></row><row><cell></cell><cell>E (x,y),(xs,ys),l</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="14,107.69,179.61,396.56,84.32"><head>Table 3 :</head><label>3</label><figDesc>Robustness of DVERGE ensembles trained with different . In each table block, we report (clean accuracy) / (black-box transfer accuracy under perturbation strength 0.03) / (white-box accuracy under perturbation strength 0.01).</figDesc><table coords="14,122.27,212.72,367.46,51.21"><row><cell>#sub-models training</cell><cell>0.03</cell><cell>0.05</cell><cell>0.07</cell></row><row><cell>3</cell><cell cols="3">92.9% / 4.2% / 22.7% 92.7% / 26.6% / 32.3% 91.4% / 53.2% / 40.0%</cell></row><row><cell>5</cell><cell cols="3">92.3% / 30.5% / 43.1% 91.5% / 57.2% / 48.9% 90.2% / 66.5% / 42.3%</cell></row><row><cell>8</cell><cell cols="3">91.3% / 42.8% / 51.9% 91.1% / 63.6% / 57.9% 89.2% / 71.3% / 52.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,107.69,449.04,398.05,259.74"><head>Table 4 :</head><label>4</label><figDesc>Accuracy v.s. against black-box transfer attacks generated from hold-out baseline ensembles. The number in the first column after the slash is the number of sub-models within the ensemble. The results are averaged over three independent runs.</figDesc><table coords="14,130.86,485.47,350.27,223.31"><row><cell></cell><cell>clean</cell><cell>0.01</cell><cell>0.02</cell><cell>0.03</cell><cell>0.04</cell><cell>0.05</cell><cell>0.06</cell><cell>0.07</cell></row><row><cell>baseline/3</cell><cell>93.9%</cell><cell>9.6%</cell><cell>0.1%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>baseline/5</cell><cell>93.9%</cell><cell>9.4%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>baseline/8</cell><cell>94.4%</cell><cell>9.7%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>ADP/3 [12]</cell><cell cols="2">92.8% 20.9%</cell><cell>0.6%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>ADP/5 [12]</cell><cell cols="2">93.2% 21.8%</cell><cell>0.6%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>ADP/8 [12]</cell><cell cols="2">93.4% 21.2%</cell><cell>0.4%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>GAL/3 [13]</cell><cell cols="6">88.3% 76.6% 59.4% 39.8% 23.2% 11.5%</cell><cell>5.8%</cell><cell>2.5%</cell></row><row><cell>GAL/5 [13]</cell><cell cols="5">91.1% 78.3% 56.9% 33.3% 15.7%</cell><cell>6.3%</cell><cell>2.9%</cell><cell>0.7%</cell></row><row><cell>GAL/8 [13]</cell><cell cols="4">92.4% 75.1% 46.5% 21.7%</cell><cell>7.5%</cell><cell>2.0%</cell><cell>0.4%</cell><cell>0.0%</cell></row><row><cell>DVERGE/3</cell><cell cols="6">91.3% 83.2% 69.7% 51.0% 30.7% 15.8%</cell><cell>6.0%</cell><cell>1.5%</cell></row><row><cell>DVERGE/5</cell><cell cols="7">91.9% 83.8% 71.8% 55.0% 37.2% 21.4% 10.5%</cell><cell>3.2%</cell></row><row><cell>DVERGE/8</cell><cell cols="8">91.1% 85.2% 76.0% 65.0% 50.2% 34.9% 22.5% 11.2%</cell></row><row><cell>AdvT/3 [3]</cell><cell cols="8">77.2% 76.3% 74.8% 73.2% 70.9% 68.7% 66.0% 62.7%</cell></row><row><cell>AdvT/5 [3]</cell><cell cols="8">78.6% 77.8% 76.1% 73.8% 71.3% 69.0% 66.0% 63.3%</cell></row><row><cell>AdvT/8 [3]</cell><cell cols="8">79.4% 78.2% 76.9% 74.9% 72.3% 69.8% 66.8% 63.9%</cell></row><row><cell cols="9">DVERGE+AdvT/3 81.4% 79.7% 77.6% 75.3% 72.5% 69.1% 66.4% 62.6%</cell></row><row><cell cols="9">DVERGE+AdvT/5 83.4% 81.6% 79.3% 76.6% 74.1% 70.3% 66.3% 61.7%</cell></row><row><cell cols="9">DVERGE+AdvT/8 85.0% 82.8% 80.2% 76.8% 73.0% 68.7% 64.0% 57.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="16,107.69,384.87,396.31,248.83"><head>Table 5 :</head><label>5</label><figDesc>Accuracy v.s. against white-box attacks. The number in the first column after the slash is the number of sub-models within the ensemble. The results are averaged over three independent runs.</figDesc><table coords="16,133.11,410.39,345.79,223.32"><row><cell></cell><cell>clean</cell><cell>0.01</cell><cell>0.02</cell><cell>0.03</cell><cell>0.04</cell><cell>0.05</cell><cell>0.06</cell><cell>0.07</cell></row><row><cell>baseline/3</cell><cell>93.9%</cell><cell>1.1%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>baseline/5</cell><cell>93.9%</cell><cell>1.8%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>baseline/8</cell><cell>94.4%</cell><cell>2.8%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>ADP/3 [12]</cell><cell>92.8%</cell><cell>9.3%</cell><cell>0.4%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>ADP/5 [12]</cell><cell cols="2">93.2% 11.2%</cell><cell>0.9%</cell><cell>0.1%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>ADP/8 [12]</cell><cell cols="2">93.4% 12.7%</cell><cell>4.9%</cell><cell>2.5%</cell><cell>1.3%</cell><cell>0.8%</cell><cell>0.5%</cell><cell>0.2%</cell></row><row><cell>GAL/3 [13]</cell><cell>88.3%</cell><cell>9.3%</cell><cell>0.3%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>GAL/5 [13]</cell><cell cols="2">91.1% 32.2%</cell><cell>7.1%</cell><cell>0.5%</cell><cell>0.1%</cell><cell>0.1%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>GAL/8 [13]</cell><cell cols="2">92.4% 38.8%</cell><cell>9.4%</cell><cell>0.9%</cell><cell>0.2%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>DVERGE/3</cell><cell cols="3">91.3% 37.4% 10.2%</cell><cell>2.2%</cell><cell>0.4%</cell><cell>0.2%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>DVERGE/5</cell><cell cols="3">91.9% 47.7% 19.0%</cell><cell>5.2%</cell><cell>0.7%</cell><cell>0.2%</cell><cell>0.1%</cell><cell>0%</cell></row><row><cell>DVERGE/8</cell><cell cols="4">91.1% 56.5% 26.3% 10.7%</cell><cell>2.8%</cell><cell>0.5%</cell><cell>0.2%</cell><cell>0.1%</cell></row><row><cell>AdvT/3 [3]</cell><cell cols="8">77.2% 69.1% 59.2% 48.2% 36.1% 26.1% 17.2% 9.5%</cell></row><row><cell>AdvT/5 [3]</cell><cell cols="8">78.6% 69.6% 59.5% 48.5% 36.5% 26.2% 16.0% 9.2%</cell></row><row><cell>AdvT/8 [3]</cell><cell cols="8">79.4% 70.9% 60.8% 48.9% 37.0% 26.6% 17.1% 9.6%</cell></row><row><cell cols="9">DVERGE+AdvT/3 81.4% 71.4% 59.1% 44.1% 30.4% 19.8% 11.1% 5.5%</cell></row><row><cell cols="7">DVERGE+AdvT/5 83.4% 73.2% 59.2% 42.2% 28.0% 17.2%</cell><cell>8.3%</cell><cell>3.6%</cell></row><row><cell cols="7">DVERGE+AdvT/8 85.0% 72.3% 57.8% 40.8% 25.7% 14.8%</cell><cell>6.7%</cell><cell>3.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="18,107.69,80.12,396.31,151.97"><head>Table 6 :</head><label>6</label><figDesc>Accuracy v.s. against black-box transfer attacks generated from hold-out ensembles that are trained with the exact defense technique used by each ensemble. The number in the first column after the slash is the number of sub-models within the ensemble.</figDesc><table coords="18,150.58,116.55,310.84,115.54"><row><cell>clean</cell><cell>0.01</cell><cell>0.02</cell><cell>0.03</cell><cell>0.04</cell><cell>0.05</cell><cell>0.06</cell><cell>0.07</cell></row><row><cell cols="2">ADP/3 [12] 93.3% 34.7%</cell><cell>6.5%</cell><cell>1.4%</cell><cell>0.2%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell cols="2">ADP/5 [12] 93.1% 34.2%</cell><cell>6.6%</cell><cell>1.6%</cell><cell>0.6%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell cols="2">ADP/8 [12] 93.0% 32.5%</cell><cell>5.6%</cell><cell>1.4%</cell><cell>0.2%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell cols="4">GAL/3 [13] 88.8% 67.8% 36.2% 13.8%</cell><cell>3.7%</cell><cell cols="2">0.6% 0.1%</cell><cell>0%</cell></row><row><cell cols="3">GAL/5 [13] 91.0% 67.5% 31.9%</cell><cell>9.2%</cell><cell>1.8%</cell><cell>0.3%</cell><cell>0%</cell><cell>0%</cell></row><row><cell cols="3">GAL/8 [13] 92.3% 64.7% 25.8%</cell><cell>5.4%</cell><cell>0.6%</cell><cell>0.1%</cell><cell>0%</cell><cell>0%</cell></row><row><cell cols="4">DVERGE/3 91.4% 75.4% 50.2% 23.8%</cell><cell>7.6%</cell><cell cols="3">2.1% 0.3% 0.2%</cell></row><row><cell cols="4">DVERGE/5 91.9% 77.2% 53.1% 26.7%</cell><cell>9.5%</cell><cell cols="3">2.6% 0.5% 0.2%</cell></row><row><cell cols="8">DVERGE/8 91.1% 77.7% 57.3% 32.0% 13.9% 3.8% 0.9% 0.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="18,147.73,516.61,314.30,37.55"><head></head><label></label><figDesc>7% / 39.3% 50.2% / 36.5% 32.8% / 31.0% 37.3% / 33.5% 51.0% / 37.4%</figDesc><table coords="18,147.73,516.61,302.84,37.55"><row><cell></cell><cell></cell><cell></cell><cell>and 5.</cell><cell></cell></row><row><cell>ResBlock 1</cell><cell>ResBlock 2</cell><cell>ResBlock 3</cell><cell>Output Layer</cell><cell>Random</cell></row><row><cell>50.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="20,169.49,345.28,275.24,37.35"><head>Table 8 :</head><label>8</label><figDesc>Accuracy against attacks with varying number of iterations.</figDesc><table coords="20,250.82,359.60,193.91,23.03"><row><cell cols="3">black-box ( = 0.03)</cell><cell cols="3">white-box ( = 0.01)</cell></row><row><cell>100</cell><cell>500</cell><cell>1000</cell><cell>50</cell><cell>500</cell><cell>1000</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work is supported by the DARPA HR00111990079 (QED for RML) program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,127.92,222.10,377.65,7.77;10,127.92,231.90,139.14,7.93" xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,127.92,245.74,376.08,7.93;10,127.92,255.70,258.24,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,269.70,377.57,7.77;10,127.60,279.50,376.41,7.93;10,127.64,289.47,82.10,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,303.46,376.08,7.77;10,127.92,313.27,359.13,7.93" xml:id="b3">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,127.92,327.27,377.65,7.77;10,127.60,337.07,376.40,7.93;10,127.70,347.03,111.57,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,361.03,376.08,7.77;10,127.92,370.83,308.22,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main">Transferable perturbations of deep feature distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nathan</forename><surname>Inkawhich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Kevin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,384.83,376.08,7.77;10,127.92,394.63,341.88,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main">Convergent learning: Do different neural networks learn the same representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FE@ NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,408.63,377.57,7.77;10,127.92,418.43,272.38,7.93" xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12152</idno>
		<title level="m">Robustness may be at odds with accuracy</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,127.92,432.27,272.79,7.93" xml:id="b8">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,446.11,376.08,7.93;10,127.92,456.07,166.75,7.93" xml:id="b9">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multiple classifier systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,470.07,376.08,7.77;10,127.92,479.87,182.82,7.93" xml:id="b10">
	<monogr>
		<title level="m" type="main">Training ensembles to detect adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Alexander</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Gordon</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04006</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,127.92,493.87,376.08,7.77;10,127.92,503.67,215.64,7.93" xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving adversarial robustness via promoting ensemble diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Chao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08846</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,127.92,517.67,376.39,7.77;10,127.92,527.47,176.99,7.93" xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Sanjay</forename><surname>Kariyappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Qureshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09981</idno>
		<title level="m">Improving adversarial robustness of ensembles with diversity training</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,127.92,541.47,376.08,7.77;10,127.92,551.27,253.89,7.93" xml:id="b13">
	<monogr>
		<title level="m" type="main">On adaptive attacks to adversarial example defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08347</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,127.92,565.27,377.65,7.77;10,127.64,575.07,376.36,7.93;10,127.92,585.04,192.35,7.93" xml:id="b14">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE European symposium on security and privacy (EuroS&amp;P)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,599.03,376.08,7.77;10,127.60,608.84,208.13,7.93" xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06081</idno>
		<title level="m">Tianyu Pang, Xiaolin Hu, and Jun Zhu. Discovering adversarial examples with momentum</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,127.92,622.67,376.08,7.93;10,127.37,632.64,287.04,7.93" xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributionally adversarial attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Tianhang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Kui</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2253" to="2260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,646.48,376.08,7.93;10,127.92,656.44,181.33,7.93" xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Lars</forename><forename type="middle">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Peter</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,670.44,376.24,7.77;10,127.92,680.24,294.98,7.93" xml:id="b18">
	<analytic>
		<title level="a" type="main">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">I</forename><surname>Ludmila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Christopher J</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="207" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,127.92,694.24,376.08,7.77;10,127.92,704.04,376.08,7.93;10,127.92,714.16,65.01,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,76.13,377.57,7.77;11,127.92,85.93,376.08,7.94;11,127.92,95.89,90.16,7.94" xml:id="b20">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Homanga</forename><surname>Bharadhwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Florian</forename><surname>Shkurti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04514</idno>
		<title level="m">Dibs: Diversity inducing information bottleneck in model ensembles</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,110.00,376.08,7.77;11,127.92,119.80,223.80,7.93" xml:id="b21">
	<monogr>
		<title level="m" type="main">Batchensemble: an alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06715</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,133.91,376.08,7.77;11,127.92,143.71,376.08,7.93;11,127.92,153.67,121.46,7.93" xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ghassen</forename><surname>Michael W Dusenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yeming</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yi-An</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jasper</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Katherine</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Balaji</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dustin</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07186</idno>
		<title level="m">Efficient and scalable bayesian neural nets with rank-1 factors</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,167.78,376.08,7.77;11,127.92,177.58,268.83,7.93" xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03453</idno>
		<title level="m">The space of transferable adversarial examples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,191.53,376.08,7.93;11,127.62,201.49,171.68,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main">Intriguing properties of adversarial training at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,215.60,377.65,7.77;11,127.92,225.40,337.00,7.93" xml:id="b25">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,239.51,377.65,7.77;11,127.92,249.31,377.65,7.93" xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,263.42,376.08,7.77;11,127.92,273.39,79.58,7.77" xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="11,127.92,287.33,376.08,7.77;11,127.92,297.13,376.08,7.93;11,127.92,307.10,133.07,7.93" xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9185" to="9193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,321.21,376.08,7.77;11,127.92,331.01,376.08,7.93;11,127.62,340.97,242.11,7.93" xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2730" to="2739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,355.08,376.08,7.77;11,127.92,364.88,368.94,7.93" xml:id="b30">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dongxian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05990</idno>
		<title level="m">Skip connections matter: On the transferability of adversarial examples generated with resnets</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,378.99,376.08,7.77;11,127.92,388.95,67.48,7.77" xml:id="b31">
	<monogr>
		<title level="m" type="main">Robustness and explainability of artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ronan</forename><surname>Hamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Henrik</forename><surname>Junklewitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ignacio</forename><surname>Sanchez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,402.90,377.57,7.77;11,127.92,412.70,325.65,7.93" xml:id="b32">
	<monogr>
		<title level="m" type="main">Linguisticallyinformed self-attention for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08199</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,426.81,58.95,7.77;11,218.52,426.81,287.06,7.77" xml:id="b33">
	<monogr>
		<title level="m" type="main">How adversarial attacks could destabilize military ai systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">David</forename><surname>Danks</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,460.68,377.65,7.77;11,127.92,470.48,377.20,7.93;11,127.92,480.61,20.17,7.77" xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards reverse-engineering black-box neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Bernt</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Mario</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="121" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,494.56,376.08,7.77;11,127.92,504.23,376.08,8.06;11,127.92,514.48,56.04,7.77" xml:id="b35">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ari</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Thomas</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th {USENIX} Security Symposium ({USENIX} Security 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,528.27,376.08,7.93;11,127.92,538.23,85.67,7.93" xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,552.34,376.08,7.77;11,127.92,562.14,377.20,7.93;11,127.92,572.26,20.17,7.77" xml:id="b37">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.92,586.21,376.08,7.77;11,127.92,596.01,239.90,7.93" xml:id="b38">
	<monogr>
		<title level="m" type="main">AdverTorch v0.1: An adversarial robustness toolbox based on pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Luyu</forename><surname>Gavin Weiguang Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Xiaomeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,610.12,377.33,7.77;11,127.92,619.92,322.50,7.93" xml:id="b39">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,127.92,634.03,376.08,7.77;11,127.92,643.83,376.08,7.93;11,127.92,653.80,90.16,7.93" xml:id="b40">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0" coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06705</idno>
		<title level="m">Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
